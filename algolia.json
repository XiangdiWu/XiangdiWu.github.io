[{"author":null,"categories":["Quant"],"content":" 图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司。\n然而，当前基于图模型的股票预测研究范式面临三个核心挑战 ：\n下游任务设计的局限性：现有研究普遍聚焦于“次日趋势预测”这类短周期任务 ，但这无法充分利用图结构所能捕捉到的、通常具有滞后性的动量溢出效应。同时，这些模型大多只关注收益率，而忽略了波动率——一个衡量市场风险的关键指标。 模型设计的普适性问题：通用的GNN架构（如GCN和GAT）假设图中节点的特征分布相似，这在由各具独特行为模式的公司构成的企业关系图（Corporate Relationship Graphs, CRGs）中难以成立。而一些为股票预测定制的复杂模型，则往往以牺牲泛化能力为代价。 图构建方法的有效性评估缺失：企业关系图的构建大多依赖于经验，缺乏一套系统性的方法来比较不同图结构的优劣。 为应对这些挑战，本文提出了一套创新的解决方案。首先，设计了一个全新的长期股票预测任务，该任务不仅预测未来一段时间的平均收益率趋势，还预测其波动率，从而更好地匹配动量溢出效应的长期特性，并为投资者提供更具操作性的洞见。其次，开发了一种名为节点级图注意力网络（Node-level Graph Attention Network, NGAT） 的新型模型架构。NGAT的核心创新在于为每个公司（节点）分配一个独特的注意力机制，使其能根据自身特性学习如何聚合邻居节点的信息，从而完美适应企业关系图的异构性。最后，通过实验揭示了仅依赖下游任务表现来评估图构建方法优劣的局限性。\n问题构建 (Problem Formulation) 为了克服传统次日预测任务的短视性，本文将研究的核心聚焦于两个全新的长期预测任务：长期收益率均值预测与长期波动率预测。这两个任务的目标是基于历史数据和企业关系图，预测目标股票在未来为期 T 个交易日内的表现。\n1. 长期收益率趋势预测 (Return Trend Prediction)\n该任务被构建为一个分类问题，旨在判断未来 T 日的平均收益率相较于过去 T 日是上涨还是下跌。\n首先，定义股票 s  …","date":1758672e3,"dir":"post/0-PaperLearning/","expirydate":-62135596800,"fuzzywordcount":4500,"html":"图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司……","keywords":null,"kind":"page","lang":"en","lastmod":1758672e3,"objectID":"1b9c583afedac1a5a86e85682c68604d","permalink":"https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/","publishdate":"2025-09-24T00:00:00Z","readingtime":9,"relpermalink":"/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/","section":"post","tags":["Quant","Model"],"title":"论文学习《基于NGAT模型的股票长期趋势与风险建模》","type":"post","url":"/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/","weight":0,"wordcount":4469},{"author":null,"categories":["Quant"],"content":"⾦融理论基础 量化⾦融的学习离不开⾦融学基础，以下是关键知识点：\n⾦融市场与⾦融⼯具 股票、债券、期权、期货等⾦融产品的定义及特点。\n⾦融市场的运作机制，包括⼀级市场与⼆级市场。\n资产定价理论 资本资产定价模型（CAPM）：系统性⻛险与预期收益的关系。\n套利定价理论（APT）：多因素模型的定价⽅法。\n投资组合管理 ⻢科维茨投资组合理论：资产分散化与最优投资组合。\n夏普⽐率与⻛险调整收益分析。\n公司⾦融 资本结构与公司估值。\n企业财务决策：投资决策、融资决策和股利政策。\n⾦融数学 数学是量化⾦融的基础⼯具，以下数学知识点贯穿整个课程：\n微积分与线性代数 微分与积分在优化问题中的应⽤，如收益最⼤化。\n线性代数的矩阵运算在⻛险建模与资产组合中的应⽤。\n随机过程 随机变量与随机过程的基本概念。\n布朗运动及其在资产价格建模中的作⽤。\n偏微分⽅程 Black-Scholes⽅程的推导与应⽤。\n利⽤偏微分⽅程解决期权定价问题。\n时间序列分析 ⾃回归模型（AR）、移动平均模型（MA）、ARIMA模型。\n时间序列数据的平稳性检测和趋势预测。\n⾦融⼯程与衍⽣品定价 ⾦融⼯程专注于设计与分析⾦融⼯具，其知识点包括：\n期权与期货 期权定价公式及其推导过程（如布莱克-舒尔斯公式）。\n期货价格的发现过程及套期保值策略。\n利率模型 短期利率模型（如Vasicek模型、CIR模型）。\n利率期限结构理论。\n⻛险中性定价 概率测度变换及其在⾦融衍⽣品定价中的作⽤。\n⽆套利定价原理。\n蒙特卡洛模拟 ⽤于复杂衍⽣品定价和⻛险评估的数值⽅法。 统计学与数据分析 统计学在量化⾦融中⽤于⻛险评估、模型构建及数据分析，其核⼼知识点包括：\n概率论 随机变量分布（正态分布、泊松分布等）。\n中⼼极限定理及其在⾦融数据分析中的应⽤。\n估计与假设检验 参数估计⽅法（如最⼩⼆乘法、最⼤似然估计）。\n假设检验在⾦融模型验证中的应⽤。\n数据回归分析 ⼀元及多元线性回归模型。\n回归诊断（多重共线性、异⽅差性等）。\n⾼频数据分析 ⾼频交易数据的特点与分析⽅法。\n波动率估计与⻛险监控。\n计算机编程与数值⽅法 量化⾦融⾼度依赖计算机技术，以下编程技能是课程重点：\n编程语⾔ Python：数据处理（pandas）、⾦融建模（numpy、scipy）、可视化（matplotlib）。\nR：统计分析和时间序列建模。\nMATLAB：⾦融⼯具箱在 …","date":1751587200,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":1500,"html":"本文梳理量化金融学习的核心知识框架，助力构建系统性认知。需先夯实金融理论基础，涵盖金融市场工具、资产定价（CAPM/APT）、投资组合管理等内容。","keywords":null,"kind":"page","lang":"en","lastmod":1751587200,"objectID":"f5948f4128d6fb257c4227c8c8c45c66","permalink":"https://xiangdiwu.github.io/2025/07/04/quantitative_finance_core_knowledge_system/","publishdate":"2025-07-04T00:00:00Z","readingtime":3,"relpermalink":"/2025/07/04/quantitative_finance_core_knowledge_system/","section":"post","tags":["Quant"],"title":"量化金融学习方案","type":"post","url":"/2025/07/04/quantitative_finance_core_knowledge_system/","weight":0,"wordcount":1487},{"author":null,"categories":["Quant"],"content":"前言 很多人对量化感兴趣，但缺乏基础知识。这一系列科普文章会是很好的入门选择。\n系列文章旨在提供一个全面而易于理解的入门指南。这些文章覆盖了量化投资的基本概念、策略构建、风险管理以及实际应用等多个方面，内容浅显易懂，无需任何先验知识。通过阅读这些文章，您还将了解到量化投资在现代金融市场中的重要性和应用前景。\nQ：量化投资与主观投资的同与不同？ 从广义上讲，量化投资（Quantitative Investment）是一种以数据为基础、 以模型为核心、通常以程序化交易为工具的投资方法。\n1）以数据为基础：一般来讲数据点及结构化数据越多则越有利于建模。如果历史上没有发生过或仅发生过几次，很难找到合适的数据来训练，基于过往总结出的“规律”则在这个阶段不一定有效；\n2）以模型为核心：不管是早期以金融逻辑为基础的线性模型，还是后来更多采用的机器学习、深度学习模型，万变不离其宗——本质上都是以数据为基础建立模型；\n3）通常以程序化交易为工具：“程序化交易”是一个工具，不管是主观投资还是量化投资，不管机构投资者还是个人投资者，都可以根据自身情况去评估和使用该工具。事实上，不论是量化投资的先驱爱德华·索普，还是早期的西蒙斯，在量化投资的初始年代，计算机技术发展还不够充分，所以很多采用电话等人工下单方式。即便到现在，量化投资也未必完全采用程序化交易实现，对于一些流动性不好的品种——D. E. Shaw的部分期权交易还是通过打电话咨询和下单。\n作为一种投资方法，量化投资和主观投资一样，本质上都属于价格发现，捕捉市场错误定价、更好降低信息不对称、防范价格扭曲。两者殊途同归，并非完全对立，在不同周期、不同层面上挖掘市场的有效价格，只是采用不同的实现方式而已——前者侧重公开的结构化数据，后者侧重基本面深度调研。两类投资方法都是成熟市场上重要的参与者，都是不可或缺的部分。\n无论采用哪种投资方法，要想持续取得长期靠前的超额收益，都要求抓住市场的本质规律，背后考验的是对市场的深刻理解，只是量化投资更侧重把对市场的深刻理解和最前沿的科学技术做有效结合。一般来讲，量化投资以捕捉短周期的市场价格信息见长，擅长纠正短期的价格偏离。而随着周期拉长，主观投资基本面深度调研相对更为有优势。以股票多头为例，无论主观还是量化，对投资组合均采用主动管理：以获得市场收益的同时获取超越市场超额收益；均可采用夏普比率、收益回 …","date":1751500800,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":45100,"html":"本文通过45个问题，帮助投资者和量化初学者了解量化的基础认知。内容覆盖量化投资的基本概念、策略构建、风险管理以及实际应用等多个方面，内容浅显易懂。通过阅读这些文章，您还将了解到量化投资在现代金融市场中的重要性和应用前景。","keywords":null,"kind":"page","lang":"en","lastmod":1751500800,"objectID":"f2d101ccbf1e956ace84d6f53a9c3931","permalink":"https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions/","publishdate":"2025-07-03T00:00:00Z","readingtime":90,"relpermalink":"/2025/07/03/learn-quantitative-investing-with-45-questions/","section":"post","tags":["Quant"],"title":"45 个问题入门量化投资","type":"post","url":"/2025/07/03/learn-quantitative-investing-with-45-questions/","weight":0,"wordcount":45059},{"author":null,"categories":["Quant"],"content":"Q1: Similarities and Differences Between Quantitative Investment and Discretionary Investment? Broadly speaking, Quantitative Investment is an investment methodology based on data, centered around models, and often utilizing programmatic trading as a tool.\nData-driven: Generally, more data points and more structured data are more conducive to modeling. If an event has never happened historically or has only occurred a few times, it is difficult to find suitable data for training. \u0026amp;ldquo;Patterns\u0026amp;rdquo; derived from past experiences may not be effective in such scenarios.\nModel-centric: Whether it\u0026amp;rsquo;s early linear models based on financial logic or later machine learning and deep learning models, the essence remains the same – building models fundamentally grounded in data.\nOften uses programmatic trading as a tool: \u0026amp;ldquo;Programmatic trading\u0026amp;rdquo; is a tool. Both discretionary and quantitative investors, whether institutional or individual, can evaluate and use this tool based on …","date":1751500800,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":23400,"html":"This article uses 45 questions to help investors and quantitative beginners understand the fundamental concepts of quantitative investing. The content covers various aspects including the basic concepts of quantitative investment, strategy construction, risk management, and practical applications, presented in an easy-to-understand manner. By reading these articles, you will also gain insights into the importance and future prospects of quantitative investing in modern financial markets.","keywords":null,"kind":"page","lang":"en","lastmod":1751500800,"objectID":"6a2a79d1fa127e6974b6b293deaf4a13","permalink":"https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions-english/","publishdate":"2025-07-03T00:00:00Z","readingtime":47,"relpermalink":"/2025/07/03/learn-quantitative-investing-with-45-questions-english/","section":"post","tags":["Quant"],"title":"Learn Quantitative Investing with 45 Questions","type":"post","url":"/2025/07/03/learn-quantitative-investing-with-45-questions-english/","weight":0,"wordcount":23378},{"author":null,"categories":["Quant"],"content":"量化投资的学习之路布满陷阱，许多聪明人在这里栽了跟头，不是因为他们不够努力，而是因为踩了那些看似诱人实则危险的误区。\n误区一：认为“会编程=会量化投资” 表现： 过度沉迷于学习Python、Pandas、NumPy、各种库（如backtrader, zipline）甚至机器学习框架（TensorFlow, PyTorch），认为掌握了这些工具就等于掌握了赚钱的钥匙。\n问题： 编程是量化投资的必要工具，但不是充分条件。量化投资的核心是金融逻辑、市场理解、策略思想和风险管理。没有扎实的金融经济基础、对市场运行机制的理解和有效的策略逻辑，再强大的编程能力也只是在生成漂亮的垃圾（Garbage In, Garbage Out）。\n正确认知： 需要学习编程，但编程是实现策略想法、进行回测和执行交易的手段。应当优先理解市场、资产定价、交易规则、风险管理等基础知识，再结合编程技能去实现和验证想法。让工具为思想服务。\n误区二：沉迷于寻找“万能指标”或“圣杯策略” 表现： 花费大量时间在网上搜索“最牛指标”、“稳赚不赔的量化策略”、“年化100%的EA”，试图找到一个神奇的公式或参数组合，一劳永逸地解决赚钱问题。过度关注指标的“神奇性”而非其背后的逻辑。\n问题： 不存在永远有效的“圣杯”。市场是动态变化的，参与者行为、信息传播方式、宏观环境都在变。任何策略都有其适用条件和生命周期。追求“圣杯”是徒劳的，且容易陷入“指标迷信”或“参数优化陷阱”（过拟合）。\n正确认知： 理解任何策略都有其优势和局限性（盈亏同源）。学习的目标是理解不同策略（如趋势跟踪、均值回归、套利、统计套利、事件驱动等）的基本原理、适用场景和风险特征，并学会根据市场环境调整或组合策略。关注策略的逻辑和风险，而非表面的高收益。\n误区三：过度依赖和迷信回测结果 表现：\n看到一个回测曲线漂亮、夏普比率高、年化收益高的策略就兴奋不已，认为找到了宝藏。\n忽略回测中至关重要的细节：前视偏差（使用未来函数/数据）、生存者偏差（只包含存活到现在的股票/数据）、交易成本（佣金、滑点）、流动性限制、过拟合等。\n认为回测好=实盘一定能赚钱。\n问题： 回测是在理想化的历史数据上运行，与充满摩擦和不确定性的实盘环境有巨大差距。过度美化或忽略回测的局限性会导致对策略效果的严重误判，实盘时损失惨重。\n正确认知： 回测是必要但非充分的验证环节。要深刻 …","date":1751414400,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":14400,"html":"量化投资的学习之路布满陷阱，许多聪明人在这里栽了跟头，不是因为他们不够努力，而是因为踩了那些看似诱人实则危险的误区。本文梳理量化金融学习过程中常见的误区，希望能帮助大家少走弯路。","keywords":null,"kind":"page","lang":"en","lastmod":1751414400,"objectID":"274f4da702032c0aa6aa05b3be3a7edd","permalink":"https://xiangdiwu.github.io/2025/07/02/common-misconceptions-in-quantitative-learning/","publishdate":"2025-07-02T00:00:00Z","readingtime":29,"relpermalink":"/2025/07/02/common-misconceptions-in-quantitative-learning/","section":"post","tags":["Quant"],"title":"量化学习常见误区","type":"post","url":"/2025/07/02/common-misconceptions-in-quantitative-learning/","weight":0,"wordcount":14358},{"author":null,"categories":["Quant"],"content":"量化入门 思想和逻辑层面了解量化投资\n编程语言入门：Python和Packages\n编程语言入门：金融数据处理、SQL 数据库\n经典策略入门：双均线、动量/反转、均值回归、配对交易、风险平价入门实现与评价等\n量化基础 金融基础：金融市场学、投资学、公司理财、期权期货及其衍生品\n可选金融基础：金融经济学、宏微观经济学、计量经济学、货币银行学、国际金融学\n数学基础：微积分、线性代数、概率论和数理统计\n数学进阶：基础统计学、高级统计学\n进阶数据处理：非结构化数据处理、高级金融数据处理\n常用工具使用：金融数据API、SQLite和MongoDB数据库、回测引擎\n简单策略构建：实现夏普比率大于1.5，最大回撤小于15%的目标\n风险管理基础：VAR计算、风险指标和业绩评价标准\n量化进阶 多因子策略：因子构建、因子分析、策略回测、策略优化、组合优化\n资金管理：凯利公式应用、风险平价模型\n压力测试：黑天鹅事件模拟\n机器学习：线性/岭回归、随机森林、XGBoost、SVM、K-means\n深度学习：CNN、RNN/LSTM、Transformer、GAN、Diffusion\n强化学习：Q-Learning、DQN、Policy Gradient、A3C、PPO\n模型构建：截面回归、XGBoost/LightGBM、集成学习、模型解释（SHAP）与过拟合防控\n竞赛参与：Worldquant和Kaggle竞赛\n可选数学基础：高等代数、抽象代数、测度论、信息分析、偏微分方程等\n可选编程语言：Java、C语言、C++\n可选资产：期货策略、期权策略、外汇策略、债券策略、高频交易\n实现深度研究目标\n量化跃迁 量化投资系统集成与开发 量化求职 绿皮书刷题、Leetcode刷题\nKaggle竞赛、WorldQuant竞赛\n内资机构面试经验\n外资机构面试经验\n经验积累 金融实务知识\n前沿论文跟踪\n大量实践经验\n","date":1751328e3,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":800,"html":"本文介绍量化学习计划，包括量化入门、量化基础、量化进阶、量化跃迁、量化求职、经验积累。并附带 @希尔伯特旅馆 量化社区资源简介。","keywords":null,"kind":"page","lang":"en","lastmod":1751328e3,"objectID":"c061eb6a4b2bca688bab3d2f102b309d","permalink":"https://xiangdiwu.github.io/2025/07/01/quantitative-learning-plan/","publishdate":"2025-07-01T00:00:00Z","readingtime":2,"relpermalink":"/2025/07/01/quantitative-learning-plan/","section":"post","tags":["Quant"],"title":"量化学习计划","type":"post","url":"/2025/07/01/quantitative-learning-plan/","weight":0,"wordcount":761},{"author":null,"categories":["Quant"],"content":" 本文系统介绍了量化投资系统的五大核心模型，旨在通过数据驱动和理论驱动的方法优化投资决策。\n量化投资系统包括五大模型：\n1、超额收益模型 Alpha Models\n2、风险管理模型 Risk Model\n3、交易成本模型 Transaction Cost Models\n4、投资组合构建模型 Portfolio Construction Models\n5、执行模型 Execution Models\n超额收益模型 Alpha Models Alpha 模型是用于预测资产或投资组合的 Alpha 值的模型。在金融投资领域，Alpha 通常指的是资产或投资组合相对于市场基准的超额收益。\nAlpha 模型旨在通过识别和预测可以产生超额收益的投资机会，帮助投资者或投资机构实现比市场平均表现更好的投资回报。他基于历史数据、统计分析和数学模型来探索和利用市场中的非随机性模式和价格变动。\n投资者回报中与市场基准无关的部分称为超额收益，Alpha Models 的本质是通过识别和利用市场中的非随机性模式和价格变动，预测资产或投资组合的超额收益。\n如果一个基金经理管理的基金上涨了12%，而他对标的指数基准上涨了10%，一个快速的粗略分析将显示他的 Alpha 收益是+2%。国内市场的指数增强产品所追求的就是基于中证指数的超额收益。\n事实上，只有少数的交易策略为寻找 Alpha 收益而存在。但这些基本策略可以通过多种方式来实现，从而能够从有限的核心思想中创造出令人难以置信的策略多样性。理解交易策略的第一个关键是理解量化交易策略对科学的看法。\n科学的两个主要分支是理论主义和经验主义。理论科学家试图通过假设为什么它是这样来理解周围世界；实证科学家相信，对世界的足够观察可以让他们预测未来的行为模式。理论科学和经验科学之间的区别与量化策略密切相关，因此也有两种 Alpha Models。\nAlpha Models 可以被分为两类：理论驱动与数据驱动。\n现有的大部分 Alpha Models 都是基于理论驱动的，基金经理们从一些经济上可行的解释开始，解释为什么市场以某种方式行事，并测试这些理论，看看他们是否可以用来预测未来市场行为，以期获得超额收益。\n理论驱动的超额收益模型 Theory-Driven Alpha Models 理论驱动的 Alpha Models 可以分为两大类：量价模型与基本面模 …","date":16848e5,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":12800,"html":"量化投资系统五大核心模型包括超额收益模型、风险管理模型、交易成本模型、投资组合构建模型、执行模型。","keywords":null,"kind":"page","lang":"en","lastmod":16848e5,"objectID":"cefd92a99915e1262c6688693ba79840","permalink":"https://xiangdiwu.github.io/2023/05/23/five-core-models-of-quantitative-investment-systems/","publishdate":"2023-05-23T00:00:00Z","readingtime":26,"relpermalink":"/2023/05/23/five-core-models-of-quantitative-investment-systems/","section":"post","tags":["Quant"],"title":"量化投资系统的五大核心模型","type":"post","url":"/2023/05/23/five-core-models-of-quantitative-investment-systems/","weight":0,"wordcount":12767},{"author":null,"categories":["Reading"],"content":"本书着重阐述了政府在中国经济发展过程中所起的作用。之所以强调政府的作用，是因为我国的市场经济是由计划经济时代过渡而来的，因此在发展过程中保留了许多计划经济时代的印记。其中最突出的特色就在于政府在发展经济的过程中始终扮演着非常重要的参与者、甚至是主导者的角色。因此，要了解我国的经济发展，必须首先理解政府的运作方式，读懂政策的隐含意义。本书通过微观和宏观两个层面达到这一目的。微观层面，本书从经济学的视角介绍了地方政府的运行逻辑，以及按照这一逻辑，政府如何获取和利用财政资源，从而推动经济发展；宏观层面，则论述了地方政府推动经济发展的模式所造成的宏观后果，包括城市化、债务、国际国内风险等，并对现行经济政策进行解读与展望。\n地方政府发展经济的模式 要理解政府治理和运作的模式，必须首先了解权力和资源在政府体系中的分布规则。这一分布取决于两个重要体制特点：一是央地关系，二是条块分割。央地关系是指整体上中央与地方之间权力要平衡；条块分割是指地方部门要同时接受上级垂直部门和横向地方政府的领导，受到双重制约。基于这两个特点，地方政府的权力划分可以从三个视角考察：从外部性的视角，若政府提供的公共服务只影响本地，就可以由本地全权处理，若存在外部性，则需要上级协调；从信息的视角，有信息优势的一方实际权威也会更大，因此上级往往也会花费大量的精力获取下级的信息；从激励相容的视角，与本地发展目标无关或相反的事项更倾向于垂直领导，与本地发展目标相符或本地能受益的事项则倾向于交给本地政府处理。根据这些事权划分原则，地方政府在招商引资、发展经济的过程中享有非常广泛的权力，因此可以深度参与资源的生产与分配过程。\n要发展经济，地方政府除了有事权还不够，更重要的是财权。1985-1993年，推行财政包干，导致中央财政收入占比逐年下降，于是1994年进行分税制改革，增加了中央的收入，却大大降低了地方的财政资源。因此地方开始实行土地财政。一方面补贴工业用地，招商引资，一方面限制商住用地，房价带动地价。但对于无法依靠土地财政的基层，则出现了财政困难，地区间不平等，需要依靠中央转移支付，并继续进行财政改革。如农村税费改革，乡财县管、省直管县等等。\n土地进入资本市场，就从土地财政变成了土地金融。政府成立地方政府融资平台，即“城投公司”，以未来的土地收益为抵押撬动大量银行贷款，就可以进行投资以推动城市化和工业化。一是投资 …","date":1643846400,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":4800,"html":"《置身事内》从中国地方政府的经济行为切入，系统分析了政府在我国经济发展中的关键角色。通过微观（财政运作、招商引资）和宏观（城市化、债务风险）双重视角，揭示了‘土地财政’与‘土地金融’模式的逻辑与影响，并探讨了经济结构失衡、债务问题及改革路径。本书为理解中国特色的政治经济体制提供了深刻洞察，是解读政府行为与经济政策的必读之作。","keywords":null,"kind":"page","lang":"en","lastmod":1643846400,"objectID":"0f40a8a3ee9cbc8073506f0732d597f0","permalink":"https://xiangdiwu.github.io/2022/02/03/reflections-on-inside-the-system/","publishdate":"2022-02-03T00:00:00Z","readingtime":10,"relpermalink":"/2022/02/03/reflections-on-inside-the-system/","section":"post","tags":null,"title":"《置身事内》读后感","type":"post","url":"/2022/02/03/reflections-on-inside-the-system/","weight":0,"wordcount":4789},{"author":null,"categories":["Reading"],"content":" 意识流手法的内在性和象征性成就了乔伊斯。由于历史、宗教、政治、语言、文体的创新与错综复杂的交织，使得《尤利西斯》內部形成了两个独立且相关的文本。它就像是一本百科全书式的文字游戏，需要我们借助其它的文本信息，才能破译其中的密码。\n《尤利西斯》讲述了一个都柏林人在1904年6月16日的遭遇和心路历程。主角布卢姆（又译“布鲁姆”）是一个匈牙利裔犹太人 ，他有一个妻子和一个女儿。自从儿子死后，布卢姆和妻子的关系逐渐僵化。\n这天，因为得知妻子要与情人约会，懦弱的布卢姆决定离家一天以逃避妻子出轨的事实。他先后去了邮局、坟地、报社、饭馆、图书馆、大街、酒吧、海滩、马博特街、妓院、马车夫棚，一直到凌晨两点才回家。\n在外晃荡的过程中，六神无主的布卢姆遇到了神志不清的斯蒂芬。布鲁姆不仅把斯蒂芬当成了自己已夭折的儿子鲁迪，还把他领回家，请他喝可可。斯蒂芬离开后，布卢姆走进卧室，发现房内的摆设有所变动。他开始脑补妻子和情人偷欢的场景，最后在困意中恢复了内心的宁静。\n这个故事，并不难懂，甚至光看介绍会觉得毫无吸引力。但是为了感受这部20世纪最伟大的意识流小说。我还是带着浓厚的兴趣，硬着头皮去读。《尤利西斯》真的太难懂。难度主要在于理解艺术创意、文学象征和政治隐喻。每个词都有构思和设计，阅读体验犹如游戏。\n《尤利西斯》的不朽，不全是因为技巧和叛逆。\n《尤利西斯》的最终指向仍是“自我追寻”（哈，人类命运共同体精神内容总是“人生的平凡、挫折、虚无与精神的不屈”）。在《尤利西斯》中，乔伊斯将布鲁姆的经历与《荷马史诗》中古希腊英雄奥德修斯的十年漂泊对照穿插。\n布卢姆的一日游满足了“自我追寻”模式的五个要素：\n- 追寻者（布卢姆）\n- 目的地（家，离家是为了回家）\n- 前往目的地的原因（逃避妻子出轨的事实）\n- 路上遇到的挑战和考验（本能、情感、宗教信仰、政治理念的冲击和挣扎）\n- 读者找到故事人物前往目的地的真正原因（对民族历史的反省、唤醒麻痹的群众、追求独立和自由思想）\n虽然使用了史诗的宏大叙事和结构，但乔伊斯的目的并不在于塑造英雄或谱写赞歌。相反，乔伊斯利用小说中丰富的文学意象，对愚民心中的英雄形象和麻痹的行为进行了反讽。\n郭军在《乔伊斯：叙述他的民族——从\u0026amp;lt;都柏林人\u0026amp;gt;到\u0026amp;lt;尤利西斯\u0026amp;gt;》给出了一种解释：\n“乔伊斯版本的、即解殖民化时代的“奥德赛”，可以说既是一个回归家园的历 …","date":1641513600,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":1700,"html":"《尤利西斯》是爱尔兰作家詹姆斯·乔伊斯的意识流巨著，记录都柏林三人组在1904年6月16日18小时内的琐碎活动与心理潜流，对应荷马史诗结构，以多语言、戏拟、内心独白和标点实验，将城市空间、身体欲望与历史记忆熔铸成现代史诗，被誉为20世纪英语小说巅峰。","keywords":null,"kind":"page","lang":"en","lastmod":1641513600,"objectID":"1f6435e69afecaae09188baddbabb20c","permalink":"https://xiangdiwu.github.io/2022/01/07/reflections-on-ulysses/","publishdate":"2022-01-07T00:00:00Z","readingtime":4,"relpermalink":"/2022/01/07/reflections-on-ulysses/","section":"post","tags":null,"title":"《尤利西斯》读后感","type":"post","url":"/2022/01/07/reflections-on-ulysses/","weight":0,"wordcount":1686},{"author":null,"categories":["Tech"],"content":"Clean White Theme for Hugo CleanWhite is a clean, elegant, but fully functional blog theme for Hugo. Here is a live demo site using this theme.\nIt is based on huxblog Jekyll Theme and Clean Blog Jekyll Theme.\nThese two upstream projects have done awesome jobs to create a blog theme, what I\u0026amp;rsquo;m doing here is porting it to Hugo, of which I like the simplicity and the much faster compiling speed. Some other features which I think could be useful, such as site search with algolia and proxy for Disqus access in China, have also been built in the CleanWhite theme. Other fancy features of upstream projects are not supported by this Hugo theme, I\u0026amp;rsquo;d like to make it as simple as possible and only focus on blog purpose, at least for now. While I created this theme, I followed the Hugo theme best practice and tried to make every part of the template as a replaceable partial html, so it could be much easier for you to make your customization based on it.\nScreenshots Home Post Search …","date":1632614400,"dir":"post/6-Tech/","expirydate":-62135596800,"fuzzywordcount":1200,"html":"CleanWhite is a clean, elegant, but fully functional blog theme for Hugo, which I use for my own blog.","keywords":null,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"98cd58ff5887b19ce2a734f71a84e29c","permalink":"https://xiangdiwu.github.io/post/6-tech/hugo-theme-clean-white/","publishdate":"2021-09-26T00:00:00Z","readingtime":3,"relpermalink":"/post/6-tech/hugo-theme-clean-white/","section":"post","tags":["Hugo"],"title":"Hugo Theme: CleanWhite","type":"post","url":"/post/6-tech/hugo-theme-clean-white/","weight":0,"wordcount":1134},{"author":null,"categories":["Quant"],"content":"一、Python 基本数据类型 Python 的三大基本数据类型，是量化分析的起点：\n整数（int） 整数是没有小数部分的数字，在金融中常用于记录 “数量”—— 比如股票的持仓股数（1000 股）、交易日天数（252 天 / 年）、订单编号（10086）等。\n示例：\n# 记录持仓股数 holdings = 1000 # 计算一年交易日（A股约252天） trading_days = 252 整数的运算非常直接（加减乘除、取余等），比如计算 “持仓 1000 股，每股涨 2 元，总收益是多少”：profit = 1000 * 2，结果为 2000（元）。\n浮点数（float） 浮点数是带小数的数字，金融中几乎所有 “连续型数据” 都用它表示：股票价格（30.5 元）、收益率（5.2%）、手续费率（0.03%）等。\n示例：\n# 股票价格 stock_price = 30.5 # 单日收益率（5.2%） daily_return = 0.052 # 手续费率（万分之三） fee_rate = 0.0003 浮点数的运算需要注意精度（避免小数位数过多导致误差），比如计算 “1000 股，每股 30.5 元，总市值”：market_value = 1000 * 30.5，结果为 30500.0（元）。\n字符串（str） 字符串是由字符组成的文本，用于给金融数据 “打标签”—— 比如股票代码（“600036.SH”）、公司名称（“招商银行”）、日期（“2023-10-01”）等。\n示例：\n# 股票代码（沪市用.SH，深市用.SZ） stock_code = \u0026amp;#34;600036.SH\u0026amp;#34; # 交易日期 trade_date = \u0026amp;#34;2023-10-01\u0026amp;#34; # 策略名称 strategy_name = \u0026amp;#34;双均线策略\u0026amp;#34; 字符串的核心作用是 “标识”，比如通过股票代码区分不同标的，通过日期筛选特定时间段的数据。在量化中，常需要将字符串转换为其他类型（如日期字符串转成时间格式），方便后续计算。\n二、Python 金融数据结构 金融数据往往不是孤立的（比如一只股票的多日价格、多只股票的持仓信息），需要用 “数据结构” 来有序存储。以下 5 种结构，是量化中最常用的 “整理工具”：\n列表（List） 列表是 Python 中最基础的 “有序集合”，可以 …","date":1625443200,"dir":"post/1-Quant/","expirydate":-62135596800,"fuzzywordcount":5800,"html":"本文介绍了量化分析中最常用的 Python 数据类型和结构，以及常用的编程逻辑。","keywords":null,"kind":"page","lang":"en","lastmod":1625443200,"objectID":"108317cbe9eb88205bff5b3556e635c8","permalink":"https://xiangdiwu.github.io/2021/07/06/quant-python-basics/","publishdate":"2021-07-05T00:00:00Z","readingtime":12,"relpermalink":"/2021/07/06/quant-python-basics/","section":"post","tags":["Quant"],"title":"量化编程基础： Python ","type":"post","url":"/2021/07/06/quant-python-basics/","weight":0,"wordcount":5797},{"author":null,"categories":["Reading"],"content":"论及中国宏观经济与金融领域的学者，余永定是非常耀眼的一位，他主持的中国社会科学院世界经济政治所，是中国宏观经济涉外政策认识与决策的主要影响者。自1996年中国“双顺差”格局显现以来，国际收支失衡已成中国宏观经济决策和经济增长模式转变的主要障碍。余先生的新文集《见证失衡》即显示了他对这一问题的思考历程。\n从经济发展的转折点来看，自1996年第三轮软着陆式的宏观调控生效以来，中国先后经历了东南亚金融危机、经济衰退、加入WTO、经济复苏到过热，到2008年下半年经济衰退显现止，在12年间经历了一个完整的经济周期。与此伴随着的，是中国执行了出口导向的发展战略，出口贸易额占GDP的比重从1998的20%上升到当前的40%左右。《见证失衡》一书副标题“双顺差、人民币汇率和美元陷阱”，正是国内外社会各界对国际收支失衡问题关注焦点的转移的三部曲。首先，出口导向的增长策略导致了1991年以来的“双顺差”为特征的中国国际收支局面，这是随后诸多故事的主要源头。其次，作为出口导向发展战略的政策辅助，压低人民币汇率成为1998年以后汇率政策主要思路，同时也催生了各国敦促人民币升值的政治压力。其三，持续双顺差形成了巨额外汇储备，除了外储保值增值的大麻烦以外，因外储过大产生的冲销、热钱涌动、资产泡沫等问题，严重影响了国内货币政策的独立性，宏观调控空间受到了很大限制。\n余永定先生对近十几年来中国国际收支失衡诸问题的认识，是相当深刻的，也奠定了目前各界对这些问题认识的基础。但问题另一面是，如果仅仅从两国或一时的国际收支来着手，恐怕并不能观察到症结的全貌。全球化是国际收支失衡的时代背景。在全球化趋势不可扭转的背景下，基本每个国家都主动或者被动的卷入其中。但参与全球化活动的收益显然并不均等，但收益多寡是一个问题，而被边缘化则是更大的威胁。\n而解决中美的国际收支失衡及因此产生的经贸摩擦，我们还必须引入一个更为根本性的因素：劳动力。中国过剩的生产能力缘自投资与生产能力。高储蓄率决定了高投资率，大规模的廉价劳动力在高投资的作用下迅速地升成为生产能力。资金、技术和商品迅速地在全球经济体系内流动，但是，劳动力却是难以在全球范围内流动的。所以，以中美国际收支失衡为代表的全球国际收支失衡局面，其根本症结在于，商品可自由流动，而劳动力不能，也就是说随着全球化的深入，而部分生产要素的流动却并未同步。这是全球化所带来的矛 …","date":1620864e3,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":2400,"html":"《见证失衡:双顺差、人民币汇率和美元陷阱》，余永定作为有影响力的经济学家，十多年来一直呼吁遏制中国经济的双顺差，理顺中国经济与世界经济的关系，但却并不如愿。今日中美经济关系的严重扭曲，使我们有必要重新审视作者当年的忧虑。《见证失衡:双顺差、人民币汇率和美元陷阱》遴选其三十多篇代表作品，以年代为序，试图表现作者一脉相承的见解，同时也表达了一个经济学家敢于反思研究历程并接受历史检验的勇气。","keywords":null,"kind":"page","lang":"en","lastmod":1620864e3,"objectID":"b544227dc058aab0a3263af1e71c2c95","permalink":"https://xiangdiwu.github.io/2021/05/13/reflections-on-witnessing-imbalance/","publishdate":"2021-05-13T00:00:00Z","readingtime":5,"relpermalink":"/2021/05/13/reflections-on-witnessing-imbalance/","section":"post","tags":null,"title":"《见证失衡》读后感","type":"post","url":"/2021/05/13/reflections-on-witnessing-imbalance/","weight":0,"wordcount":2326},{"author":null,"categories":["Quant"],"content":"自然语言处理的应用（即NLP任务）分布非常广泛。本章节以任务的形式进行划分，主要介绍三类具有代表性的自然语言处理应用，包括文本分类（序列-编码-类别）、文本摘要（序列-编码-解码-序列）、以及机器阅读理解（序列-编码-同步序列）。每个部分首先介绍了应用的概念及挑战，然后介绍了一些具有代表性的论文工作。注意，从接触一个领域的具体任务开始，已经初步进入到了对该科研领域的探索之中。每个领域都有一些具体的任务，而确定一个任务往往就是着手开展一项研究工作的第一步。\n本节介绍的相关工作均为大语言模型流行之前的工作范式。\n文本分类 文本分类(text classification) 在自然语言处理领域有着广泛的应用，例如垃圾邮件检测(spam detection)、情感分析(sentiment analysis)、语言识别(language id) 以及新闻类别标注(news classification) 等。文本分类有着非常多的实现方法，每种实现方法中文本的表示也存在着很大的不同。但总的来说，文本分类是一个监督学习问题，需要数据样本以及其对应标签。\n分类问题又可以分为二分类、多分类以及多标签分类等类型。其中的多标签分类指的是为每个数据标定数量不相同的标签，其由于类标签数量不确定、类标签之间可能有相互依赖、多标签的训练集比较难以获取等原因，难度是分类问题中最大的，因此也称为了分类问题的研究热点。目前主流的多标签分类的实现方式有如下几种：\n(1) 不考虑标签之间的关联性：将每个标签的出现与否视为二分类任务。\n(2) 考虑标签的关联性：分类器链、序列生成任务、通过标签共现(组合) 的方式转换为多分类任务。\n对于基于神经网络的分类器，实现多标签分类，只需将输出层的softmax激活函数改为sigmoid激活函数，对每个类别进行二分类即可。在这个过程中，每个标签之间不是独立的，其关联性也会被神经网络学习到。\n分本分类通常用准确率(accuracy)、召回率(recall)、精准率(precision)和F1(又分为micro和macro) 等指标进行评价。\n基于朴素贝叶斯算法的文本分类 朴素贝叶斯算法是一种生成模型，其实现简单，常常用于文本分类的baseline。在朴素贝叶斯算法中，文本常用词袋模型进行表示，即每个文档表示为一个大小为$|V|$的向量，其中$|V|$为词汇表的大小，每一 …","date":1603584e3,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":12600,"html":"本文主要介绍了自然语言处理的应用，包括文本分类、文本摘要、机器阅读理解等。","keywords":null,"kind":"page","lang":"en","lastmod":1603584e3,"objectID":"c77a2dc4e6d345134adefa34a105f492","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/","publishdate":"2020-10-25T00:00:00Z","readingtime":25,"relpermalink":"/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理：自然语言处理应用","type":"post","url":"/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/","weight":0,"wordcount":12517},{"author":null,"categories":["Quant"],"content":"ELMo 预训练词向量(如word2vec和GloVe等)通常只能为一个单词产生一个特定的词向量，而忽略了该单词的上下文(context) 关系，因而无法解决一词多义或一义多词的问题。ELMo(embeddings from language models) 本质上是一个深度双向LSTM模型，用于为一个句子中的每个单词生成上下文相关的词向量。将这些上下文相关词向量编码了单词的深层次语义和句法信息，因此当ELMo应用到许多NLP任务中，这些任务的效果相对于使用静态的词向量往往能得到很大的提升。\nELMo是整个输入句子的函数，其输出为句子中每个单词的上下文相关词向量。给定一个含有$N$个标记的序列$(t_1,t_2,\\cdots,t_N)$，前向语言模型(forward language model) 通过建模在给定之前的标记序列$(t_1,\\cdots,t_{k-1})$下$t_k$的概率来计算该句子(标记序列)的概率： $$\rp\\left(t_{1}, t_{2}, \\cdots, t_{N}\\right)=\\prod_{k=1}^{N} p\\left(t_{k} | t_{1}, t_{2}, \\cdots, t_{k-1}\\right)\r$$在ELMo之前的语言模型通常为第$k$个位置的单词(通过embedding等方式)计算出一个上下文无关的词表示$\\mathbf x_k^{LM}$，然后将其送入一个$L$层的前向LSTM。在每个位置$k$，每一层LSTM会输出一个上下文相关的表示为$\\overrightarrow{\\mathbf h}_{k,j}^{LM}$，其中$j=1,2,\\cdots,L$。最顶层的LSTM输出$\\overrightarrow{\\mathbf h}_{k,L}^{LM}$在下游任务被用来预测下一个标记(通过sottmax层等方式)，即$t_{k+1}$。\n反向语言模型(backward language model) 与前向语言模型的计算方向正好相反： $$\rp\\left(t_{1}, t_{2}, \\cdots, t_{N}\\right)=\\prod_{k=1}^{N} p\\left(t_{k} | t_{k+1}, t_{k+2}, \\cdots, t_{N}\\right)\r$$ 因此在第$k$个位置，第$j$层LSTM的输出表示 …","date":1603497600,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":14600,"html":"本文主要介绍了ELMo和Transformer的预训练模型，以及其在机器翻译和其他NLP任务中的应用。","keywords":null,"kind":"page","lang":"en","lastmod":1603497600,"objectID":"e2810d87344dbcfd7be0b9150a4ec1ed","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-24T00:00:00Z","readingtime":30,"relpermalink":"/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理：预训练模型","type":"post","url":"/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":14542},{"author":null,"categories":["Quant"],"content":"序列到序列模型 许多单输出问题得以解决，比如命名实体识别、单词预测等。然而许多任务的输出是一个序列，比如机器翻译、对话系统以及自动摘要等。这种问题应当使用seq2seq实现。\n序列到序列，或者seq2seq，是一个比较新的模型，在2014年被提出用于英语-法语翻译。在更高的层面上，seq2seq是一个由两个循环神经网络组成的端到端模型：\n(1) 一个编码器(encoder)，将模型的输入序列作为输入，然后编码固定大小的“上下文向量”。\n(2) 一个解码器(decoder)，使用来自解码器生成的上下文向量作为从其生成输出序列的“种子”。\n因此，seq2seq模型通常被称为“编码器-解码器模型”。其示意图如下：\n首先，编码器将输入句子编码为一个隐含层向量作为整个输入句子的嵌入向量，然后将该向量作为解码器的初始隐含层向量，输入一个\u0026amp;lt;START\u0026amp;gt;特殊标记，通过自回归的方式不断生成单词，直到生成\u0026amp;lt;END\u0026amp;gt;符号，或者达到长度限制为止。以上decoder生成(测试)句子的方式为greedy decoding，即每次选取输出softmax值最大的单词。\ngreedy decoding的缺陷是无法undo decision，即一步错，步步错。解决方案是beam search，即explore several hypotheses and select the best one。On each step of decoder, keep track of the k(beam size) most probable partial translations。beam search相当于全局搜索的剪枝过程，其不保证找到最优解，但是效果更好。beam size为2的情况如下：\n注意力机制 当人们听到句子“the ball is on the field”，人们不会认为这6个单词都一样重要，而是首先会注意到“ball”，“on” 和 “field”，因为这些单词是人们觉得最重要的。类似的，Bahdanau等人注意到使用RNN的最终状态作为seq2seq模型的单个“上下文向量”的缺点：输入的不同部分具有不同的重要程度。再者，输出的不同部分甚至可以考虑输入的不同部分“重要”。例如，在翻译任务中，输出的第一个单词是一般是基于输入的前几个词，输出的最后几个词可能基于输入的后几 …","date":1603411200,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":6600,"html":"本文主要介绍了seq2seq与attention的相关内容，包括seq2seq的基本结构，attention的基本结构，以及attention在seq2seq中的应用。","keywords":null,"kind":"page","lang":"en","lastmod":1603411200,"objectID":"24cc0cb39b1d62195a6982b40c2de6f2","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/","publishdate":"2020-10-23T00:00:00Z","readingtime":14,"relpermalink":"/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理：“序列到序列”与“注意力机制”","type":"post","url":"/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/","weight":0,"wordcount":6563},{"author":null,"categories":["Quant"],"content":"N元语法 N元语言模型可以用来预测一个句子的下一个单词的概率，或者计算一个句子的概率。该模型常用于语音识别、拼写检查以及语法检查等领域。\n首先，一个句子“its water is so transparent that”后出现某一单词“the”的概率为：\n但是由于语料的多样性以及巨大的数量，许多句子不会在语料中出现，并且计数过程会消耗大量的时间。同样，对于一个句子的概率进行计算，有：\n$$\r\\begin{aligned}\rP(X_{1} \\cdots X_{n}) \u0026amp;=P(X_{1}) P(X_{2}|X_{1}) P(X_{3}|X_{1}^{2}) \\cdots P(X_{n}|X_{1}^{n-1}) \\\\\r\u0026amp;=\\prod_{k=1}^{n} P(X_{k}|X_{1}^{k-1})\r\\end{aligned}\r$$$$\r\\begin{aligned}\rP(w_{1}^{n}) \u0026amp;=P(w_{1}) P(w_{2}|w_{1}) P(w_{3}|w_{1}^{2}) \\cdots P(w_{n}|w_{1}^{n-1}) \\\\\r\u0026amp;=\\prod_{k=1}^{n} P(w_{k}|w_{1}^{k-1})\r\\end{aligned}\r$$应用N-gram语言模型，假设一个单词只由其之前$N-1$个单词决定，可以减少计算量。比如二元语言模型： $$\rP(w_k|w_1^{k-1})=P(w_k|w_{k-1})\r$$ 二元语言模型的假设也称为马尔可夫假设。\n根据马尔可夫假设，一个句子的概率计算公式为： $$\rP(w_{1}^{n}) \\approx \\prod_{k=1}^{n} P(w_{k}|w_{k-1})\r$$ 上式中，二元条件概率$P(w_{k}|w_{k-1})$的计算公式为： $$\rP(w_{k}|w_{k-1})=\\frac{C(w_{k-1} w_{k})}{C(w_{k-1})}\r$$ 在实践中，常常使用三元语法模型(3-gram language model)，甚至四元、五元语法模型(当数据量充足时)。实现时，往往对概率取log，将乘法变为加法，提升运算速度。\n评估语言模型 语言模型的评估分为外部评估(extrinsic evaluation) 与内部评估(intrinsic evaluation) 两种。在外部评估中，语言模型被某个应用使 …","date":1603324800,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":3e3,"html":"本文主要介绍了语言模型的相关内容，包括N-gram语言模型，困惑度，泛化与未知词汇，平滑，神经语言模型，Tensorflow实现神经语言模型。","keywords":null,"kind":"page","lang":"en","lastmod":1603324800,"objectID":"e614b9fbf6479e48930e902779794f8a","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-22T00:00:00Z","readingtime":6,"relpermalink":"/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理：语言模型","type":"post","url":"/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":2924},{"author":null,"categories":["Quant"],"content":"词向量概述 在自然语言处理领域，词的表示(representation) 是一个核心问题。我们希望将单词通过某种嵌入的形式表示，以捕获词的含义(meaning)以及词和词之间的关系(relationship)。一个解决方法是，使用wordnet(a thesaurus containing lists of synonym sets and hypernyms)，如下所示：\n然而，wordnet存在着许多问题，例如新词汇含义的缺失、上下文无关以及对人类劳动力的需求过大等。\n另一种想法是将单词用one-hot向量来表示。假设词汇表大小为$|V|$，则每个单词所对应的向量如下所示： $$\rw^{\\text {aardvark}}=\\left[\\begin{array}{c}1 \\\\0 \\\\0 \\\\\\vdots \\\\0\\end{array}\\right], w^{a}=\\left[\\begin{array}{c}0 \\\\1 \\\\0 \\\\\\vdots \\\\0\\end{array}\\right], w^{a t}=\\left[\\begin{array}{c}0 \\\\0 \\\\1 \\\\\\vdots \\\\0\\end{array}\\right], \\cdots w^{z e b r a}=\\left[\\begin{array}{c}0 \\\\0 \\\\0 \\\\\\vdots \\\\1\\end{array}\\right]\r$$ 即每个向量中只有一个位置为1，其余位置为0，为1的位置的下标对应单词的id。这样的词表示存在两个问题：(1) 没有给出两个单词之间的相似性，即任何两个向量的余弦相似度为0，这样会丢失语义信息；(2) 当词汇表很大时，每个单词对应的向量是一个高维稀疏向量。\n因此，或许可以尝试降低维度，使用一个子空间(维度为$N$，$N\\ll |V|$)来表达单词。在传统机器学习方法中，可以利用**奇异值分解(single value decomposition, SVD)**实现该过程。\n首先遍历一个很大的数据集，统计词的共现计数矩阵$X$，然后对矩阵$X$进行奇异值分解得到$USV^\\text T$，然后我们使用矩阵$U$的行来作为字典中所有词的词向量。$X$矩阵有两种选择的方式：(1) 统计每个单词在每个文档中出现的次数，生成单词-文本矩阵(word-document …","date":1603238400,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":9600,"html":"本文主要介绍了词向量的相关内容，包括词向量的概述、CBOW和skip-gram模型、word2vec的优化。","keywords":null,"kind":"page","lang":"en","lastmod":1603238400,"objectID":"df1cf39d6e91bb0e498ab57317f0be53","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/","publishdate":"2020-10-21T00:00:00Z","readingtime":20,"relpermalink":"/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理：词向量","type":"post","url":"/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/","weight":0,"wordcount":9595},{"author":null,"categories":["Quant"],"content":"自然语言与编程语言 自然语言处理(natural language processing, NLP) 是一门融合了计算机科学、人工智能以及语言学的交叉学科(interdisciplinary field)。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现最终目标：理解人类语言或人工智能。\n自然语言处理同义词：注重语言学结构的学者喜欢使用计算语言学(conputational linguistics)，而强调最终目的的学者则更偏好自然语言理解(natural language understanding) 这个术语。\n完全理解和表达语言是极其困难的，正确的语言表达也没有精确并完备的特性。\n虽然自然语言和编程语言都称为“语言”，但二者有极大差异。自然语言和编程语言之间的不同：\n(1) 词汇量：编程语言的关键词数量有限且确定，而自然语言的词汇量是无尽的、可创造的。\n(2) 结构化：自然语言是非结构化的，而编程语言是结构化的。\n(3) 歧义性：自然语言含有大量歧义，这些歧义根据语境的不同而表现为特定的义项。语言中的歧义问题是自然语言难以处理的原因。\n(4) 容错性：自然语言具有很高的容错性，人们可以猜出有错的句子的含义；编程语言的拼写必须保证绝对正确，语法必须保证绝对规范。\n(5) 易变性：编程语言的变化缓慢温和，而自然语言的变化迅速嘈杂。\n(6) 简略性：人类语言往往简洁干练，我们经常省略大量背景知识或常识。\n自然语言处理的任务 (1) 语音、图像和文本的处理\n自然语言处理系统的输入源有语音、图像和文本。其中文本占主要地位，语音和图像受制于存储容量和传输速度的限制，它们的信息总量不如文本多。通常进行**语音识别(speech recognition)将语音转化为文本，使用光学字符识别(optical character recognition, OCR)**将图像转化为文本。\n(2) 词法分析\n这三个任务都是围绕词语进行的分析，所以统称词法分析(lexical analysis)。词法分析的主要任务是：将文本分隔为有意义的词语，即分词(segmentation)；确定每个词语的类别和浅层的歧义消除，即词性标注(part-of-speech tagging)；并且识别出一些较长的专有名词，即命名实体识别(named entity …","date":1603152e3,"dir":"post/5-NLP/","expirydate":-62135596800,"fuzzywordcount":9400,"html":"本文主要介绍了自然语言处理的相关知识。","keywords":null,"kind":"page","lang":"en","lastmod":1603152e3,"objectID":"31eb0c2cfdfe1cb3886901bd7bc7ff0a","permalink":"https://xiangdiwu.github.io/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/","publishdate":"2020-10-20T00:00:00Z","readingtime":19,"relpermalink":"/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/","section":"post","tags":["Quant","Model","NLP"],"title":"自然语言处理概述","type":"post","url":"/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/","weight":0,"wordcount":9329},{"author":null,"categories":["Quant"],"content":"图神经网络概述 近年来，深度学习领域关于图神经网络(graph neural network, GNN) 的研究热情日益高涨，图神经网络已经成为各大深度学习顶会的研究热点。GNN处理非结构化数据时的出色能力使其在网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题方面都取得了新的突破。\n随着机器学习、深度学习的发展，语音、图像、自然语言处理逐渐取得了很大的突破，然而语音、图像、文本都是很简单的序列或者网格数据，是很结构化的数据，深度学习很善于处理该种类型的数据。然而，现实世界中并不是所有的事物都可以表示成一个序列或者一个网格，例如社交网络、知识图谱、论文引用网络等，也就是说很多事物都是非结构化的。非结构化的数据可以使用 图(graph) 这种数据结构进行表示，其特点包括：\n(1) 图的大小是任意的，图的拓扑结构复杂，没有像图像一样的空间局部性；\n(2) 图没有固定的节点顺序，或者说没有一个参考节点；\n(3) 图经常是动态图，而且包含多模态的特征。\n图神经网络是对图进行表示，并将深度学习扩展使得能够建模问题和数据，以完成各类基于图的任务的神经网络结构。其所能够完成的常见任务包括结点分类、社区发现以及链接预测等。\n图神经网络中的矩阵 以如下所示的图结构为例：\n一个图$G$的定义为$G=(V, E)$，其中$V$为结点集，结点数量$m = |V|$；$E$为边集，边数量$n = |E|$。通常用一些矩阵来刻画图结构：\n(1) 邻接矩阵$\\boldsymbol A$：结点相邻接的位置标为1，其余位置为0的矩阵。无向图的邻接矩阵为对称矩阵。 $$\r\\boldsymbol A=\\left[\\begin{array}{llllll}\r0 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\\r1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \\\\\r1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\r\\end{array}\\right]\r$$ (2) 度数矩阵$\\boldsymbol D$：对角矩阵，标识每个结点的度。 $$\r\\boldsymbol D=\\left[\\begin{array}{llllll}\r3 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 …","date":1603065600,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":4900,"html":"本文主要介绍了图神经网络的相关内容，包括图神经网络的概述、图神经网络中的矩阵、图卷积神经网络、Text Graph Convolutional Networks。","keywords":null,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"8d2ab9598055a492da56252b091fe805","permalink":"https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","publishdate":"2020-10-19T00:00:00Z","readingtime":10,"relpermalink":"/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：图神经网络","type":"post","url":"/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","weight":0,"wordcount":4894},{"author":null,"categories":["Quant"],"content":"根据通用近似定理，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。\n为了减少计算复杂度，通过部分借鉴生物神经网络的一些机制，我们引入了局部连接、权重共享以及汇聚操作来简化神经网络结构。虽然这些机制可以有效缓解模型的复杂度和表达能力之间的矛盾，但是我们依然希望在不过度增加模型复杂度(主要是模型参数)的情况下来提高模型的表达能力。\n神经网络中可以存储的信息量称为网络容量(network capacity)。一般来讲，利用一组神经元来存储信息时，其存储容量和神经元的数量以及网络的复杂度成正比。如果要存储越多的信息，神经元数量就要越多或者网络要越复杂，进而导致神经网络的参数成倍地增加。\n人脑的生物神经网络同样存在网络容量问题，工作记忆大概只有几秒钟的时间，类似于循环神经网络中的隐状态。而人脑每个时刻接收的外界输入信息非常多，包括来自于视觉、听觉、触觉的各种各样的信息。单就视觉来说，眼睛每秒钟都会发送千万比特的信息给视觉神经系统。人脑在有限的资源下，并不能同时处理这些过载的输入信息。大脑神经系统有两个重要机制可以解决信息过载问题：注意力(attention) 和记忆(memory) 机制。注意力机制通过自上而下的信息选择机制来过滤掉大量的无关信息；记忆机制引入额外外部记忆，优化神经网络的记忆结构来提高神经网络存储信息的容量。\n注意力 认知神经学中的注意力 注意力是一种人类不可或缺的复杂认知功能，指人可以在关注一些信息的同时忽略另一些信息的选择能力。在日常生活中，我们通过视觉、听觉、触觉等方式接收大量的感觉输入。但是人脑还能在这些外界的信息轰炸中有条不紊地工作，是因为人脑可以有意或无意地从这些大量输入信息中选择小部分的有用信息来重点处理，并忽略其他信息。这种能力就叫做注意力。注意力可以作用在外部的刺激(听觉、视觉、味觉等)，也可以作用在内部的意识(思考、回忆等)。注意力一般分为两种：\n一种是自上而下的有意识的注意力，称为聚焦式注意力(focus attention)。聚焦式注意力是指有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注意力。\n另一种是自下而上的无意识的注意力，称为基于显著性的注意 …","date":1602979200,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":8800,"html":"本文主要介绍了注意力机制和外部记忆的相关内容，包括注意力机制的原理、注意力机制的应用、注意力机制的变体、注意力机制的应用。","keywords":null,"kind":"page","lang":"en","lastmod":1602979200,"objectID":"41caa2377e5e6ca5a7b8a2499a242b95","permalink":"https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/","publishdate":"2020-10-18T00:00:00Z","readingtime":18,"relpermalink":"/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：注意力机制与外部记忆","type":"post","url":"/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/","weight":0,"wordcount":8775},{"author":null,"categories":["Quant"],"content":"概率生成模型 概率生成模型，简称生成模型(generative model)，是概率统计和机器学习中的一类重要模型，指一系列用于随机生成可观测数据的模型。假设在一个连续的或离散的高维空间$\\mathcal X$中，存在一个随机向量$\\boldsymbol X$服从一个未知的数据分布$p_r(\\boldsymbol x),\\boldsymbol x \\in \\mathcal X$。生成模型是根据一些可观测的样本$\\boldsymbol x^{(1)},\\boldsymbol x^{(2)},\\cdots,\\boldsymbol x^{(N)}$来学习一个参数化的模型$p_\\theta(\\boldsymbol x)$来近似未知分布$p_r(\\boldsymbol x)$，并可以用这个模型来生成一些样本，使得生成样本和真实样本尽可能地相似。\n自然情况下， 直接建模$p_r(\\boldsymbol x)$比较困难。深度生成模型就是利用深度神经网络可以近似任意函数的能力来建模一个复杂的分布$p_r(\\boldsymbol x)$。假设一个随机向量$\\boldsymbol Z$服从一个简单的分布$p(\\boldsymbol z),z \\in \\mathcal Z$(例如标准正态分布)，我们使用一个深度神经网络$g: \\mathcal Z \\rightarrow \\mathcal X$，并使得$g(\\boldsymbol z)$服从$p_r(\\boldsymbol x)$。\n生成模型一般具有两个功能：密度估计和样本生成。\n给定一组数据$\\mathcal D=\\{\\boldsymbol x^{(i)}\\}, 1 \\leqslant i \\leqslant N$，假设它们都是独立地匆匆相同的概率密度函数为$p_r(\\boldsymbol x)$的未知分布中产生的。概率密度估计(probabilistic density estimation) 是根据数据集$\\mathcal D$来估计其概率密度函数$p_\\theta(\\boldsymbol x)$。在机器学习中，概率密度估计是一种非常典型的无监督学习问题。如果要建模的分布包含隐变量(如高斯混合模型)，就需要利用EM算法来进行密度估计。\n生成样本就是给定义一个概率密度函数为$p_\\theta(\\boldsymbol x)$的分布，生成 …","date":1602892800,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":5200,"html":"本文主要介绍深度生成模型，包括概率生成模型和生成式对抗网络。","keywords":null,"kind":"page","lang":"en","lastmod":1602892800,"objectID":"6b05f46189278f640374fcb5eee385d8","permalink":"https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-17T00:00:00Z","readingtime":11,"relpermalink":"/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：深度生成模型","type":"post","url":"/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":5196},{"author":null,"categories":["Quant"],"content":"自编码器 自编码器(auto-encoder, AE) 是通过无监督的方式来学习一组数据的有效编码(或表示)。\n假设有一组$d$维的样本$\\boldsymbol{x}^{(n)} \\in \\mathbb{R}^{d}, 1 \\leqslant n \\leqslant N$，自编码器将这组数据映射到$p$维的特征空间得到每个样本的编码$\\boldsymbol{z}^{(n)} \\in \\mathbb{R}^{p}, 1 \\leqslant n \\leqslant N$，并且希望这组编码可以重构出原来的样本。自编码器的结构可分为两部分：\n编码器(encoder)：$f:\\mathbb R^d \\rightarrow \\mathbb R^p$；\n解码器(decoder)：$g:\\mathbb R^p \\rightarrow \\mathbb R^d$。\n自编码器的学习目标是最小化重构错误(reconstruction error)： $$\r\\begin{aligned}\r\\mathcal{L} \u0026amp;=\\sum_{n=1}^{N}\\|\\boldsymbol{x}^{(n)}-g(f(\\boldsymbol{x}^{(n)}))\\|^{2} \\\\\r\u0026amp;=\\sum_{n=1}^{N}\\|\\boldsymbol{x}^{(n)}-f \\circ g(\\boldsymbol{x}^{(n)})\\|^{2}\r\\end{aligned}\r$$ 如果特征空间的维度$p$小于原始空间的维度$d$，自编码器相当于是一种降维或特征抽取方法。如果$p \\geqslant d$，一定可以找到一组或多组解使得$f \\circ g$为单位函数(identity function)$I(x)=x$，并使得重构错误为0。然而，这样的解并没有太多的意义。但是，如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的稀疏性、取值范围，$f$和$g$的具体形式等。如果我们让编码只能取个$k$不同的值($k \u0026lt; N$)，那么自编码器就成为一个$k$类的聚类问题。\n最简单的自编码器是如下图所示的两层神经网络。输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。\n我们使用自编码器是为了得到有效的数据表示，因此在训练结束后，一般会去掉解码器，只保留编码器。编码器的输出可以直接作为后续机器学习模型的输 …","date":1602806400,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":2500,"html":"本文主要介绍自编码器的基本原理和Tensorflow实现。","keywords":null,"kind":"page","lang":"en","lastmod":1602806400,"objectID":"c71c624f6e293a222b72cc5b1f458cdc","permalink":"https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/","publishdate":"2020-10-16T00:00:00Z","readingtime":5,"relpermalink":"/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：自编码器","type":"post","url":"/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/","weight":0,"wordcount":2439},{"author":null,"categories":["Quant"],"content":"网络优化的难点 网络结构多样性 神经网络的种类非常多，比如卷积网络、循环网络等，其结构也非常不同。有些比较深，有些比较宽。不同参数在网络中的作用也有很大的差异，比如连接权重和偏置的不同，以及循环网络中循环连接上的权重和其它权重的不同。由于网络结构的多样性，我们很难找到一种通用的优化方法。不同的优化方法在不同网络结构上的差异也都比较大。此外，网络的超参数一般也比较多，这也给优化带来很大的挑战。\n高维变量的非凸优化 低维空间的非凸优化问题主要是存在一些局部最优点。基于梯度下降的优化方法会陷入局部最优点，因此低维空间非凸优化的主要难点是如何选择初始化参数和逃离局部最优点。深度神经网络的参数非常多，其参数学习是在非常高维空间中的非凸优化问题，其挑战和在低维空间的非凸优化问题有所不同。在高维空间中，非凸优化的难点并不在于如何逃离局部最优点，而是如何逃离鞍点(saddle point)。鞍点的叫法是因为其形状像马鞍。鞍点的梯度是0，但是在一些维度上是最高点，在另一些维度上是最低点。\n在高维空间中，局部最优点要求在每一维度上都是最低点，这种概率非常低。假设网络有10,000维参数，一个点在某一维上是局部最低点的概率为$p$，那么在整个参数空间中，局部最优点的概率为$p^{10,000}$，这种可能性非常小。也就是说高维空间中，大部分梯度为0的点都是鞍点。基于梯度下降的优化方法会在鞍点附近接近于停滞，同样很难从这些鞍点中逃离。\n深度神经网络的参数非常多，并且冗余性很高，这使得每个参数对最终损失的影响都比较小，这导致了损失函数在局部最优点附近是一个平坦的区域，称为平坦最小值(flat minima)。\n目前，深度神经网络的参数学习主要是通过梯度下降法来寻找一组可以最小化结构风险的参数。在具体实现中，梯度下降法可以分为批量梯度下降、随机梯度下降以及小批量梯度下降三种形式。除了在收敛效果和效率上的差异，这三种方法都存在一些共同的问题，比如如何改进优化算法、如何初始化参数、如何预处理数据等。\n优化算法 小批量梯度下降 在训练深度神经网络时，训练数据的规模通常都比较大。如果在梯度下降时，每次迭代都要计算整个训练数据上的梯度，这就需要比较多的计算资源。另外大规模训练集中的数据通常会非常冗余，也没有必要在整个训练集上计算梯度。因此，在训练深度神经网络时，经常使用小批量梯度下降 …","date":160272e4,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":14e3,"html":"本文主要介绍神经网络的优化，包括网络结构多样性、高维变量的非凸优化、优化算法、参数初始化等。","keywords":null,"kind":"page","lang":"en","lastmod":160272e4,"objectID":"2230e3f5c56d452d033360b00227de7a","permalink":"https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/","publishdate":"2020-10-15T00:00:00Z","readingtime":28,"relpermalink":"/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：神经网络的优化","type":"post","url":"/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/","weight":0,"wordcount":13997},{"author":null,"categories":["Quant"],"content":"序列数据和语言模型 全连接神经网络和卷积神经网络只能单独处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(recurrent neural network)。\nRNN是在自然语言处理领域中最先被用起来的，比如，RNN可以为语言模型来建模。例如我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：我昨天上学迟到了，老师批评了__。\n语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。\n使用RNN之前，语言模型主要是采用N-gram语言模型。N可以是一个自然数，它的含义是，假设一个词出现的概率只与前面N个词相关。N-gram模型的缺陷是，当处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-gram模型会占用海量的存储空间。\n与传统语言模型不同的是，循环神经网络理论上可以往前看(往后看)任意多个词。\n网络结构 给定一个输入序列\n$$\\boldsymbol x_{1:T}=(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\ldots, \\boldsymbol{x}_{t}, \\cdots, \\boldsymbol{x}_{T})$$，RNN通过下面公式更新带反馈边的隐藏层的活性值$\\boldsymbol h_t$： $$\r\\boldsymbol{h}_{t}=f(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_{t})\r$$ 其中$\\boldsymbol h_0=0$(即RNN隐藏层的初始值为0)，$f(·)$为一个非线性函数，也可以是一个前馈网络。\n循环层的基础结构如上图所示。循环神经网络可以看成一个 …","date":1602633600,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":7500,"html":"本文主要介绍循环神经网络，包括循环神经网络的应用、训练、长程依赖问题，以及循环神经网络的变体，如LSTM、GRU等。","keywords":null,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"e2eaa15d9e29bbd351718e537b8f49dd","permalink":"https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","publishdate":"2020-10-14T00:00:00Z","readingtime":15,"relpermalink":"/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：循环神经网络","type":"post","url":"/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","weight":0,"wordcount":7459},{"author":null,"categories":["Quant"],"content":"卷积神经网络(convolutional neural network, CNN) 是一种具有局部连接、权重共享等特性的前馈神经网络。\n卷积神经网络最早是主要用来处理图像信息。在用全连接前馈网络来处理图像时，会存在参数太多、局部特征不变形等缺陷。卷积神经网络利用局部连接、权重共享以及汇聚三大结构上的特性，使得数据具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。\n卷积神经网络主要使用在图像和视频分析的各种任务(如图像分类、人脸识别、物体识别、图像分割等)上，其准确率一般也远远超出了其它的神经网络模型。近年来卷积神经网络也广泛地应用到自然语言处理、推荐系统等领域。\n卷积 一维卷积 卷积(convolution) 操作是分析数学中一种重要的运算，在信号处理或图像处理领域应用广泛。一维卷积经常用在信号处理中，用于计算信号的延迟累积。假设一个信号发生器每个时刻$t$产生一个信号$x_t$，其信息的衰减率为$w_k$，即在$k-1$个时间步长后，信息为原来的$w_k$倍。假设$w_1=1,w_2=1/2,w_3=1/4$，那么在时刻$t$收到的信号$y_t$为当前时刻产生的信息和以前时刻延迟信息的叠加，即： $$\r\\begin{aligned}\ry_{t} \u0026amp;=1 \\times x_{t}+1 / 2 \\times x_{t-1}+1 / 4 \\times x_{t-2} \\\\\r\u0026amp;=w_{1} \\times x_{t}+w_{2} \\times x_{t-1}+w_{3} \\times x_{t-2} \\\\\r\u0026amp;=\\sum_{k=1}^{3} w_{k} \\cdot x_{t-k+1}\r\\end{aligned}\r$$ 我们将$w_1,w_2,\\cdots$称为滤波器(filter)或卷积核(convolution kernel)。假设滤波器长度为$m$，它和一个信号序列$x_1,x_2,\\cdots$的卷积运算可以写为： $$\ry_{t}=\\sum_{k=1}^{m} w_{k} \\cdot x_{t-k+1}\r$$ 写作向量形式：$\\boldsymbol{y}=\\boldsymbol{w} \\otimes \\boldsymbol{x}$，其中$\\otimes$表示卷积运算。当滤波器$w_k=1/m,1\\leqslant k …","date":1602547200,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":8200,"html":"本文主要介绍神经网络，包括神经网络的基本结构、激活函数、前馈神经网络、反向传播算法、卷积神经网络、循环神经网络、图神经网络等。","keywords":null,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"4cfa47486a1be29e06f9020867fdbf34","permalink":"https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","publishdate":"2020-10-13T00:00:00Z","readingtime":17,"relpermalink":"/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：卷积神经网络","type":"post","url":"/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","weight":0,"wordcount":8140},{"author":null,"categories":["Quant"],"content":"人工神经网络(artificial neural network, ANN) 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象，构建人工神经元，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为神经网络(neural network, NN)。\n神经网络最早是作为一种主要的连接主义模型。20世纪80年代中后期，最流行的一种连接主义模型是分布式并行处理(parallel distributed processing, PDP)模型，其有3个主要特性：(1) 信息表示是分布式的(非局部的)；(2) 记忆和知识是存储在单元之间的连接上；(3)通过逐渐改变单元之间的连接强度来学习新的知识。\n连接主义的神经网络有着多种多样的网络结构以及学习方法，虽然早期模型强调模型的生物可解释性(biological plausibility)，但后期更关注于对某种特定认知能力的模拟，比如物体识别、语言理解等. 尤其在引入误差反向传播来改进其学习能力之后，神经网络也越来越多地应用在各种机器学习任务上。随着训练数据的增多以及(并行)计算能力的增强，神经网络在很多机器学习任务上已经取得了很大的突破，特别是在语音、图像等感知信号的处理任务上，神经网络表现出了卓越的学习能力。\n目前受到关注的是采用误差反向传播来进行学习的神经网络，即作为一种机器学习模型的神经网络。从机器学习的角度来看，神经网络一般可以看作是一个非线性模型，其基本组成单元为具有非线性激活函数的神经元，通过大量神经元之间的连接，使得神经网络成为一种高度非线性的模型。神经元之间的连接权重就是需要学习的参数，可以在机器学习的框架下通过梯度下降方法来进行学习。\n神经网络的基本结构 神经网络的基本组成单位是神经元(neuron)，如下所示：\n假设一个神经元接收$D$个输入$x_1,x_2,\\cdots,x_D$，令向量$\\boldsymbol{x}=\\left[x_{1} ; x_{2} ; \\cdots ; x_{D}\\right]$表示这组输入，并用净输入(net input)$z\\in \\mathbb R$表示一个神经元所获得的输入信号的加权和，则神经元的前向(forward)运算过程如下： $$\r\\begin{aligned}\rz …","date":1602460800,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":11600,"html":"本文主要介绍神经网络，包括神经网络的基本结构、激活函数、前馈神经网络、反向传播算法、卷积神经网络、循环神经网络、图神经网络等。","keywords":null,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"cbcb1136c70e738f93885ebbeeccd69d","permalink":"https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","publishdate":"2020-10-12T00:00:00Z","readingtime":24,"relpermalink":"/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习：神经网络","type":"post","url":"/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","weight":0,"wordcount":11524},{"author":null,"categories":["Quant"],"content":"表示学习 为了提高机器学习系统的准确率，需要将输入信息转换为有效的特征，或者更一般称为表示(representation)。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就是可以叫做表示学习(representation learning)。\n表示学习的关键是解决语义鸿沟(semantic gap) 问题。语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，不同图片在像素级别上的表示(即底层特征)差异性也会非常大。但是我们人理解这些图片是建立在比较抽象的高层语义概念上的。如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高。如果可以有一个好的表示在某种程度上可以反映出数据的高层语义特征，那么我们就可以相对容易地构建后续的机器学习模型。\n在表示学习中，有两个核心问题：一是“什么是一个好的表示”；二是“如何学习到好的表示”。\n局部表示和分布式表示 一般而言，一个好的表示具有以下几个优点：\n(1) 一个好的表示应该具有很强的表示能力，即同样大小的向量可以表示更多信息。\n(2) 一个好的表示应该使后续的学习任务变得简单，即需要包含更高层的语义信息。\n(3) 一个好的表示应该具有一般性，是任务或领域独立的。\n在机器学习中，我们经常使用两种方式来表示特征：局部表示(local representation)和分布式表示(distributed representation)。以颜色的表示为例，二者之间的区别如下：\n局部表示通常可以表示为one-hot向量的形式(只有一个维度为1，其他维度为0)。局部表示具有很好的解释性，方便人工进行特征总结，并通过特征组合进行高效的特征工程。通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高。但局部表示有两个不足之处：(1) one-hot向量的维数很高，且无法扩展。如果有一种新的颜色，就需要增加一维来表示；(2) 不同颜色之间的相似度为0，这样丢失了所有语义信息。\n和局部表示相比，分布式表示的表示能力要比局部表示强很多。分布式表示的向量维度一般都比较低，我们只需要用一个三维的稠密向量(例如RGB)就可以表示所有颜色。并且分布式表示也很容易表示新的颜色名。此外，不 …","date":1602374400,"dir":"post/4-DL/","expirydate":-62135596800,"fuzzywordcount":8e3,"html":"本文主要介绍深度学习的基本概念和发展历程。","keywords":null,"kind":"page","lang":"en","lastmod":1602374400,"objectID":"4aeb1bf06b99901323dd0f15d4d2419d","permalink":"https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","publishdate":"2020-10-11T00:00:00Z","readingtime":16,"relpermalink":"/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","section":"post","tags":["Quant","Model","Deep Learning"],"title":"深度学习概述","type":"post","url":"/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","weight":0,"wordcount":7957},{"author":null,"categories":["Quant"],"content":"概率图模型概述 概率图模型(probabilistic graphical model, PGM)，简称图模型(graphical model, GM)，是指一种用图结构来描述多元随机变量之间条件独立关系的概率模型，从而给研究高维空间中的概率模型带来了很大的便捷性。\n对于一个$K$维随机向量$\\boldsymbol X=[X_1,X_2,\\cdots,X_K]^\\text T$，其联合概率为高维空间中的分布，一般难以直接建模。假设每个变量为离散变量并有$m$个取值，在不作任何独立假设条件下，则需要个$m^K-1$参数才能表示其概率分布，当$m$和$K$很大时，参数量远远超出了目前计算机的存储能力。\n一种有效减少参数量的方法是独立性假设。一个$K$维随机向量$\\boldsymbol X$的联合概率分解为$K$个条件概率的乘积： $$\r\\begin{aligned}\rp(\\boldsymbol{x}) \u0026amp; \\triangleq P(\\boldsymbol{X}=\\boldsymbol{x}) \\\\\r\u0026amp;=p\\left(x_{1}\\right) p\\left(x_{2} \\mid x_{1}\\right) \\cdots p\\left(x_{K} \\mid x_{1}, \\cdots, x_{K-1}\\right) \\\\\r\u0026amp;=\\prod_{k=1}^{K} p\\left(x_{k} \\mid x_{1}, \\cdots, x_{k-1}\\right)\r\\end{aligned}\r$$ 其中$x_k$表示变量$X_k$的取值。如果某些变量之间存在条件独立，其参数量就可以大幅减少。\n假设有四个二值变量$X_1,X_2,X_3,X_4$，在不知道这几个变量依赖关系的情况下，可以用一个联合概率表来记录每一种取值的概率，共需要15个参数。假设在已知$X_1$时，$X_2$和$X_3$独立，即有 $$\rp(x_{2} | x_{1}, x_{3})=p(x_{2} | x_{1}) \\\\\rp(x_{3} | x_{1}, x_{2})=p(x_{3} | x_{1})\r$$ 在已知$X_2$和$X_3$时，$X_4$也和$X_1$独立，即有 $$\rp(x_{4} | x_{1}, x_{2}, x_{3})=p(x_{4} | x_{2}, x_{3})\r$$ 那么其联合概 …","date":1602288e3,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":10100,"html":"本文主要介绍概率图模型，包括概率图模型概述、模型表示、学习、推断。","keywords":null,"kind":"page","lang":"en","lastmod":1602288e3,"objectID":"be5f0175989efe9d4ef944329e65741d","permalink":"https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-10T00:00:00Z","readingtime":21,"relpermalink":"/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：概率图模型","type":"post","url":"/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":10032},{"author":null,"categories":["Quant"],"content":"单词向量空间与话题向量空间 潜在语义分析(latent semantic analysis, LSA) 是一种无监督学习方法，主要用于文本的话题分析，其特点是通过矩阵分解发现文本与单词之间基于话题的语义关系。\n文本信息处理中，传统的方法以单词向量表示文本的语义内容，以单词向量空间的度量表示文本之间的语义相似度。潜在语义分析旨在解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现潜在的话题，以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度。这也是话题分析(topic modeling)的基本想法。\nLSA首先将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解，从而得到话题向量空间，以及文本在话题向量空间的表示。\n单词向量空间 信息检索、文本数据挖掘等领域的一个核心问题是对文本的语义内容进行表示，并进行文本之间的语义相似度计算。最简单的办法是向量空间模型(vector space model)：用一个向量表示某一文本的“语义”，向量的每一维对应一个单词，其数值为单词在文本中出现的频数或权值。如此一来，文本集合中的每个文本都表示为一个向量，存在于一个向量空间，文本间的内积表示文本相似度。\n严格定义：给定一个含有$n$个文本的集合$D=\\{d_1, d2, \\cdots , d_n\\}$，以及在所有文本中出现的$m$个单词的集合$W=\\{w_1, w_2, \\cdots , w_m\\}$。单词-文本矩阵$X$定义为： $$\rX=\\left[\\begin{array}{cccc}\rx_{11} \u0026amp; x_{12} \u0026amp; \\cdots \u0026amp; x_{1 n} \\\\\rx_{21} \u0026amp; x_{22} \u0026amp; \\cdots \u0026amp; x_{2 n} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\rx_{m 1} \u0026amp; x_{m 2} \u0026amp; \\cdots \u0026amp; x_{m n}\r\\end{array}\\right]\r$$ 其中$x_{ij}$表示单词$w_i$在文本$d_j$中再出现的频数或权值，矩阵的一列对应一个文本。由于单词的种类很多，而每个文本中出现单词的种类通常较少，因此该矩阵是一个稀疏矩阵。\n权值通常用单词频率-逆文本频率TF-IDF表示，其定义是： $$\rT F_{w}=\\frac{\\text {给定词} w \\text {出 …","date":1602201600,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":6600,"html":"本文主要介绍话题模型，包括单词向量空间与话题向量空间，以及潜在语义分析算法、概率潜在语义分析模型、潜在狄利克雷分配。","keywords":null,"kind":"page","lang":"en","lastmod":1602201600,"objectID":"b4732fb1728a11c62964d47a1ffd4224","permalink":"https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-09T00:00:00Z","readingtime":14,"relpermalink":"/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：话题模型","type":"post","url":"/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":6563},{"author":null,"categories":["Quant"],"content":"子集搜索与评价 给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。对当前学习任务有用的属性称为相关特征(relevant feature)，无用的属性称为无关特征(irrelevant feature)。特征选择(feature selection) 是一个重要的数据预处理过程，指从给定的特征集合中选择出相关特征的子集的过程。特征选择能够减少维数灾难问题，同时降低学习任务的难度。\n如果我们想从特征集合中选区一个包含了所有重要信息的特征子集，如果我们没有任何先验知识，那么就只能遍历可能的子集，这会带来很高的计算复杂度；可行的一种做法是先产生一个“候选子集\u0026amp;quot;，评价它的好坏，然后在此基础之上产生下一个子集。这里涉及到两个关键环节，一是如何根据评价结果选择下一个候选子集，二是如何评价子集的好坏。\n第一个环节是子集搜索(subset search)，给定特征集合$\\{a_1,a_2,\\cdots,a_d\\}$，可以将每个单独的特征看作一个候选子集，对这$d$个候选单特征子集进行评定，假如$\\{a_2\\}$最优，那么我们就把$\\{a_2\\}$作为第一轮的选定集；然后，在上一轮选定集中加入一个特征，构成两个特征的候选子集，我们假定$\\{a_2,a_4\\}$最优且优于$\\{a_2\\}$，那么我们就得到了新的候选子集，以此类推。这是一种前向搜索。我们也可以从完整的属性集合开始，每次去掉一个无关特征，这样的方法叫后向搜索。还可以将前向和后向搜索结合起来进行双向搜索。显然，该策略是贪心的。\n第二个环节是子集评价(subset evaluation)。给定数据集$D$，假定$D$中第$i$类样本所占比例为$p_i$。假设属性均为离散型，对属性子集$A$，根据其取值将$D$分为$V$个子集$\\{D^1,D^2,\\cdots,D^V\\}$，每个子集中的样本在$A$上取值相同，于是我们可以计算属性子集$A$的信息增益： $$\r\\operatorname{Gain}(A)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)\r$$ 其中信息熵的定义为： $$\r\\operatorname{Ent}(D)=- …","date":1602115200,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":3600,"html":"本文主要介绍特征选择，包括特征选择的基本原理、常见的特征选择方法以及scikit-learn实现。","keywords":null,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"ca597ee805a8d42878eca80dd345f448","permalink":"https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","publishdate":"2020-10-08T00:00:00Z","readingtime":8,"relpermalink":"/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：特征选择","type":"post","url":"/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","weight":0,"wordcount":3531},{"author":null,"categories":["Quant"],"content":"聚类的基本概念 在无监督学习(unsupervised learning) 中，训练样本的标记信息是未知的，目标是通过对无标记样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。聚类(clustering)任务是一种常见的无监督学习方法。\n聚类试图将数据集中的样本划分为若干个不相交的子集，每个子集称为一个簇(cluster)。通过这样的划分，每个簇可能对应于一些潜在的概念(类别)。这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名。\n聚类既能作为一个单独过程，用于寻找数据内在的分布结构，也可作为分类等其他学习任务的先驱过程。例如在一些商业应用中需对新用户的类型进行判别，但定义用户类型对商家来说却可能不太容易，此时往往可先对已有用户数据进行聚类，根据聚类结果将每个簇定义为一个类，然后再基于这些类训练分类模型，用于判别新用户的类型。\n相似度或距离 剧烈的核心概念是相似度(similarity)或距离(distance)。有多种相似度或距离的定义。因为相似度直接影响聚类的结果，所以其选择是聚类的根本问题。具体哪种相似度更合适取决于应用问题的特性。\n(1) 闵可夫斯基距离(Minkowski distance)： $$\r\\operatorname{dist}_{m k}(x_{i}, x_{j})=(\\sum_{u=1}^{n}|x_{i u}-x_{j u}|^{p})^{\\frac{1}{p}}\r$$ 当$p=2$时，称为欧氏距离(Euclidean distance)；当$p=1$时称为曼哈顿距离(Manhattan distance)；当$p=\\infty$时称为切比雪夫距离(Chebyshev distance)，即取各个坐标数值差的绝对值的最大值。\n(2) 马哈拉诺比斯距离(Mahalanobis distance)：简称马氏距离，也是一种常用的相似度，考虑各个分量(特征)之间的相关性并与各个分量的尺度无关。给定一个样本集合$X=(x_{ij})_{m \\times n}$，其协方差矩阵记作$S$。样本$x_i$与样本$x_j$之间的马哈拉诺比斯距离$d_{ij}$定义为$d_{ij}=[(x_i-x_j)^{\\text T}S^{-1}(x_i-x_j)]^{\\frac{1}{2}}$。 …","date":1602028800,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":4900,"html":"本文主要介绍聚类，包括聚类的基本概念、聚类算法、聚类算法的拓展。","keywords":null,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"425d3b67af3cd967e257e08e8d2f0173","permalink":"https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/","publishdate":"2020-10-07T00:00:00Z","readingtime":10,"relpermalink":"/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：聚类","type":"post","url":"/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/","weight":0,"wordcount":4862},{"author":null,"categories":["Quant"],"content":"主成分分析 主成分分析(principal component analysis, PCA) 是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如下图所示的二维数据，如果将这些数据投影到一维空间，选择数据方差最大的方向进行投影(蓝轴)，才能最大化数据的差异性，保留更多的原始数据信息。\n假设有一组$d$维样本$\\boldsymbol x^{(n)} \\in \\mathbb R^d, 1 \\leqslant n \\leqslant N$，我们希望将其投影到一维空间中，投影向量为$\\boldsymbol w \\in \\mathbb R^d$。不失一般性，限制$\\boldsymbol w$的模为1，即$\\boldsymbol w^\\text T \\boldsymbol w=1$。每个样本点$\\boldsymbol x^{(n)}$投影之后的表示为$z^{(n)}=\\boldsymbol w^\\text T \\boldsymbol x^{(n)}$。\n用矩阵$X=\\left[\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}, \\cdots, \\boldsymbol{x}^{(N)}\\right]$表示输入样本，$\\bar{\\boldsymbol{x}}=\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}$为原始样本点的中心店，所有样本投影后的方差为： $$\r\\begin{aligned}\r\\sigma(X ; \\boldsymbol{w}) \u0026amp;=\\frac{1}{N} \\sum_{n=1}^{N}(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(n)}-\\boldsymbol{w}^{\\mathrm{T}} \\bar{\\boldsymbol{x}})^{2} \\\\\r\u0026amp;=\\frac{1}{N}(\\boldsymbol{w}^{\\mathrm{T}} X-\\boldsymbol{w}^{\\mathrm{T}} \\bar{X})(\\boldsymbol{w}^{\\mathrm{T}} X-\\boldsymbol{w}^{\\mathrm{T}} \\bar{X})^{\\mathrm{T}} \\\\ …","date":1602028800,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":2900,"html":"本文主要介绍降维的相关算法，包括主成分分析、流形学习、t-SNE等。","keywords":null,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"2c4929e30233a72d7b8012e365e705bc","permalink":"https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/","publishdate":"2020-10-07T00:00:00Z","readingtime":6,"relpermalink":"/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：降维","type":"post","url":"/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/","weight":0,"wordcount":2855},{"author":null,"categories":["Quant"],"content":"EM算法的引入 概率模型有时既含观测变量，又含隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。我们仅讨论极大似然估计，极大后验概率估计与其类似。\n三硬币模型 假设有3枚硬币，分别记作$A, B, C$。这些硬币正面出现的概率分别是$\\pi, p, q$。进行如下投掷试验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反而选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复$n$次试验(这里$n=10$)，预测结果如下：1, 1, 0, 1, 0, 0, 1, 0, 1, 1。\n假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三枚硬币分别出现正面的概率，即三硬币模型的参数。\n三硬币模型可以写作： $$\r\\begin{aligned}\rP(y|\\theta) \u0026amp;=\\sum_{z} P(y, z|\\theta)=\\sum_{z} P(z|\\theta) P(y|z, \\theta) \\\\\r\u0026amp;=\\pi p^{y}(1-p)^{1-y}+(1-\\pi) q^{y}(1-q)^{1-y}\r\\end{aligned}\r$$ 其中，随机变量$y$是观测变量，表示一次试验观测的结果是0或1；随机变量$z$为隐变量，表示未观测到的硬币$A$的结果；$\\theta=(\\pi,p,q)$是模型参数。这一模型是以上数据的生成模型。注意，随机变量$y$的数据可以观测，随机变量$z$的数据不可观测。\nEM算法过程 将观测数据表示为$Y=(Y_1,Y_2,\\cdots,Y_n)^\\text T$，未观测数据表示为$Z=(Z_1,Z_2,\\cdots,Z_n)^\\text T$，则观测数据的似然函数为(下式可以理解为全概率公式)： $$\rP(Y|\\theta)=\\sum_Z P(Z|\\theta)P(Y|Z,\\theta)\r$$ 考虑到每次试验是独立的，则整个观测数据$Y$的似然函数为$Y$中所有独立重复试验的似然的乘积： $$\rP(Y|\\theta)=\\prod_{j=1}^{n}[\\pi …","date":1601942400,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":3700,"html":"本文主要介绍EM算法，包括EM算法的引入、EM算法过程、EM算法的Python实现。","keywords":null,"kind":"page","lang":"en","lastmod":1601942400,"objectID":"3125c8fc78270582e2b93b1f9c0f26db","permalink":"https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/","publishdate":"2020-10-06T00:00:00Z","readingtime":8,"relpermalink":"/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：EM算法","type":"post","url":"/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/","weight":0,"wordcount":3661},{"author":null,"categories":["Quant"],"content":"个体与集成 集成学习(ensemble learning) 通过构建并结合多个学习期来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning) 等。\n集成学习的思路如下：首先产生一组个体学习器(indicidual learner)，再用某种策略将它们结合起来。个体学习器(基学习器)通常由一个现有的学习算法(基学习算法)从训练数据中产生，例如决策树、BP神经网络等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，这样的集成是同质的(homogeneous)，同质集成中的个体学习器亦称为基学习器(base learner)。集成也可以包含不同类型的个体学习器，此时称其为异质的(heterogenous)，相应的个体学习器一般不称基学习器，而是称为组件学习器(component learner)。\n历史上，Kearns和Valiant首先提出了强可学习(strongly learnable)和弱可学习(weakly learnable) 的概念。他们指出：在概率近似正确(probably approximately correct, PAC) 学习的框架中，一个概念(一个类)，如果存在一个多项式的学习算法能够学习它，并且正确率很高，就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，就称这个概念是弱可学习的。后来Schapire证明强可学习和弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。集成学习便是研究如何将弱学习算法提升为强学习算法的理论。\n集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对弱学习器尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。但需注意的是，虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中，往往会使用比较强的学习器进行集成。\n在经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习要获得比最好的单一学习器更好的性能，个体学习器应“好而不同”，即个体学习器要同时具备准确性和多样性。\n根据个 …","date":1601856e3,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":7800,"html":"本文主要介绍集成学习的基本概念、基本思想、基本方法和基本流程。","keywords":null,"kind":"page","lang":"en","lastmod":1601856e3,"objectID":"afe34d29be20eb46062064aa04fb14c6","permalink":"https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","publishdate":"2020-10-05T00:00:00Z","readingtime":16,"relpermalink":"/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：集成学习","type":"post","url":"/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","weight":0,"wordcount":7737},{"author":null,"categories":["Quant"],"content":"决策树模型 决策树(decision tree) 是一种基本的分类与回归算法。决策树呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用训练好的决策模型进行分类。\n分类决策树模型是一种由结点(node)和有向边(directed edge) 组成的树形结构。结点有两种类型：内部节点(internal node)，表示一个特征或属性；叶结点(leaf node)，表示一个类。用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至到达叶结点。对吼将实例分到叶结点对应的类中。\n决策树可以看成一个if-then规则的集合，满足互斥并且完备的性质。决策树根结点到叶结点的每一条路径中，路径上的内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。\n决策树还表示给定特征条件下类的条件概率分布，这一分布定义在特征空间的一个划分(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region)，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么该条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分下单元的集合，$Y$取值于类的集合。各叶结点(单元)上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的一类去。\n决策树学习的本质是从训练数据集中归纳出一组分类规则。与训练集不相矛盾的决策树(即能对训练数据进行正确分类的决策树)可能有多个，也可能一个都没有。我们需要的是一个与训练数据矛盾较小的，同时具有很好的泛化能力的决策树。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间的类的条件概率模型有无穷多个，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预 …","date":1601769600,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":6900,"html":"本文主要介绍决策树的基本原理，包括决策树的生成和剪枝。","keywords":null,"kind":"page","lang":"en","lastmod":1601769600,"objectID":"527d70cef160c021e652d6bbf8f347e8","permalink":"https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/","publishdate":"2020-10-04T00:00:00Z","readingtime":14,"relpermalink":"/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：决策树","type":"post","url":"/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/","weight":0,"wordcount":6829},{"author":null,"categories":["Quant"],"content":"线性支持向量机与硬间隔最大化 支持向量机(support vector machine, SVM) 是一个经典的机器学习二分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强的优势。\n给定一个二分类数据集$\\mathcal{D}=\\{(\\boldsymbol{x}^{(n)}, y^{(n)})\\}_{n=1}^{N}$，其中$y_n \\in \\{+1,-1\\}$，如果两类样本是线性可分的，即存在一个超平面$\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0$将两类样本分开，对于每一类样本都有$y^{(n)}(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(n)}+b)\u0026gt;0$。\n数据集$\\mathcal D$中的每个样本$\\boldsymbol x^{(n)}$到分割超平面的距离为： $$\r\\gamma^{(n)}=\\frac{\\|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(n)}+b\\|}{\\|\\boldsymbol{w}\\|}=\\frac{y^{(n)}(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(n)}+b)}{\\|\\boldsymbol{w}\\|}\r$$ 定义整个数据集$\\mathcal D$中所有样本到分割超平面的最短距离为间隔(margin)，用$\\gamma$来表示： $$\r\\gamma=\\min _{n} \\gamma^{(n)}\r$$ 如果间隔$\\gamma$越大，其分割超平面对两个数据集的划分越稳定，不容易受噪声等因素影响。支持向量机的目标是寻找一个超平面$(\\boldsymbol w^*,b^*)$使得$\\gamma$最大，即： $$\r\\begin{aligned}\r\\underset{\\boldsymbol{w}, b}\\max \\ \\ \u0026amp; \\gamma \\\\\r\\text { s.t. } \\ \\ \u0026amp; \\frac{y^{(n)}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(n)}+b\\right)}{\\|\\boldsymbol{w}\\|} \\geq \\gamma, \\forall …","date":1601683200,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":3200,"html":"本文主要介绍支持向量机，包括线性支持向量机与硬间隔最大化、线性支持向量机与软间隔最大化、非线性支持向量机与核函数等。","keywords":null,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"83ca3435d556c1d63584850a9e220b0e","permalink":"https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","publishdate":"2020-10-03T00:00:00Z","readingtime":7,"relpermalink":"/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：支持向量机","type":"post","url":"/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","weight":0,"wordcount":3114},{"author":null,"categories":["Quant"],"content":"逻辑回归模型 逻辑回归(logistic regression, LR) 模型是一种处理二分类问题的线性模型。逻辑回归模型由logistic分布(logistic distribution)导出。设$X$是连续随机变量，$X$服从logistic分布是指$X$具有下列分布函数和密度函数： $$\r\\begin{aligned}\rF(x)\u0026amp;=P(X\\leqslant x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\\\\\rf(x)\u0026amp;=F\u0026#39;(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}\r\\end{aligned}\r$$ 式中，$\\mu$为未知参数，$\\gamma\u0026gt;0$为形状参数。其分布函数$F(x)$以点$(\\mu,\\frac 1 2)$为中心对称。\n二分类的逻辑回归模型是如下的条件概率分布： $$\r\\begin{aligned}\rP(Y=1|x)\u0026amp;=\\frac{\\exp(w\\cdot x+b)}{1+\\exp(w\\cdot x+b)}=\\frac{1}{1+e^{-(w \\cdot x+b)}}\\\\\rP(Y=0|x)\u0026amp;=\\frac{1}{1+\\exp(w\\cdot x+b)}=1-P(Y=1|x)\r\\end{aligned}\r$$ 二分类逻辑回归模型比较两个条件概率值$P(Y=1|x)$和$P(Y=0|x)$的大小，将实例$x$分到概率值较大的那一类。将$w$和$x$变为增广形式，可得： $$\r\\begin{aligned}\rP(Y=1|x)\u0026amp;=\\frac{\\exp(w\\cdot x)}{1+\\exp(w\\cdot x)}=\\frac{1}{1+e^{-(w \\cdot x)}}\\\\\rP(Y=0|x)\u0026amp;=\\frac{1}{1+\\exp(w\\cdot x)}=1-P(Y=1|x)\r\\end{aligned}\r$$ 将上式变形后得到： $$\r\\begin{aligned}\rw\\cdot x\u0026amp;=\\log \\frac{p(Y=1|x)}{1-p(Y=1|x)}\\\\\r\u0026amp;=\\log \\frac{p(Y=1|x)}{p(Y=0|x)}\r\\end{aligned}\r$$ 其中，$\\log$中的形式称为几率(odds)，指一个事件发生与不发生的比值。\n逻辑回归采用交叉熵(cross …","date":1601596800,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":4800,"html":"本文主要介绍逻辑回归与最大熵模型，包括逻辑回归模型、softmax回归模型、最大熵模型。","keywords":null,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"c804aef44d2b924b934a8463892c3eb8","permalink":"https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/","publishdate":"2020-10-02T00:00:00Z","readingtime":10,"relpermalink":"/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：逻辑回归与最大熵模型","type":"post","url":"/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":4788},{"author":null,"categories":["Quant"],"content":"感知机算法 感知机(perceptron) 由Frank Rosenblatt于1957年提出，是一种广泛使用的线性分类器。感知器可谓是最简单的人工神经网络，只有一个神经元，是对生物神经元的简单数学模拟，有与生物神经元相对应的部件，如权重(突触)、偏置(阈值)及激活函数(细胞体)，输出为+1或-1。\n感知机是一种二分类线性模型，其分类准则为$\\hat y=\\text{sgn}(\\boldsymbol w^\\text T \\boldsymbol x)$。给定$N$个样本的训练集$\\{(\\boldsymbol x^{(n)},y^{(n)})\\}_{n=1}^N$，其中$y^{(n)}\\in \\{+1,-1\\}$，感知机学习算法视图找到一组参数$\\boldsymbol w^*$，使得对于每个样本$\\{(\\boldsymbol x^{(n)},y^{(n)})\\}$有： $$\ry^{(n)} \\boldsymbol{w}^{* \\mathrm{T}} \\boldsymbol{x}^{(n)}\u0026gt;0, \\quad \\forall n \\in[1, N]\r$$ 只有当训练集是线性可分的情况下，上式才能满足。感知机的学习算法是一种错误驱动的在线学习算法。先初始化一个权重向量$\\boldsymbol w \\leftarrow \\boldsymbol 0$，然后每次分错一个样本$(\\boldsymbol x,y)$时，即$y \\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}\u0026lt;0$，就用该样本更新权重： $$\r\\boldsymbol{w} \\leftarrow \\boldsymbol{w}+y \\boldsymbol{x}\r$$ 可以在$y\\boldsymbol x$前加一个学习率。根据感知机的学习策略，可以反推出感知机在样本$(\\boldsymbol x,y)$上的损失函数为： $$\r\\mathcal{L}(\\boldsymbol{w} ; \\boldsymbol{x}, y)=\\max \\left(0,-y \\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}\\right)\r$$ 采用随机梯度下降，其每次更新的梯度为： $$\r\\frac{\\partial \\mathcal{L}(\\boldsymbol{w} ; …","date":1601510400,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":3e3,"html":"本文主要介绍感知机算法的原理和实现。","keywords":null,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"faf94a6ea61210b0818dce688c262e84","permalink":"https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/","publishdate":"2020-10-01T00:00:00Z","readingtime":6,"relpermalink":"/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：感知机","type":"post","url":"/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/","weight":0,"wordcount":2980},{"author":null,"categories":["Quant"],"content":"朴素贝叶斯的学习与分类 朴素贝叶斯(naive Bayse) 算法是基于贝叶斯定理与特征条件独立假设的分类方法。设输入空间$\\mathcal X \\subseteq \\mathbb R^n$为$n$维向量的集合，输出空间为类标记集合$\\mathcal Y={c_1,c_2,\\cdots,c_K}$。输入为特征向量$x \\in \\mathcal X$，输出为类标记$y \\in \\mathcal Y$。$P(X,Y)$是输入空间和输出空间上的随机变量$X$和$Y$的联合概率分布，训练数据集(含$N$个数据)由$P(X,Y)$独立同分布产生。朴素贝叶斯在数据集上学习联合概率分布$P(X,Y)$。具体地，先学习以下先验概率分布及条件概率分布：\n(1) 先验概率分布：$P(Y=c_k),k=1,2,\\cdots,K$。\n(2) 条件概率分布：$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\\cdots,K$。\n朴素贝叶斯对条件概率分布作了条件独立性假设： $$\rP(X=x | Y=c_{k})=P(X^{(1)}=x^{(1)}, \\ldots, X^{(n)}=x^{(n)} | Y=c_{k})=\\prod_{j=1}^{n} P(X^{(j)}=x^{(j)} | Y=c_{k})\r$$ 朴素贝叶斯算法实际上学习到生成数据的机制，属于生成模型。条件独立性假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯算法变得简单，但会牺牲一定的分类准确率。\n朴素贝叶斯分类时，对给定的输入$x$，通过学习到的模型计算后验概率，最后将后验概率最大的类别作为$x$的类输出(后验概率最大化)： $$\ry=f(x)=\\arg \\max _{c_{k}} P(Y=c_{k}) \\prod_{j=1}^{n} P(X^{(j)}=x^{(j)} | Y=c_{k})\r$$ 朴素贝叶斯将实例分到后验概率最大的类，这等价于期望风险最小化。假设使用0-1损失函数：\n$$\rL(Y, f(X))=\\left\\{\\begin{array}{ll}\r1, \u0026amp; Y \\neq f(X) \\\\\r0, \u0026amp; Y=f(X)\r\\end{array}\\right.\r$$ 式中的$f(X)$是分类决策函数。这时，期 …","date":1601510400,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":3e3,"html":"本文主要介绍贝叶斯分类器的基本原理，以及基于numpy和scikit-learn的实现。","keywords":null,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"4b0ae15eeaf59f54359c3f6eab8dc2a0","permalink":"https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","publishdate":"2020-10-01T00:00:00Z","readingtime":6,"relpermalink":"/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：贝叶斯分类器","type":"post","url":"/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","weight":0,"wordcount":2980},{"author":null,"categories":["Quant"],"content":"$\\boldsymbol k$近邻算法原理 $k$近邻($k$-nearest neighbor, $k$NN)算法是一种常用的监督学习方法。其基本思想为：给定一组数据，基于某种距离度量找出训练集中与其最靠近的$k$个训练样本，然后基于这$k$个邻居的信息来进行预测。通常，在分类任务中可使用投票法，即选择$k$个样本中出现最多的类别标记作为预测结果；在回归任务中可使用平均法，即将$k$个样本的实值输出标记的平均值作为预测结果。该算法属于“惰性学习(lazy learning)”方法之一，没有显式的学习过程。相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习(eager learning)”。\n$k$近邻算法的形式化表示如下： $$\ry=\\arg\\underset{c_j}\\max \\sum_{x_i\\in{N_k(x)}}\\mathbb I(y_i=c_j),i=1,2,\\cdots,N;j=1,2,\\cdots,K\r$$ 其中，$x_i$为实例特征向量，$y_i=\\{c_1,c_2\\cdots,c_k\\}$为实例的类别，$N$为实例总数，$N_k(x)$为$x$在训练集总最邻近的$k$个点，$\\mathbb I$为指示函数。\n$k$近邻算法的特殊情况是$k=1$的情形，称为最近邻算法。\n$\\boldsymbol k$近邻算法的参数 $k$近邻算法有三个基本的超参数(super parameter)：距离度量、$k$值的选择以及分类决策规则的决定。\n距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映。$k$近邻模型的特征空间一般是$n$维实数空间，使用的距离是欧式距离，但也可以是其他距离，例如一般的$L_p$距离，即Minkowski距离： $$\rL_p(x_i,x_j)=(\\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}\r$$ 在二维空间中的Minkowski距离的等距线示意图如下：\n当$p=2$时，称为欧氏距离(Euclidean distance)；当$p=1$时，称为曼哈顿距离(Manhattan distance)。不同的距离度量确定的最近邻点不同。\n$\\boldsymbol k$值的选择 $k$值的选择会对$k$近邻算法的结果产生重大影响。如果选择较小的$k$值，就相当于用 …","date":1601424e3,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":4700,"html":"本文主要介绍KNN算法的原理、参数、kd树、基于numpy的k近邻算法实现。","keywords":null,"kind":"page","lang":"en","lastmod":1601424e3,"objectID":"58da39079eac065d654a1bff5315b69e","permalink":"https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/","publishdate":"2020-09-30T00:00:00Z","readingtime":10,"relpermalink":"/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：K近邻算法","type":"post","url":"/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/","weight":0,"wordcount":4662},{"author":null,"categories":["Quant"],"content":"线性回归模型 线性回归(linear regression) 是机器学习和统计学中最基础和广泛应用的模型，是一种对自变量和隐变量之间关系进行建模的回归分析。自变量数量为1时称为简单线性回归，自变量数量大于1时称为多元线性回归。\n从机器学习的角度看，输入就是样本的特征向量$\\boldsymbol x\\in \\mathbb R^d$，其每一维对应一个自变量(数据的特征)；输出是标签$y$，这里$y\\in \\mathbb R$是连续值(实数或连续整数)。假设空间是一组参数化的线性函数$f(\\boldsymbol x;\\boldsymbol w;b)=\\boldsymbol w^\\text T \\boldsymbol x+b$，其中权重向量$\\boldsymbol w$与输入$\\boldsymbol x$维度相同，偏置$b$是一个标量，二者都是可学习的参数。令$\\boldsymbol x$再拼接一个常数$1$，$\\boldsymbol w$再拼接一个偏置$b$，线性模型就可以更简洁地用$f(\\boldsymbol x;\\boldsymbol w)=\\boldsymbol w^\\text T \\boldsymbol x$来表示。\n线性回归的示意图如下：\n线性回归经验风险最小化 给定一组包含$N$个训练样本的训练集$\\mathcal D=\\{(\\boldsymbol x^{(n)},y^{(n)})\\},1\\leq n\\leq N$，我们希望能够学习一个最优的线性回归的模型参数$\\boldsymbol w$。由于线性回归的标签$y$和模型输出都为连续实值，因此平方损失函数非常适合来衡量真实标签和预测标签之间的差异。根据经验风险最小化准则，训练集上的经验风险定义为： $$\r\\begin{aligned}\r\\mathcal R(w)\u0026amp;=\\sum_{n=1}^{N}L(y^{(n)},f(\\boldsymbol x^{(n)};\\boldsymbol w))\\\\\r\u0026amp;=\\frac{1}{2}\\sum_{n=1}^{N}(y^{(n)}-\\boldsymbol w^\\text T \\boldsymbol x^{(n)})^2\\\\\r\u0026amp;=\\frac{1}{2}\\|\\boldsymbol y-X^\\text T \\boldsymbol w\\|^2\r\\end{aligned}\r$$其 …","date":1601337600,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":4600,"html":"本文主要介绍线性回归模型。","keywords":null,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"1b236828872cae3d5e0e9bc96295b733","permalink":"https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","publishdate":"2020-09-29T00:00:00Z","readingtime":10,"relpermalink":"/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习：线性回归","type":"post","url":"/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","weight":0,"wordcount":4584},{"author":null,"categories":["Quant"],"content":"人工智能 智能(intelligence)是现代生活中很常见的一个词，比如智能手机、智能家居、智能驾驶等。在不同使用场合中，智能的含义也不太一样。比如“智能手机”中的“智能”一般是指由计算机控制并具有某种智能行为的意思。这里的“计算机控制”+“智能行为”隐含了对人工智能的简单定义。\n简单地讲，人工智能(Artificial Intelligence, AI) 就是让机器具有人类的智能，这也是人们长期追求的目标。这里关于什么是“智能”并没有一个很明确的定义，但一般认为智能(或特指人类智能)是知识和智力的总和，都和大脑的思维活动有关。人类大脑是经过了上亿年的进化才形成了如此复杂的结构，但至今尚未完全了解。虽然随着神经科学、认知心理学等学科的发展，人们对大脑的结构有了一定程度的了解，但对大脑的智能究竟怎么产生的还知道的很少。我们并不理解大脑的运作原理，以及如何产生意识、情感、记忆等。因此，通过“复制”一个人脑来实现人工智能在目前阶段是不切实际的。\n1950年，阿兰·图灵(Alan Turing)发表了一篇有着重要影响力的论文“Computing Machinery and Intelligence”，讨论了创造一种“智能机器”的可能性。由于“智能”一词比较难以定义，他提出了著名的图灵测试(Turing test)：“一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答。如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的”。图灵测试是促使人工智能从哲学探讨到科学研究的一个重要因素，引导了人工智能的很多研究方向。因为要使得计算机能通过图灵测试，计算机必须具备理解语言、学习、记忆、推理、决策等能力。这样，人工智能就延伸出了很多不同的子学科，比如机器感知(计算机视觉、语音信息处理)，学习(模式识别、机器学习、强化学习)，语言(自然语言处理)、记忆(知识表示)、决策(规划、数据挖掘) 等。所有这些研究领域都可以看成是人工智能的研究范畴。\n人工智能是计算机科学的一个分支，主要研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统等。和很多其它学科不同，人工智能这个学科的诞生有着明确的标志性事件，即1956年的达特茅斯会议。在这次会议上，“人工智能”被提出并作为本研究领域的名称。同时，人工智能研究的使命也得以确 …","date":1601251200,"dir":"post/3-ML/","expirydate":-62135596800,"fuzzywordcount":16800,"html":"本文主要介绍机器学习的基本概念，包括人工智能、统计学习、统计学习的分类、统计学习的三要素等。","keywords":null,"kind":"page","lang":"en","lastmod":1601251200,"objectID":"a3baf0e5c97e5839249db963a18d47e0","permalink":"https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","publishdate":"2020-09-28T00:00:00Z","readingtime":34,"relpermalink":"/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","section":"post","tags":["Quant","Model","Machine Learning"],"title":"机器学习基础","type":"post","url":"/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","weight":0,"wordcount":16761},{"author":null,"categories":["Quant"],"content":"信息论(information theory) 是数学、物理、计算机科学等多个学科的交叉领域。信息论是由Claude Shannon最早提出的，主要研究信息的量化、存储和通信等方法。这里，“信息”是指一组消息的集合。假设在一个噪声通道上发送消息，我们需要考虑如何对每一个信息进行编码、传输以及解码，使得接收者可以尽可能准确地重构出消息。在机器学习相关领域，信息论也有着大量的应用。比如特征抽取、统计推断、自然语言处理等。\n熵 熵(entropy) 最早是物理学的概念，用于表示一个热力学系统的无序程度。在信息论中，熵用来衡量一个随机事件的不确定性。\n自信息和熵 自信息(self information) 表示一个随机事件所包含的信息量。一个随机事件发生的概率越高，其自信息越低。如果一个事件必然发生，则其自信息为0。对于一个随机变量$X$(取值集合为$\\mathcal X$，概率分布为$p(x)$)，当$X=x$时的自信息$I(x)$定义为：$I(x)=-\\log p(x)$。在自信息的定义中，对数的底可以使用2、自然常数$e$或是10。当以2为底时，自信息的单位为bit；当以$e$为底时，自信息的单位为nat。\n对于分布为$p(x)$的随机变量$X$，其自信息的数学期望，即熵(entropy)$H(X)$定义为： $$\r\\begin{aligned}\rH(X) \u0026amp;=\\mathbb{E}_{X}[\\mathrm{I}(x)] \\\\\r\u0026amp;=\\mathbb{E}_{X}[-\\log p(x)] \\\\\r\u0026amp;=-\\sum_{x \\in \\mathcal{X}} p(x) \\log p(x)\r\\end{aligned}\r$$ 熵越高，则随机变量的信息越多；熵越低，则随机变量的信息越少。如果随机变量$X$当且仅当在$x$时$p(x)=1$，则其熵为0。也就是说，对于一个确定的信息，其熵为0，信息量也为0.如果其概率分布为一个均匀分布，则熵最大。\nimport math # 计算不同概率分布的熵 p1 = [0.1, 0.3, 0.6] p2 = [0.33, 0.33, 0.34] p3 = [0.0, 0.0, 1] def entropy(p): ent = 0.0 for i in range(len(p)): if p[i] != 0: ent += p[i] * …","date":1601164800,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":2600,"html":"本文主要介绍信息论的相关概念，包括熵、自信息、互信息、KL散度、JS散度等。","keywords":null,"kind":"page","lang":"en","lastmod":1601164800,"objectID":"c99e5455c8718ae46012d87ea8a2ba57","permalink":"https://xiangdiwu.github.io/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/","publishdate":"2020-09-27T00:00:00Z","readingtime":6,"relpermalink":"/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/","section":"post","tags":["Quant","Math"],"title":"量化数学基础：信息论","type":"post","url":"/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/","weight":0,"wordcount":2548},{"author":null,"categories":["Quant"],"content":"概率统计的基本概念 样本空间与随机事件 样本空间(sample space)是一个随机试验所有可能结果的集合。例如，如果抛一枚硬币，那么样本空间的集合就是{正面, 反面}；如果抛一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为样本点。\n一般称试验$E$的样本空间$S$的子集为$E$的随机事件，简称事件。在每次试验中，当且仅当这一子集中的一个样本点出现时，称这一事件发生。\n有些试验有两个或多个可能的样本空间。例如，从52张扑克牌中随机抽出一张，样本空间可以是数字(A到K)，也可以是花色(黑桃, 红桃, 梅花, 方块)。如果要完整地描述一张牌，就需要同时给出数字和花色，这时样本空间可以通过构建上述两个样本空间的笛卡儿乘积来得到。具体选用什么样的样本空间，由任务需求来决定。\n计数原理 计数(counting)是样本空间的基本概念。计数的基本原理是加法原理(sum rule)和乘法原理(product rule)。加法原理指的是：做一件事情，完成它有$n$类方式，第一类方式有$m_1$种方法，第二类方式有$m_2$种方法，以此类推，第$n$类方式有$m_n$种方法，那么完成这件事情共有$m_1+m_2+\\cdots+m_n$种方法。乘法原理指的是：做一件事，完成它需要分成$n$个步骤，做第一 步有$m_1$种不同的方法，做第二步有$m_2$种不同的方法，以此类推，做第$n$步有$m_n$种不同的方法。那么完成这件事共有$N=m_1 \\times m_2 \\times \\cdots \\times m_n$种不同的方法。\n排列与组合 下图为排列(permutation)数、组合(combination)数以及将物体放入桶中问题的计算方法：\n# scipy实现阶乘和组合数的计算 import math from scipy import special from scipy import stats # 计算阶乘 print(math.factorial(20)) # 计算组合数 print(special.binom(5, 3)) 概率的公理 将事件$E$发生的概率定义为$P(E)$，全集为$S$，则概率$P(E)$满足如下三条公理：\n(1) $0 \\leqslant P(E) \\leqslant 1$；\n(2) $P(S)=1$； …","date":1601078400,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":6700,"html":"本文主要介绍概率统计的基本概念。","keywords":null,"kind":"page","lang":"en","lastmod":1601078400,"objectID":"6c0079ebec7c81c3b268ca5dfe1b03b8","permalink":"https://xiangdiwu.github.io/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/","publishdate":"2020-09-26T00:00:00Z","readingtime":14,"relpermalink":"/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/","section":"post","tags":["Quant","Math"],"title":"量化数学基础：概率统计","type":"post","url":"/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/","weight":0,"wordcount":6604},{"author":null,"categories":["Quant"],"content":"数学优化(mathematical optimization)问题也叫最优化问题，指在一定约束条件下，求解一个目标函数的最大值或最小值问题。数学优化问题的定义为：给定一个目标函数(也叫代价函数)$f:A\\rightarrow\\mathbb R$，寻找一个变量$\\boldsymbol x^* \\in \\mathcal D$，使得对于所有$\\mathcal D$中的$\\boldsymbol x$，$f(\\boldsymbol{x}^{*}) \\leqslant f(\\boldsymbol{x})$(最小化)；或者$f(\\boldsymbol{x}^{*}) \\geqslant f(\\boldsymbol{x})$(最大化)，其中$\\mathcal D$为变量$\\boldsymbol x$的约束集，也叫可行域；$\\mathcal D$中的变量被称为可行解。\n数学优化的类型 离散优化和连续优化 离散优化(discrete optimization)问题是目标函数的输入变量为离散变量，比如为整数或有限集合中的元素。离散优化问题主要有两个分支：\n(1) 组合优化(combinatorial optimization)：其目标是从一个有限集合中找出使得目标函数最优的元素。在一般的组合优化问题中，集合中的元素之间存在一定的关联，可以表示为图结构。典型的组合优化问题有旅行商问题、最小生成树问题、图着色问题等。很多机器学习问题都是组合优化问题，比如特征选择、聚类问题、超参数优化问题以及结构化学习(structured learning)中标签预测问题等。\n(2) 整数规划(integer programming)：输入变量$\\boldsymbol x \\in \\mathbb Z^d$是一个整数向量。常见的整数规划问题通常为整数线性规划。整数线性规划的一种最直接的求解方法是：1. 去掉输入必须为整数的限制，将原问题转换为一般的线性规划问题，这个线性规划问题为原问题的松弛问题；2. 求得相应松弛问题的解；3. 把松弛问题的解四舍五入到最接近的整数。但是这种方法得到的解一般都不是最优的，因为原问题的最优解不一定在松弛问题最优解的附近。另外，这种方法得到的解也不一定满足约束条件。\n离散优化问题的求解一般都比较困难，优化算法的复杂度都比较高。\n连续优化(continuous …","date":1600992e3,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":6200,"html":"本文主要介绍数学优化问题，包括数学优化问题的类型、凸优化基础、优化算法、拉格朗日乘数法与KKT条件。","keywords":null,"kind":"page","lang":"en","lastmod":1600992e3,"objectID":"c42f884828e22dc7489500c8ab136714","permalink":"https://xiangdiwu.github.io/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/","publishdate":"2020-09-25T00:00:00Z","readingtime":13,"relpermalink":"/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/","section":"post","tags":["Quant","Math"],"title":"量化数学基础：数学优化","type":"post","url":"/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/","weight":0,"wordcount":6185},{"author":null,"categories":["Quant"],"content":"微积分(calculus)是研究函数的微分(differentiation)、积分(integration)及其相关应用的数学分支。\n微分 导数 对于定义域和值域都是实数域的函数$f: \\mathbb R \\rightarrow \\mathbb R$，若$f(x)$在点$x_0$的某个邻域$\\Delta x$内，极限 $$\rf^{\\prime}(x_{0})=\\lim _{\\Delta x \\rightarrow 0} \\frac{f(x_{0}+\\Delta x)-f(x_{0})}{\\Delta x}\r$$ 存在，则称函数$f(x)$在点$x_0$处可导，$f^{\\prime}(x_0)$称为其导数(derivative)，或导函数，也可以记为${\\text d f(x_0)}/{\\text d x}$。在几何上，导数可以看做函数曲线上的切线斜率。下图给出了一个函数导数的可视化示例，其中函数$g(x)$的斜率为函数$f(x)$在点$x$的导数，$\\Delta y=f(x+\\Delta x)-f(x)$。\n下表中给出了几个机器学习领域常见函数的导数：\n高阶导数：对一个函数的导数继续求导，可以得到高阶导数。函数$f(x)$的导数$f^\\prime(x)$称为一阶导数，$f^\\prime(x)$的导数称为二阶导数，记为$f^{\\prime\\prime}(x)$、$f^{(2)}(x)$或$\\text d^2f(x)/\\text d x^2$。\n偏导数(partial derrivative)：对于一个多元变量函数$f:\\mathbb R^d \\rightarrow \\mathbb R$，其偏导数是关于其中一个变量$x_i$的导数，而保持其他变量固定，可以记为$f_{x_{i}}^{\\prime}(\\boldsymbol{x})$，$\\nabla_{x_{i}} f(\\boldsymbol{x})$或$\\partial f(\\boldsymbol x)/\\partial x_i$。\n微分 给定一个连续函数，计算其导数的过程称为微分(differentiation)。若函数$f(x)$在其定义域包含的某区间内每一个点都可导，那么也可以说函数$f(x)$在这个区间内可导。如果一个函数$f(x)$在定义域中的所有点都存在导数，则$f(x)$为可微函 …","date":1600905600,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":1800,"html":"本文主要介绍微积分的基础知识，包括导数、积分、矩阵微积分等。","keywords":null,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"d2dc56d9af4f961e840201b0cba16658","permalink":"https://xiangdiwu.github.io/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/","publishdate":"2020-09-24T00:00:00Z","readingtime":4,"relpermalink":"/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/","section":"post","tags":["Quant","Math"],"title":"量化数学基础：微积分","type":"post","url":"/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/","weight":0,"wordcount":1758},{"author":null,"categories":["Quant"],"content":"向量和向量空间 向量 标量(scalar)是一个实数，一般用斜体小写字母$a,b,c$来表示。向量(vector)是由一组实数组成的有序数组，一个n维向量$\\boldsymbol a$由n个有序实数组成，表示为$\\boldsymbol{a}=\\left[a_{1}, a_{2}, \\cdots, a_{n}\\right]$，其中$a_{i}$称为向量$\\boldsymbol{a}$的第$i$个分量(第$i$维)。\n# numpy库常用于实现线性代数中向量和矩阵的基本操作 import numpy as np # numpy中向量的定义 v_1 = np.array([1, 2, 3, 4, 5]) v_2 = np.array([5.6, 4.6, 3.6, 2.6, 1.6]) 向量空间 向量空间(vector space)也称线性空间(linear space)，是指由向量组成的集合，并满足以下两个条件：\n(1) 向量加法封闭性：向量空间$\\mathcal V$中的任意两个向量$\\boldsymbol a$和$\\boldsymbol b$，它们的和$\\boldsymbol a+\\boldsymbol b$也属于向量空间$\\mathcal V$；\n# numpy实现向量加法 v_a = np.add(v_1, v_2) print(v_a, v_a.shape) # 结果为向量 (2) 标量乘法封闭性：向量空间$\\mathcal V$中的任一向量$\\boldsymbol a$和任一标量$c$，它们的乘积$c\\boldsymbol a$也属于向量空间$\\mathcal V$。\n# numpy实现向量与标量相乘 k = 2.0 v_k2 = k * v_1 print(v_k2, v_k2.shape) # 结果为向量 一个常用的线性空间是欧式空间(Euclidean space)，常表示为$\\mathbb R^n$，其中$n$为空间维度(dimension)。欧式空间中的向量加法和标量乘法定义为： $$\r\\begin{aligned}\r\\left[a_{1}, a_{2}, \\cdots, a_{n}\\right]+\\left[b_{1}, b_{2}, \\cdots, b_{n}\\right] \u0026amp;=\\left[a_{1}+b_{1}, a_{2}+b_{2}, …","date":1600819200,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":8700,"html":"本文主要介绍线性代数基础知识，包括向量和向量空间、矩阵、线性映射、矩阵操作、矩阵的逆、矩阵的特征值、矩阵的分解、矩阵的运算、矩阵的应用、矩阵的应用。","keywords":null,"kind":"page","lang":"en","lastmod":1600819200,"objectID":"6984efe4988b26945131e81acaad1f88","permalink":"https://xiangdiwu.github.io/post/2-math/math1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","publishdate":"2020-09-23T00:00:00Z","readingtime":18,"relpermalink":"/post/2-math/math1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","section":"post","tags":["Quant","Math"],"title":"量化数学基础：线性代数","type":"post","url":"/post/2-math/math1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","weight":0,"wordcount":8682},{"author":null,"categories":["Quant"],"content":"统计、数据和计算机 统计学（Statistics）：收集、整理、分析并解释数据以支持决策的科学。 描述统计（Descriptive Statistics）：用图表和概括量描述数据基本特征的方法。 推断统计（Inferential Statistics）：利用样本数据对总体进行估计与假设检验的技术。 变量（Variable）：可以取不同数值或类别的观测特征。 无序类别变量（Nominal Categorical Variable）：仅有名称而无内在顺序的分类特征。 有序类别变量（Ordinal Categorical Variable）：类别间存在自然顺序但间距未知的特征，比如：很好、好、一般、差、很差。 数值变量（Numerical Variable）：又称为定量变量，以数值表示并可进行算术运算的特征。 无序类别数据（Nominal Data）：属于无序类别的观察结果集合。 有序类别数据（Ordinal Data）：属于有序类别的观察结果集合。 数值数据（Numerical Data）：由数值构成的观察结果集合。 总体（Population）：研究对象的全部个体（数据）的集合。 样本（Sample）：从总体中抽取用于研究的子集。 样本量（Sample Size）：样本中个体的数量。 简单随机抽样（Simple Random Sampling）：每个个体等概率被抽中的抽样方法。 分层抽样（Stratified Sampling）：将总体分层后在各层内随机抽样的方法。 系统抽样（Systematic Sampling）：按固定间隔从有序总体中抽取个体的抽样方法。 整群抽样（Cluster Sampling）：以自然群体为单位整群抽取的抽样方法。 用图表展示数据 频数分布（Frequency Distribution）：展示各取值或区间出现次数的表格或图形。 频数（Frequency）：某取值或区间在数据集中出现的次数。 比例（Proportion）：部分在全体中所占的相对份额。 比率（Ratio）：两个数值间的对比关系。 条形图（Bar Chart）：用等宽条形高度表示类别频数的图形。 帕累托图（Pareto Chart）：按频数降序排列并叠加累积比例的条形图。 饼图（Pie Chart）：用扇形面积表示类别比例的圆形图。 环形图（Donut Chart）：中间留空的饼 …","date":1600732800,"dir":"post/2-Math/","expirydate":-62135596800,"fuzzywordcount":14700,"html":"本文主要介绍基础统计学相关概念。","keywords":null,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"75ce63c4eda2eece19468a8f9c7a5e59","permalink":"https://xiangdiwu.github.io/post/2-math/math0-%E5%9F%BA%E7%A1%80%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","publishdate":"2020-09-22T00:00:00Z","readingtime":30,"relpermalink":"/post/2-math/math0-%E5%9F%BA%E7%A1%80%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","section":"post","tags":["Quant","Math"],"title":"基础统计学名词解释","type":"post","url":"/post/2-math/math0-%E5%9F%BA%E7%A1%80%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","weight":0,"wordcount":14694},{"author":null,"categories":["Tech"],"content":"Mind Map\nKubernetes 基本理念 自动化部署，缩扩容和管理容器应用 预期状态管理(Desired State Management) Kubernetes API 对象（声明预期状态） Kubernetes Control Plane（确保集群当前状态匹配预期状态） Kubernetes Master kube-apiserver（API Server） 对外提供各种对象的CRUD REST接口 对外提供Watch机制，通知对象变化 将对象存储到Etcd中 kube-controller-manager（守护进程） 功能：通过apiserver监视集群的状态，并做出相应更改，以使得集群的当前状态向预期状态靠拢 controllers replication controller endpoints controller namespace controller serviceaccounts controller \u0026amp;hellip;\u0026amp;hellip; kube-scheduler（调度器） 功能：将Pod调度到合适的工作节点上运行 调度的考虑因素 资源需求 服务治理要求 硬件/软件/策略限制 亲和以及反亲和要求 数据局域性 负载间的干扰 \u0026amp;hellip;\u0026amp;hellip; Work Node Kubelet（节点代理） 接受通过各种机制（主要是通过apiserver）提供的一组PodSpec 确保PodSpec中描述的容器处于运行状态且运行状况良好 Kube-proxy（节点网络代理） 在节点上提供Kubernetes API中定义Service 设置Service对应的IPtables规则 进行流量转发（userspace模式） 部署模式 Single node Single head node，multiple workers API Server，Scheduler，and Controller Manager run on a single node Single etcd，HA heade nodes，multiple workers Multiple API Server instances fronted by a load balancer Multiple Scheduler and Controller Manager …","date":1582329600,"dir":"post/6-Tech/","expirydate":-62135596800,"fuzzywordcount":1900,"html":"Kubernetes 相关知识汇总","keywords":null,"kind":"page","lang":"en","lastmod":1582329600,"objectID":"c0f547feb94067d39c80a6c022ffc97a","permalink":"https://xiangdiwu.github.io/post/6-tech/k8s-mindmap/","publishdate":"2020-02-22T00:00:00Z","readingtime":4,"relpermalink":"/post/6-tech/k8s-mindmap/","section":"post","tags":["Kubernetes","Knowledge Graph"],"title":"Kubernetes 知识图谱","type":"post","url":"/post/6-tech/k8s-mindmap/","weight":0,"wordcount":1848},{"author":null,"categories":["Tech"],"content":"Git是程序员工作中使用频率非常高的工具，要提高日常的工作效率，就需要熟练掌握Git的使用方法。相对于传统的版本控制系统而言，Git更为强大和灵活，其各种命令和命令参数也非常多，如果不了解Git的内部原理，要把Git使用得顺手的话非常困难。本文将用一个具体的例子来帮助理解Git的内部存储原理,加深对Git的理解，从掌握各种Git命令，以在使用Git进行工作时得心应手。\nGit 目录结构 Git的本质是一个文件系统，其工作目录中的所有文件的历史版本以及提交记录(Commit)都是以文件对象的方式保存在.git目录中的。\n首先创建一个work目录，并采用git init命令初始化git仓库。该命令会在工作目录下生成一个.git目录，该目录将用于保存工作区中所有的文件历史的历史版本，提交记录，branch，tag等信息。\n$ mkdir work $ cd work $ git init 其目录结构如下：\n├── branches 不这么重要，暂不用管 ├── config git配置信息，包括用户名，email，remote repository的地址，本地branch和remote | branch的follow关系 ├── description 该git库的描述信息，如果使用了GitWeb的话，该描述信息将会被显示在该repo的页面上 ├── HEAD 工作目录当前状态对应的commit，一般来说是当前branch的head，HEAD也可以通过git checkout 命令被直接设置到一个特定的commit上，这种情况被称之为 detached HEAD ├── hooks 钩子程序，可以被用于在执行git命令时自动执行一些特定操作，例如加入changeid │ ├── applypatch-msg.sample │ ├── commit-msg.sample │ ├── post-update.sample │ ├── pre-applypatch.sample │ ├── pre-commit.sample │ ├── prepare-commit-msg.sample │ ├── pre-push.sample │ ├── pre-rebase.sample │ └── update.sample ├── info 不这么重要，暂不用管 │ └── …","date":1548028800,"dir":"post/6-Tech/","expirydate":-62135596800,"fuzzywordcount":5700,"html":"Git是程序员工作中使用频率非常高的工具，要提高日常的工作效率，就需要熟练掌握Git的使用方法。相对于传统的版本控制系统而言，Git更为强大和灵活，其各种命令和命令参数也非常多，如果不了解Git的内部原理，要把Git使用得顺手的话非常困难。本文将用一个具体的例子来帮助理解Git的内部存储原理,加深对Git的理解，从掌握各种Git命令，以在使用Git进行工作时得心应手。","keywords":null,"kind":"page","lang":"en","lastmod":1548028800,"objectID":"cd63d7514ec4204ee69f91ab0d418f36","permalink":"https://xiangdiwu.github.io/post/6-tech/git/","publishdate":"2019-01-21T00:00:00Z","readingtime":12,"relpermalink":"/post/6-tech/git/","section":"post","tags":["Git"],"title":"Git内部存储原理","type":"post","url":"/post/6-tech/git/","weight":0,"wordcount":5604},{"author":null,"categories":["Reading"],"content":"有人说这不过是拉罗什福科这位法国落魄贵族的经验教训总结，人皆可写之。这个评价过于轻蔑和傲慢。\n这本《伪善是邪恶向美德的致敬》仿佛痴人梦呓一般的道德箴言集，不像蒙田那样动辄引用优美的诗句，也不像帕斯卡尔用宗教来武装自己，不作规训，用观察视角把对人类行为动机的洞察写出，粗暴地扯下人性的虚伪面具。\n拉罗什福科所处的时代总体上是一个沉思和自省的时代。其早期活动是流血政治型，而到了晚年则变成文化沉思型。他早年阅历丰富，见识过各种各样的人，品尝过各种各样的德行和恶行。他受教育少，却有着很好的感受力。\n拉罗什福科的作品或许代表着法兰西民族中的一种典型性格：集疯狂和冷静、虚荣与真诚、放荡不羁和深刻反省于一身。\n按照《不列颠百科全书》的说法，拉罗什福科是以一本书立身的人，也就是这本。鲁迅说：“无论单就人生哲理，还是单就处世指导，拉罗什福科的影响都要超过培根，这诚足信。”\n","date":1501718400,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":400,"html":"书于1665年首度在巴黎出版。“法国思想之父”伏尔泰百年后称赞本书强烈影响了法国的民族个性，造就后世法国人思想上的幽微内省与形容精确。拉罗什福柯认为，人的思绪就像滑溜的鳗鱼，不管条理如何分明，逻辑如何清晰，都会从人的思考中溜窜逸逃，除非以简洁优雅的字句将之纪录下来。于是，他便选择以格言体，写下他对人性的分析与观察。","keywords":null,"kind":"page","lang":"en","lastmod":1501718400,"objectID":"76b7a3e7a0e6a7cfd2bc3fc071de37fc","permalink":"https://xiangdiwu.github.io/2017/08/03/reflections-on-maximes-et-reflections-morales/","publishdate":"2017-08-03T00:00:00Z","readingtime":1,"relpermalink":"/2017/08/03/reflections-on-maximes-et-reflections-morales/","section":"post","tags":null,"title":"《伪善是邪恶向美德的致敬》读后感","type":"post","url":"/2017/08/03/reflections-on-maximes-et-reflections-morales/","weight":0,"wordcount":377},{"author":null,"categories":["Reading"],"content":" 马克思•韦伯是德国著名的社会学家、经济学家和历史学家，也是现代社会学的创始人之一。在其代表作品《新教伦理与资本主义精神》中，他采用独特的研究视角，从宗教和文化的角度探寻了资本主义经济发展背后隐藏在文化深处的精神动力问题。他通过比较东西方国家的不同宗教及其资本主义的现实发展得出结论：新教伦理及资本主义精神是推动西方资本主义经济产生和发展的精神动力，为西方资本主义的发展提供了道德依托和价值呵护。同时，在资本主义发展一定阶段之后，资本主义已经脱离出了新教伦理成为了占据世界的价值体系。正如作者所说“The Puritan wanted to work in calling; we are forced to do so.”。\n这本书创作的背景：\n19世纪末期至20世纪初期的非理性主义思潮，使得西欧的理性主义传统遭遇了危机。作为生活在这一时期的马克思韦伯，他不仅受到传统的理性主义的影响，同时也看到了在西欧尤其是德国的非理性主义产生的影响，这就促使作者对这些不同思想进行深入的研究。\n其次，在这一时期，德国境内天主教徒和新教徒这两个群体之间产生了巨大的矛盾，这就促使韦伯研究宗教与经济发展之间复杂关系来解释这一复杂的社会现象。\n最后，马克思韦伯出生于一个较为富裕的中产阶级家庭，他的母亲是一位虔诚的加尔文派教徒，她对作者影响巨大。从这些方面来看，这些家庭和成长背景为作者从宗教和文化的角度研究资本主义发展的净胜动力提供了可能。\n在韦伯看来，新教伦理与资本主义精神的核心就是禁欲主义，节俭、勤奋（而非奢侈、糜烂）才是资本主义发展的动力。\n韦伯所谓的资本主义，其实有着严格的“时空限定”。他考察的只是西欧和美国的资本主义（36页），而且他回避了资本原始积累的阶段，在一定程度上也否认今后的资本主义是典型的资本主义，因为他觉得“初创时期”过后，资本主义已经发生了偏离，它已不需要资本主义精神的支持。所以，他考察的是需要资本主义精神支持的资本主义。在他看来，尽管中国、印度、巴比伦、古代的希腊和罗马、中世纪都存在过资本主义，但“那里的资本主义缺乏这种独特的精神气质”（36页）。\n韦伯在书中考察的禁欲主义其实有三个阶段，而处于这三个阶段的禁欲主义，其特质和逻辑是不同的。\n第一阶段是传统基督教的禁欲主义。这是我们所熟知的。这种逻辑是说，人要获得神的救赎，或者显示出其是上帝的选民，就必须不停地劳作而非游手好 …","date":1491609600,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":3700,"html":"《新教伦理与资本主义精神》是马克斯·韦伯著名的作品。在这部作品中，韦伯提出了一个知名的论点：新教教徒的思想影响了资本主义的发展。宗教教徒往往排斥世俗的事务，尤其是经济成就上的追求，但为什么新教教徒却是例外？韦伯在该书中论述宗教观念（新教伦理）与隐藏在资本主义发展背后的某种心理驱力（资本主义精神）之间的关系。韦伯列举了新教、清教、加尔文教等教徒生活、学习的例子并加以分析得出：资本主义的兴起和成功与新教盛行存在着相互影响的关系。新教入世禁欲主义伦理为资本主义企业家提供了心理驱动力和道德能量，从而成为现代资本主义得以兴起的重要条件之一。","keywords":null,"kind":"page","lang":"en","lastmod":1491609600,"objectID":"73af71fb1ff52a86afab5cd566df6c1a","permalink":"https://xiangdiwu.github.io/2017/04/08/reflections-on-the-protestant-ethic-and-the-spirit-of-capitalism/","publishdate":"2017-04-08T00:00:00Z","readingtime":8,"relpermalink":"/2017/04/08/reflections-on-the-protestant-ethic-and-the-spirit-of-capitalism/","section":"post","tags":null,"title":"《新教伦理与资本主义精神》读后感","type":"post","url":"/2017/04/08/reflections-on-the-protestant-ethic-and-the-spirit-of-capitalism/","weight":0,"wordcount":3600},{"author":null,"categories":["Reading"],"content":"《消费社会》：鲍德里亚与消费社会的符号牢笼\n法国思想家让·鲍德里亚犀利地指出，现代消费社会已经超越了单纯满足需求的阶段，进入了一个全新的符号秩序。当我们购买一款最新型号的智能手机时，我们购买的不仅是通讯工具，更是某种社会身份、生活方式的象征。那些铺天盖地的广告从不告诉我们产品的基本功能，而是精心编织着关于成功、爱情、自由的神话。消费不再是手段，而成为了目的本身，一种现代人的“公民义务”。\n在鲍德里亚看来，这种消费逻辑已经渗透到社会生活的每一个角落。甚至连教育、文化、人际关系都被商品化了。我们购买昂贵的课程不是为了知识本身，而是为了获取象征“高素质”的证书；我们参与慈善活动有时不过是为了在社交媒体上展示道德优越感。一切都是表演，一切都是符号交换，而真正的价值与意义在这场盛大的假面舞会中消失殆尽。\n他还提到，消费主义通过制造“个性化”的幻觉来实现真正的同质化。市场上充斥着看似多样的选择，这种差异只是表面的、预先编码的差异。我们以为自己通过消费表达了独特性，实则只是在一个巨大的符号系统中扮演被分配的角色。\n这种符号消费不仅制造了虚假满足，还带来了深刻的精神空虚。因为符号价值是永远无法真正满足的——它建立在差异和比较的基础上，总会有更高阶的符号等待我们去追求。我们如同推石上山的西西弗斯，在消费的循环中徒劳地寻求着永远无法抵达的满足感。\n然而，鲍德里亚的描述是否过于悲观？其实，意识本身已经是解放的第一步。真正的抵抗或许不在于拒绝消费，而在于保持一种清醒的距离感。在必要的消费之外，培育那些不能被符号化的真实价值：创造性的劳动、真诚的人际关系、对知识的纯粹追求。避免矫枉过正，不沦为符号的奴隶的同时，依然要欣赏物质世界的美好。\n","date":1481241600,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":800,"html":"《消费社会》是鲍德里亚早期的代表性著作，对国际哲学、社会学以及新兴的文化研究领域产生了广泛而深远的影响，被公认为该主题的重要经典之一，在马克思主义理论、社会学、政治学、文艺学以及相关领域，该书亦是最流行的教学参考书。在这本书中，鲍德里亚将政治经济学批判理论、符号学理论、精神分析理论等整合在一起，形成了消费社会批判理论。鲍德里亚指出：与过去将消费看作经济活动中的一环不同，在消费社会中，消费是一种主动的结构，商品的符号价值推动着人们在消费中对自身、对社会的想象性认同，确证了现代社会的合法性。要透视这一社会，就需要从政治经济学批判走向符号政治经济学批判。","keywords":null,"kind":"page","lang":"en","lastmod":1481241600,"objectID":"841403b59d5227a1476bc6a778327c1f","permalink":"https://xiangdiwu.github.io/2016/12/09/reflections-on-la-societe-de-consommation/","publishdate":"2016-12-09T00:00:00Z","readingtime":2,"relpermalink":"/2016/12/09/reflections-on-la-societe-de-consommation/","section":"post","tags":null,"title":"《消费社会》读后感","type":"post","url":"/2016/12/09/reflections-on-la-societe-de-consommation/","weight":0,"wordcount":712},{"author":null,"categories":["Reading"],"content":"勒庞的《乌合之众》，“是一部以阅读法国大革命时间为基础的群体行为的社会心理学著作”。罗伯特•墨顿的序“勒庞《乌合之众》的得与失”写得很好，他提到一点，“《乌合之众》的当代意义，在于它发现问题的功能而非解决问题的功能。”\n勒庞是保守派社会精英，他反对传统的国家主义，反对新兴的集体主义，推崇英美式的自由主义。他生活在法国革命不断的年代里，经历过巴黎公社和法兰西第二帝国等历史时期，亲眼目睹了法国民众在传统的信仰与权威崩塌后，在近乎宗教般的革命激情中，退化成一群野蛮、善变、极端的原始人，在少数人的怂恿下，民众会毫不犹豫地做出骇人听闻的暴行，事后却要求爱国主义的荣誉勋章。于是，勒庞在他的传世名作《乌合之众》中总结道：民众缺乏理性，依赖于信仰与权威的引导，用想象来判断，模仿他人行为，简而言之，民众是盲从的。\n“群体不善推理，却急于行动。它们目前的组织赋予它们巨大的力量。我们目睹其诞生的那些教条，很快也会具有旧式教条的威力，也就是说，不容讨论的专横武断的力量。群众的神权就要取代国王的神权了。”（P4）\n“有时，在某种狂暴的感情——譬如因为国家大事——的影响下，成千上万孤立的个人也会获得一个心理群体的特征。在这种情况下，一个偶然事件就足以使他们闻风而动聚集在一起，从而立刻获得群体行为特有的属性。”（P12）\n“群体中的个人不但在行动上和他本人有着本质的区别，甚至在完全失去独立性之前，他的思想和感情就已经发生了变化，这种变化是如此深刻，它可以让一个守财奴变得挥霍无度，把怀疑论者改造成信徒，把老实人变成罪犯，把懦夫变成豪杰。在1789年8月4日那个值得纪念的晚上，法国的贵族一时激情澎湃，毅然投票放弃了自己的特权，他们如果是单独考虑这件事，没有一个人会表示同意。”（P19）\n“群体在智力上总是低于孤立的个人，但是从感情及其激发的行动这个角度看，群体可以比个人表现得更好或更差，这全看环境如何。一切取决于群体所接受的暗示具有什么性质。……群体固然经常是犯罪群体，然而它也常常是英雄主义的群体。正是群体，而不是孤立的个人，会不顾一切地慷慨赴难，为一种教义或观念的凯旋提供了保证；会怀着赢得荣誉的热情赴汤蹈火……这种英雄主义毫无疑问有着无意识的成分，然而正是这种英雄主义创造了历史。如果人民只会以冷酷无情的方式干大事，世界史上便不会留下他们多少记录了。”（P19）\n“孤立的个人很清楚，在孤身一人时，他 …","date":1466467200,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":2900,"html":"古斯塔夫・勒庞 Gustave Le Bon(1841-1931) 自1894年始，写下一系列社会心理学著作，以本书最为著名；在社会心理学领域已有的著作中，最有影响的，也是这本并不很厚的《乌合之众》。古斯塔夫・勒庞在他在书中极为精致地描述了集体心态，对人们理解集体行为的作用以及对社会心理学的思考发挥了巨大影响。","keywords":null,"kind":"page","lang":"en","lastmod":1466467200,"objectID":"906c92fc9925aeed4cc3467c31cced15","permalink":"https://xiangdiwu.github.io/2016/06/21/the-crowd-reading-notes/","publishdate":"2016-06-21T00:00:00Z","readingtime":6,"relpermalink":"/2016/06/21/the-crowd-reading-notes/","section":"post","tags":null,"title":"《乌合之众》读书笔记","type":"post","url":"/2016/06/21/the-crowd-reading-notes/","weight":0,"wordcount":2898},{"author":null,"categories":null,"content":"Does your name have a special meaning?\nFunny you ask. My name is made up of three Chinese characters. The second character is actually the abbreviation for Hunan, which is my dad\u0026amp;rsquo;s hometown.\nThe third character linked to lotus seeds. You konw, Hunan is known as the home of the lotus flower.\nIn Chinese culture, lotus seeds stand for purity and strength, like a lotus blooming through muddy waters while staying flawless.\nThey also symbolize family blessings. The many seeds in one pod mean abundant love and generations of prosperity.\nAnd to me personally, it also reflects inner peace—just as the seed grows patiently toward light, it reminds me to stay calm and true to myself.\nHow would you choose names for your next generation?\nWhen choosing names for the next generation, I\u0026amp;rsquo;d definitely want something meaningful, and also make sure it sounds good and fits well with our last name. It’s a big decision, so I’d discuss it with my partner to find the perfect name together.\nAre there …","date":1462492800,"dir":"post/English/","expirydate":-62135596800,"fuzzywordcount":400,"html":"IELTS Speaking Part 1 Name","keywords":null,"kind":"page","lang":"en","lastmod":1462492800,"objectID":"c25a4ac28540b2f68ffb37ec5b704041","permalink":"https://xiangdiwu.github.io/2016/05/06/IELTS-speaking-part-1-name/","publishdate":"2016-05-06T00:00:00Z","readingtime":2,"relpermalink":"/2016/05/06/IELTS-speaking-part-1-name/","section":"post","tags":["English"],"title":"English Practice Topic: Name","type":"post","url":"/2016/05/06/IELTS-speaking-part-1-name/","weight":0,"wordcount":353},{"author":null,"categories":["Reading"],"content":"《美国大城市的死与生（the Death and Life of Great American Cities）》出版于1961年，从此后就变成建筑界、城市规划领域最著名的书之一。\n这本书所探讨的问题，和它成书之中所处的时代背景是分不开的。美国经历了20世纪30年代的经济大萧条，其间为了解决人力过剩问题，进行了大量城市建设。这个方法正是罗斯福从当时主持纽约建设的罗伯特•摩西（Robert Moses）那儿学来的。那几年，曾有84000人为摩西建设过在书中被雅各布斯强烈批判的纽约公园。二战之后美国建筑与城市规划界的思路发生了一些变化，一方面是经济上升期对城市建设的需求，一方面是原先欧洲的现代主义建筑“旗手”纷纷移居美国，现代主义设计手法、现代主义城市规划，正是多快好省建设世界的“良方”。\n摩西带领规划师们运用了三种那个时期的流行手段——城市郊区化、建筑现代主义化、着力建造城市地标，在“为了实现每个复员军人的梦想”的口号下，展开了城市建设。纽约像现今的北京一样，开始迅速地“摊大饼”，扩大城市之后为了加强各个区域之间的关系，不得不开始修建各种高速路构成的交通网。就这样，一波波拆迁之后，纽约原有的社区形态分崩离析。\n雅各布斯较早地发现了这种建设行为带来的灾难，城市原有的生活复杂性为交通复杂性所取代，居住、工作、商业截然分开，建筑毫无特色。1961年的《纽约时报》上有一则新闻中提到，摩西先生承认有些新住宅很可能是“丑陋、封闭、墨守成规、千篇一律、缺乏个性、没有风格”，但他说：“这样的住宅，只要周围有公园就可以了。”但现实是，1930年代人们为摩西的公园大唱赞歌，在1950年代这些公园变成了犯罪频生、藏污纳垢之所，其原因简单说来正是人口外迁，公园供大于求，城市建设也使得原有社区关系弱化，人与人关系冷漠。\n本书的成就恰恰是针对这种背景的，它强调了城市的复杂性和多样性，认为只有具有多样性的城市社区才是有活力的；它发现了社区内部“街道眼”的存在，因为这本书后来产生了一整套城市设计方法，用以增强社区内部关系，使其具有安全防卫功能的；它还号召居民反对大规模城市建设，主动参与到城市规划的决策中去，发出自己的声音。\n","date":1444521600,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":900,"html":"雅各布斯以纽约、芝加哥等美国大城市为例，以充满激情的文字，深入考察了都市结构的基本元素，以及它们在城市生活中发挥功能的方式。是什么使得街道安全或不安全？是什么构成了街区，它在更大的城市机体中发挥什么样的作用？为什么有些街区仍然贫困，有些街区却获得新生？通过对这些问题的回答，雅各布斯具体阐释了城市的复杂性及其发展取向，也为评估城市的活力提供了一个基本框架。","keywords":null,"kind":"page","lang":"en","lastmod":1444521600,"objectID":"b7027c9d8eeb82190634f5cb75baffd8","permalink":"https://xiangdiwu.github.io/2015/10/11/reflections-on-the-death-and-life-of-great-american-cities/","publishdate":"2015-10-11T00:00:00Z","readingtime":2,"relpermalink":"/2015/10/11/reflections-on-the-death-and-life-of-great-american-cities/","section":"post","tags":null,"title":"《美国大城市的死与生》读后感","type":"post","url":"/2015/10/11/reflections-on-the-death-and-life-of-great-american-cities/","weight":0,"wordcount":876},{"author":null,"categories":["Reading"],"content":"在《物理大牛的八卦》中，费曼被狠狠吐槽：“他有个最大的毛病，就是喜欢装牛B，明明自己也是费尽九牛二虎之力才作出来的，非得装着一晚上想出来的，用来打击别人。”\n费曼就是这么个家伙，虚荣、爱吹牛、喜欢卖弄小聪明，但是他诚实，在《别闹了，费曼先生》中，他全承认了。这不是一本自我吹嘘的书，在书里费曼自己招供，他的缺点和他的优点一样鲜明，但是你却因为他的种种缺点而无法不爱上他。\n世界上有这么一位非常独特的人。他满不在乎地活在这个世界上，靠追逐他的兴趣而指引生活的方向。如同他年幼时自己发明了一套三角函数符号一样，他对世界也有自己的一套符号。所以，大家都在意的事物他未必在意，大家不在意的事物他却穷追不舍。他也爱着这个世界，但是是用他的方式。所以他能在物理之外发现生活中诸多的乐趣和美来，所以不是人人都会到巴西讲学时跑去街头学敲鼓，而且获得了佳年华会的冠军乐队。所以不是人人都会去学绘画，然后还愿意让别人买去挂在妓院的墙上。如孩子一样的眼睛，如孩子一样的好奇心，因此世界以奇迹还赠他，于一生中成就别人的无数生，使得他周围人在失去他时如同整个世界都黯然失色。\n全书中只有一段关于情感的描写，但非常动人：费曼的妻子阿莲得了绝症，而费曼对此无能为力。当时他正参加曼哈顿工程，为美国制造原子弹。他只能在每个周末一次次跑去医院探望妻子，看她日渐憔悴，步向无可避免的死亡。当阿莲去世的时候，费曼没有任何悲伤的感觉，觉得这对于她来说是漫长痛苦后的解脱，而他自己已经早已麻木。书中说，很久以后的一天，他偶然在街道上看见商店橱窗里的一套女装，下意识地想到：阿莲穿上该多好看啊。然后，巨大的悲伤突然无可抑制袭来，他终于失声痛哭。\n《别闹了，费曼先生》，一本理科生写给世界的情书。讲述了在理形世界里，曾经有过这样一个生命，他曾经这样爱过这个世界。\n","date":1437868800,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":800,"html":"理查德·费曼（Richard P.Feyrman）1918年出生，1939年于麻省理工学院毕业后，进入普林斯顿大学研究院，又加入罗拉拉摩斯实验室，对原子弹的发展贡献卓著。1965年与薛温格和朝永振一郎共获诺贝尔物理奖。1988年2月困患癌症辞世。费曼的思想如天马行空，喜自辟蹊径，且从不固执，求知欲极强。他很爱恶作剧，但往往只为点出世间许多荒谬之处。","keywords":null,"kind":"page","lang":"en","lastmod":1437868800,"objectID":"ad415f663c81f998908bad345a2e4823","permalink":"https://xiangdiwu.github.io/2015/07/26/surely-youre-joking-mr-feynman/","publishdate":"2015-07-26T00:00:00Z","readingtime":2,"relpermalink":"/2015/07/26/surely-youre-joking-mr-feynman/","section":"post","tags":null,"title":"《别闹了，费曼先生》读后感","type":"post","url":"/2015/07/26/surely-youre-joking-mr-feynman/","weight":0,"wordcount":754},{"author":null,"categories":null,"content":"在光影交织的艺术长廊中，电影不仅是时间的切片，更是人类情感与思想的镜像。当镜头聚焦于那些被精心雕琢的叙事与画面时，我们得以窥见导演的哲思、时代的脉搏，以及人性深处的复杂光谱。\n2001 Kubrick, Stanley 1968（《漫游太空2001，库布利克》）\nPasolini, Pier Paolo 1961（《乞丐》，帕索里尼）\nBabette\u0026amp;rsquo;s Feast Axel, Gabriel 1987（《芭贝特的盛宴》）\nBattleship Potemkin, The Eisenstein, Sergei 1925（《战舰波将金号》，爱森斯坦）\nBicycle Thieves, The De Sica, Vittorio 1949（《偷自行车的人》，德-西卡）\nBlade Runner: The Director\u0026amp;rsquo;s Cut Scott, Ridley 1991（《银翼杀手》，雷德利-斯科特）\nBlue Velvet Lynch, David 1986（《蓝丝绒》，大卫-林奇）\nChronicle of a Summer Rouch, Jean 1960（《夏日纪事》，让-鲁什）\nClockwork Orange Kubrick, Stanley 1971（《发条橙子》，库布利克）\nCook, The Thief, His Wife and Her Lover, the Greenaway, Peter 1989 （《情欲色香味》，格林纳威）\nCrash Cronenberg, David 1996（《撞车》，柯南博格）\nDemon Seed Cammell, Donald 1977（《魔种》）\nDesk Set Lang, Walter 1957\nDraughtsman\u0026amp;rsquo;s Contract, The Greenaway, Peter 1982（《画师的合约》，格林纳威）\nDreamlife of Angels, The Zonca, Erick 1998（《两极天使》，宗卡）\nEarly Works Richter, Hans 1927\nEraserhead Lynch, David 1977（《橡皮头》，大卫-林奇）\nFifth Element, The Besson, Luc 1997（《第五元素》，吕克-贝松） …","date":1436227200,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":3200,"html":"在光影交织的艺术长廊中，电影不仅是时间的切片，更是人类情感与思想的镜像。当镜头聚焦于那些被精心雕琢的叙事与画面时，我们得以窥见导演的哲思、时代的脉搏，以及人性深处的复杂光谱。","keywords":null,"kind":"page","lang":"en","lastmod":1436227200,"objectID":"d79a8f7066c7d48e3f1f6450d5396e11","permalink":"https://xiangdiwu.github.io/2015/07/07/mit-recommend-150-movies-for-improving-the-art-of-beauty/","publishdate":"2015-07-07T00:00:00Z","readingtime":7,"relpermalink":"/2015/07/07/mit-recommend-150-movies-for-improving-the-art-of-beauty/","section":"post","tags":["movie"],"title":"MIT 推荐提高审美必看的 150 部电影","type":"post","url":"/2015/07/07/mit-recommend-150-movies-for-improving-the-art-of-beauty/","weight":0,"wordcount":3127},{"author":null,"categories":["Reading"],"content":" 人生，无论有多长，始终短暂。 短得让你来不及添加任何东西。—— 辛波斯卡\n不知所云，知所云却不知诗意何在的诗歌非常多。辛波斯卡的诗歌则不存在这种问题，正如诺贝尔文学奖授奖辞中对她的评价一样：看似单纯，却富有意义。她的诗有时近乎白话。\n诗人和诗歌的价值究竟在哪里？或许连诗人自己都“从未真正相信它们的价值”，因此这一称呼总让被称呼者自己也略感尴尬。\n辛波斯卡从不吝于自嘲。在诺贝尔奖颁奖仪式上的演讲中，她颇为幽默的说道：“诗人比较喜欢用笼统的名称‘作家’，或者以写作之外所从事的任何工作的名称来代替‘诗人’。办事官员或公交车乘客发现和自己打交道的对象是一位诗人的时候，会流露出些许怀疑或惊惶的神色。”她又顺带幽了哲学家一默 ，“我想哲学家也许会碰到类似的反应，不过他们还可以为自己的职业冠以学术性头衔——哲学教授——这样听起来体面多了。”这种自嘲在辛波斯卡的诗歌中也随处可见。\n辛波斯卡用一种微带戏谑的目光看待的，不仅仅是自己，还有时间、死亡、爱情、战争、恐怖袭击——沉重的话题在简洁的诗句中变得轻飘，但这轻飘自有另一种分量。\n“即便我爬上了山丘，也无法如玫瑰般盛开/只有玫瑰才能盛开如玫瑰/别的不能”（《企图》，1957年）\n她书写爱情，却从来不多愁善感，也不在这些诗句中投入自己的影子，她更多是抱着一种疏离感，以抽象的我存在于诗歌中，让爱情诗更偏向于哲理诗。\n她写一对恋人沉浸在幸福的爱情里，处处恩爱无匹——\n“请看看那对幸福的恋人/他们难道不能至少试着掩饰一下/看在朋友的份上假装有点难过！/你听他们的笑声——真是刺耳。/他们使用的语言——清楚得让人起疑。/还有他们的庆典、仪式，精心安排互相配合的例行工作——这分明是在人类背后搞鬼！”（《幸福的爱情》，1972年）\n在这种戏谑之后，却又笔锋一转——\n“就让那些从未找到过幸福爱情的人/不断去说世上没有这种东西”。\n她是那种“借用了沉重的字眼，又劳心费神地使它们看似轻松”的诗人。\n世界如此广阔，有这样多的人与我们生活于同一时空，可惜——\n“一如手电筒的光，飞掠过黑暗，/只照亮最靠近的几张脸孔，/其余则被视若无睹地略过，/从未被想起，也没有遗憾”（《巨大的数目》，1976年）\n","date":1392595200,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":1e3,"html":"维斯拉瓦·辛波斯卡（Wislawa Szymborska），1923-2012，一九二三年生于波兰小镇布宁。她擅长以幽默、诗意的口吻描述严肃主题和日常事物，以诗歌回答生活。是波兰最受欢迎的诗人，也是公认为当代最为迷人的诗人之一，享有“诗界莫扎特”的美誉。一九九六年获得诺贝尔文学奖，是文学史上第三位获奖女诗人。","keywords":null,"kind":"page","lang":"en","lastmod":1392595200,"objectID":"b6b12faa7f346c1bd7175274ea4e8082","permalink":"https://xiangdiwu.github.io/2014/02/17/poems-new-and-collected/","publishdate":"2014-02-17T00:00:00Z","readingtime":2,"relpermalink":"/2014/02/17/poems-new-and-collected/","section":"post","tags":null,"title":"《万物静默如谜》读后感","type":"post","url":"/2014/02/17/poems-new-and-collected/","weight":0,"wordcount":900},{"author":null,"categories":["Reading"],"content":" 不管多少回实验的结果和某一理论相一致，你永远不可能断定下一次结果不会和它矛盾。——霍金\n随着科学在19和20世纪变得日益专门化和数学化，哲学家们已经无法理解科学的最新发展，从而“语言分析成为哲学唯一剩下的研究领域（维根斯坦）”。而普通人大概更是只能看个热闹。对于普通人，既不知道我们的问题是否有道理，也不知道找谁能够获得靠谱的回答，只能把问题记录在读书笔记中。\n量子不确定性：\n关于量子不确定性问题：似乎测不准或不确定性是由于观测者必须使用光子（或其他粒子）来测量被观测粒子的位置和速率，被观测对象越小，需要使用波长越短频率越高从而能量也越高的光子，但这样使用的光子会干扰被观测粒子，从而观测者的存在会干扰被观测对象的运动。可见，这一测不准的前提是观测者使用光子来进行测量。尽管光子是人目前能够使用的最有效的工具，但如果未来有一天人能够控制某种现在尚未知的但不会与被观测粒子发生作用的粒子，用它来进行测量，或许就能解决测不准的问题，从而测不准就成了一个阶段性的问题，或者成了技术性而非理论性问题。霍金说的“We could still imagine that there is a set of laws that determine events completely for some supernatural being, who could observe the present state of the universe without disturbing it”，其实也可以认为不排除这一可能。\n奇点：\n作者将广义相对论和量子理论结合，得出宇宙的时空可以有限但没有边缘，并以地球南北极为比喻，从而排除了奇点的必要性，宇宙也可以变得无始无终。“The universe would be completely self-contained and not affected by anything outside itself”。\n记忆：\n讨论记忆的时间箭头时，作者用计算机来比喻人脑，认为尽管计算机的记忆储存是将磁记录由无序变为有序，但由于这一过程要消耗能量，能量消耗带来无序的增加要大于记忆本身带来有序的增加，因此整个过程还是导致无序的增加。因此，记忆的时间箭头只能是和热力学时间箭头的方向一致，即沿着熵增加的方向。\n生命：\n生命只能存在于一维时间和三维空间的时空区域内。这里作 …","date":1389744e3,"dir":"post/Reading/","expirydate":-62135596800,"fuzzywordcount":1200,"html":"《时间简史》讲述是探索时间和空间核心秘密的故事，是关于宇宙本性的最前沿知识，包括我们的宇宙图像、空间和时间、膨胀的宇宙不确定性原理、基本粒子和自然的力、黑洞、黑洞不是这么黑、时间箭头等内容。第一版中的许多理论预言，后来在对微观或宏观宇宙世界观测中得到证实。","keywords":null,"kind":"page","lang":"en","lastmod":1389744e3,"objectID":"eaf064d9aa8ab6cc0897e0dcaec4ed7d","permalink":"https://xiangdiwu.github.io/2014/01/15/reflections-on-a-brief-history-of-time/","publishdate":"2014-01-15T00:00:00Z","readingtime":3,"relpermalink":"/2014/01/15/reflections-on-a-brief-history-of-time/","section":"post","tags":null,"title":"《时间简史》读后感","type":"post","url":"/2014/01/15/reflections-on-a-brief-history-of-time/","weight":0,"wordcount":1141},{"author":null,"categories":null,"content":"Hi，我是吴湘菂，一个金融量化投资相关技术布道者及实践者。\nAbout Me Xiangdi Wu is a quant researcher. She has a solid experience in the quantitative investment and financial industry for more than 8 years.\nThroughout her career, she has built a number of quantitative investment strategies and has been involved in the development of quantitative investment tools.\nShe loves open source and has been contributing to various open source projects.\nShe also has strong interests in various technical topics such as Artificial Intelligence, high-frequency trading, multi-factor models, and machine learning applications in quantitative finance. Additionally, she is passionate about building distributed systems and open-source quantitative platforms. She loves sharing her ideas about these things in her blog.\nCurrently, Xiangdi works as a quant researcher at a top-tier quant hedge fund, and also wears the hat of Hilbert Quant Community Manager. She is also the co-founder of the Quant-Qlib open source project.\nWhile she is …","date":-62135596800,"dir":"about/","expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://xiangdiwu.github.io/about/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/about/","section":"","tags":null,"title":"","type":"page","url":"/about/","weight":0,"wordcount":252},{"author":null,"categories":null,"content":"","date":-62135596800,"dir":"search/","expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8946788897930c0c0c39fbfcd30ff2e4","permalink":"https://xiangdiwu.github.io/search/placeholder/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/placeholder/","section":"search","tags":null,"title":"","type":"search","url":"/search/placeholder/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":"——2025—— 延迟 统计文件读取速度、因子值计算和回测所需的时间是重要的。如果一个算子每计算一次要花费3-5分钟，可能等计算完毕，行情 bar 已经接近走完，那么回测层面可能意味着回测结果不准确，交易层面可能意味着错失机会。通常我们会去计算“从数据获取到产生交易信号”与“实时行情”的时间差（delay）。对于 15 分钟的模型，这个延迟至少要控制在 10 秒内（大概 4 秒左右），如果使用 C++ 编写，这个延迟可以控制到 20 毫秒内。从另一个角度来说，如果我们想知道做出来的模型能在什么分钟级别去实盘，不仅仅是看那些回测指标，还要看回测延迟占 bar 的时间比例。比如策略是在一分钟上，而模型的延迟是十秒。那很可能这个模型就是不能使用的。再具体一点，要去观察每个一分钟的 bar 上，它的前十秒是不是已经把这一分钟的行情走完？剩下的行情在这个一分钟 bar 上又占了多少？\n——2024—— 激励 员工薪酬和激励制度：能力和岗位的最佳匹配，决定职位和薪酬；一过性的功劳应该对应一笔确切的支付；战略性的功劳应该给予股权。 ​​​\n指数级反弹 指数级别的反弹出现通常较为突然，速度较快，经常出现第一时间反应不过来，这是非常正常的现象，不过，一般而言，指数级别反弹吸引资金流入后，后续市场会出现2-3个月的结构性行情，而且这种结构性行情一定是存在一条清晰的主线进行引领。 复盘过去五轮指数级别反弹后，市场选择主线的思路，无外乎从政策催化、宏观趋势、产业趋势和增量资金方向四个角度考量。\nBRK 模式 研究BRK越多，就越明白BRK的模式无法复制。\n早期BRK的超额收益很多来自于巴芒的选股能力，但随着市场越来越有效，同时BRK的规模接近万亿，获取超额收益就变得异常困难。 目前BRK得以持续增长的关键之一在于浮存金。可以看到BRK早年的浮存金成本远低于国债收益，甚至成本为负。这意味着即便BRK的钱不用于投资，只是存在银行，也可以稳稳套利美联储。 所以当利率高于浮存金成本，BRK就可以轻松跑赢指数，而利率低于浮存金成本，BRK业绩就会被拖累。 这也是为什么过去20年巴菲特跑不赢指数，因为过去20年是非常宽松的20年。而单看高利率的年份，BRK都跑赢了指数。\n目前BRK的浮存金成本大概是2-3%，当美债利率高于3%时，BRK相较于指数存在优势，美股BRK也就值得投资。\n巴菲特投日本资产 对 …","date":-62135596800,"dir":"vibe/","expirydate":-62135596800,"fuzzywordcount":7e3,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"362ebed09ed6ed66ca9fbefb20f1389e","permalink":"https://xiangdiwu.github.io/vibe/","publishdate":"0001-01-01T00:00:00Z","readingtime":14,"relpermalink":"/vibe/","section":"","tags":null,"title":"Vibe","type":"page","url":"/vibe/","weight":0,"wordcount":6946},{"author":null,"categories":null,"content":" “Do not go gentle into that good night”\nIn-N-Out Burger - Milpitas, USA, 2024.05 In-N-Out 是加州才有的快餐连锁店，加州人对 In-N-Out 有着特殊的感情。在疫情之后，其他的快餐店都涨价了，In-N-Out 仍然保持着之前的价格，并且其食材新鲜，口味独特，是加州人的最爱。其装修风格也很有特色，简洁明快的红白色调，再加上墙上加州风格的装饰画，让人感觉心情愉悦。\n雕像：在图书馆前玩耍的孩子们 - Milpitas 图书馆，Milpitas，美国，2024.05 在图书馆里阅读一本童话，在草地上和小伙伴一起嬉闹，无忧无虑的童年是最美好的时光。 无名礁石 - Santa Cruz，美国，2024.05 大自然是最耐心的雕刻家，以海浪为刻刀，用千百年的时间将礁石打磨成各种奇妙的形状。\n冲浪的情侣 - Santa Cruz，美国，2024.05 Santa Cruz 是加州最著名的冲浪胜地之一，这对情侣正在准备下海冲浪。\n雅女湖 - 四川，眉山 2024.01 清晨的湖面平静如镜，湖边的树木和远处的瓦屋山倒影在湖中，如同一幅水墨画。\n金门大桥 - 旧金山，美国，2022.11 巨大的红色桥塔横跨在桥面上，如同一个昂首阔步的巨人。\n唐人街 - 旧金山，美国，2022.11 传奇武术大师李小龙在旧金山留下的印记至今仍然清晰可见。\n芝加哥 - 芝加哥，美国，2022.11 芝加哥是美国很少的几个有摩天大楼的城市之一，这里的建筑风格独具特色，具有强烈的现代感。\n穿藏族服装的少女 - 理塘，甘孜，2022.08 青春的少女穿着藏族服装，背景是一片蓝天白云，仿佛置身于天空之城。 兴伊措 - 227国道稻城和理塘之间，甘孜，2022.08 开车途经一段异常崎岖，遍地石头的路段，突然开阔起来，一片湖泊映入眼帘，湖水清澈见底，周围群山环绕，仿佛置身于仙境。\n香格里拉圣路 - 227国道稻城和理塘之间，甘孜，2022.08 前方黑色的云层中正酝酿着一场风暴，巨大的环状旋风连接了天空和地面，仿佛在天地之间打开了一个通道。\n海子山地貌 - 227国道稻城和理塘之间，海子山，甘孜，2022.08 海子山上遍布巨大的冰川漂砾，蛮荒得如同来到了火星。\n第四纪冰川形成的石头河 - 227国道稻城和理塘之间，海子山，甘 …","date":-62135596800,"dir":"travel/","expirydate":-62135596800,"fuzzywordcount":1300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f0c10fb0daa3b8dc0c0e33e2f272daa0","permalink":"https://xiangdiwu.github.io/travel/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/travel/","section":"","tags":null,"title":"旅行日志","type":"page","url":"/travel/","weight":0,"wordcount":1255}]