<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：集成学习"><meta property="og:title" content="机器学习：集成学习"><meta property="twitter:title" content="机器学习：集成学习"><meta name=description content="本文主要介绍集成学习的基本概念、基本思想、基本方法和基本流程。"><meta property="og:description" content="本文主要介绍集成学习的基本概念、基本思想、基本方法和基本流程。"><meta property="twitter:description" content="本文主要介绍集成学习的基本概念、基本思想、基本方法和基本流程。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：集成学习-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：集成学习</h1><h2 class=subheading>Ensemble Learning</h2><span class=meta>Posted by
XiangdiWu
on
Monday, October 5, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=个体与集成>个体与集成</h1><p><strong>集成学习(ensemble learning)</strong> 通过构建并结合多个学习期来完成学习任务，有时也被称为<strong>多分类器系统(multi-classifier system)</strong>、<strong>基于委员会的学习(committee-based learning)</strong> 等。</p><p>集成学习的思路如下：首先产生一组<strong>个体学习器(indicidual learner)</strong>，再用某种策略将它们结合起来。个体学习器(基学习器)通常由一个现有的学习算法(基学习算法)从训练数据中产生，例如决策树、BP神经网络等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，这样的集成是<strong>同质的(homogeneous)</strong>，同质集成中的个体学习器亦称为<strong>基学习器(base learner)</strong>。集成也可以包含不同类型的个体学习器，此时称其为<strong>异质的(heterogenous)</strong>，相应的个体学习器一般不称基学习器，而是称为<strong>组件学习器(component learner)</strong>。</p><div align=center><img src=/Kimages/2/image-20200429101621224.png style=zoom:30%></div><p>历史上，Kearns和Valiant首先提出了<strong>强可学习(strongly learnable)<strong>和</strong>弱可学习(weakly learnable)</strong> 的概念。他们指出：在<strong>概率近似正确(probably approximately correct, PAC)</strong> 学习的框架中，一个概念(一个类)，如果存在一个多项式的学习算法能够学习它，并且正确率很高，就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，就称这个概念是弱可学习的。后来Schapire证明强可学习和弱可学习是等价的，也就是说，<strong>在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的</strong>。集成学习便是研究如何将弱学习算法提升为强学习算法的理论。</p><p>集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对<strong>弱学习器</strong>尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。但需注意的是，虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中，<strong>往往会使用比较强的学习器进行集成</strong>。</p><p>在经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习要获得比最好的单一学习器更好的性能，个体学习器应“<strong>好而不同</strong>”，即<strong>个体学习器要同时具备准确性和多样性</strong>。</p><p>根据个体学习器的生成方式，目前集成学习方法大致可分为两大类，即个体学习器之间存在强依赖关系、必须串行生成的<strong>序列化方法</strong>，以及个体学习器间不存在强依赖关系，可同时生成的<strong>并行化方法</strong>。前者的代表是<strong>Boosting</strong>，后者的代表是<strong>Bagging和随机森林</strong>。</p><h1 id=集成学习的误差分析>集成学习的误差分析</h1><p>给定一个学习任务，假设输入$x$与输出$y$的真实关系为$y=h(x)$。对于$M$个不同的模型$f_1(x),\cdots,f_M(x)$，每个模型的<strong>期望错误</strong>为：</p>$$
\begin{aligned}
\mathcal{R}(f_{m}) &=\mathbb{E}_{{x}}[(f_{m}({x})-h({x}))^{2}] \\
&=\mathbb{E}_{{x}}[\epsilon_{m}({x})^{2}]
\end{aligned}
$$<p>其中$\epsilon_{m}(\boldsymbol{x})=f_{m}({x})-h({x})$为模型$m$在样本$x$上的错误。那么所有模型的平均错误为：</p>$$
\bar{\mathcal{R}}(f)=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\boldsymbol{x}}[\epsilon_{m}({x})^{2}]
$$<p>使用<strong>投票法</strong>进行模型结合时，最终模型的输出为：</p>$$
F({x})=\frac{1}{M} \sum_{m=1}^{M} f_{m}({x})
$$<p>对于投票法的误差，有如下定理：对于$M$个不同的模型$f_1(x),\cdots,f_M(x)$，其平均期望错误为$\bar{\mathcal R}(f)$。基于简单投票机制的集成模型$F(x)$的<strong>期望错误</strong>位于$\frac{1}{M}\bar{\mathcal R}(f)$和$\bar{\mathcal R}(f)$之间。证明如下：</p><p>根据定义，集成模型的期望错误为：</p>$$
\begin{aligned}
\mathcal{R}(F) &=\mathbb{E}_x\left[\left(\frac{1}{M} \sum_{m=1}^{M} f_{m}({x})-h({x})\right)^{2}\right] \\
&=\mathbb{E}_x\left[\left(\frac{1}{M} \sum_{m=1}^{M} \epsilon_{m}({x})\right)^{2}\right] \\
&=\frac{1}{M^{2}} \mathbb{E}_x\left[\sum_{m=1}^{M} \sum_{n=1}^{M} \epsilon_{m}({x}) \epsilon_{n}({x})\right] \\
&=\frac{1}{M^{2}} \sum_{m=1}^{M} \sum_{n=1}^{M} \mathbb{E}_{{x}}[\epsilon_{m}({x}) \epsilon_{n}({x})]
\end{aligned}
$$<p>其中</p>$$\mathbb{E}_{\boldsymbol{ x }}[\epsilon_{m}({x}) \epsilon_{n}({x})]$$<p>是两个<strong>不同模型错误的相关性</strong>。若每个模型的错误<strong>不相关</strong>，即</p>$$\forall m \neq n, \mathbb{E}_{{x}}[\epsilon_{m}({x}) \epsilon_{n}({x})]=0$$<p>，此时$\mathcal R(F)$达到<strong>下界</strong>。如果每个模型的错误都是<strong>相同的</strong>，则$\forall m \neq n, \epsilon_{m}({x})=\epsilon_{n}({x})$，此时$\mathcal R(F)$达到<strong>上界</strong>。并且由于$\epsilon_{m}({x}) \geq 0, \forall m$，可以得到：</p>$$
\bar{\mathcal{R}}(f) \geq \mathcal{R}(F) \geq \frac{1}{M} \bar{\mathcal{R}}(f)
$$<p>即集成模型的期望错误大于等于所有模型的平均期望错误的$1/M$，小于等于所有模型的平均期望错误。因此模型之间的差异性对于提升模型集成效果是十分重要的。</p><h1 id=boosting>Boosting</h1><p>Boosting类集成模型的目标是学习一个<strong>加性模型(additive model)</strong>。：</p>$$
F({x})=\sum_{m=1}^{M} \alpha_{m} f_{m}({x})
$$<p>其中$f_m(x)$为弱分类器或基分类器，$\alpha_m$为弱分类器的继承权重，$F(x)$称为强分类器。</p><p>Boosting类方法的关键是如何训练每个<strong>弱分类器</strong>$f_m(x)$以及对应的<strong>权重</strong>。为了提高集成的效果，应当<strong>尽量使得每个弱分类器的差异尽可能大</strong>。一种有效的算法是采用迭代的方法来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。在学习了第$m$个弱分类器后，<strong>增加其分错样本的权重</strong>，使第$m+1$个弱分类器<strong>更关注于前面弱分类器分错的样本</strong>。这样增加每个弱分类器的差异，最终提升集成分类器的准确率。这种方法称为<strong>AdaBoost算法</strong>。</p><div align=center><img src=/Kimages/2/image-20200429101848886.png style=zoom:30%></div><p>AdaBoost算法是一种<strong>迭代式的训练算法</strong>，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少分对样本的权重，从而得到一个新的数据分布。</p><p>以<strong>二分类</strong>为例，AdaBoost算法的训练过程如下所示。<strong>最初赋予每个样本同样的权重</strong>，在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器，然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。</p><div align=center><img src=/Kimages/2/image-20200429154348279.png style=zoom:40%></div><h1 id=基于numpy的adaboost算法实现>基于numpy的AdaBoost算法实现</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Adaboost</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, n_estimators<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators <span style=color:#ff79c6>=</span> n_estimators  <span style=color:#6272a4># 弱分类器个数</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Adaboost拟合算法</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        m, n <span style=color:#ff79c6>=</span> X<span style=color:#ff79c6>.</span>shape
</span></span><span style=display:flex><span>        <span style=color:#6272a4># (1) 初始化权重分布为均匀分布 1/N</span>
</span></span><span style=display:flex><span>        w <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>full(m, (<span style=color:#bd93f9>1</span><span style=color:#ff79c6>/</span>m))
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 基分类器列表</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># (2) for m in (1,2,...,M)</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators):
</span></span><span style=display:flex><span>            <span style=color:#6272a4># (2.a) 训练一个弱分类器：决策树桩</span>
</span></span><span style=display:flex><span>            estimator <span style=color:#ff79c6>=</span> DecisionStump()
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 设定一个最小化误差</span>
</span></span><span style=display:flex><span>            min_error <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>float</span>(<span style=color:#f1fa8c>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 遍历数据集特征，根据最小分类误差率选择最优划分特征</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(n):
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 获取特征值</span>
</span></span><span style=display:flex><span>                values <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>expand_dims(X[:, i], axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 特征取值去重</span>
</span></span><span style=display:flex><span>                unique_values <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>unique(values)
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 尝试将每一个特征值作为分类阈值</span>
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>for</span> threshold <span style=color:#ff79c6>in</span> unique_values:
</span></span><span style=display:flex><span>                    p <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 初始化所有预测值为1</span>
</span></span><span style=display:flex><span>                    pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>ones(np<span style=color:#ff79c6>.</span>shape(y))
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 小于分类阈值的预测值为-1</span>
</span></span><span style=display:flex><span>                    pred[X[:, i] <span style=color:#ff79c6>&lt;</span> threshold] <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 2.b 计算误差率</span>
</span></span><span style=display:flex><span>                    error <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>sum</span>(w[y <span style=color:#ff79c6>!=</span> pred])
</span></span><span style=display:flex><span>                    
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 如果分类误差大于0.5，则进行正负预测翻转</span>
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 例如 error = 0.6 =&gt; (1 - error) = 0.4</span>
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>if</span> error <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0.5</span>:
</span></span><span style=display:flex><span>                        error <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> error
</span></span><span style=display:flex><span>                        p <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 一旦获得最小误差则保存相关参数配置</span>
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>if</span> error <span style=color:#ff79c6>&lt;</span> min_error:
</span></span><span style=display:flex><span>                        estimator<span style=color:#ff79c6>.</span>label <span style=color:#ff79c6>=</span> p
</span></span><span style=display:flex><span>                        estimator<span style=color:#ff79c6>.</span>threshold <span style=color:#ff79c6>=</span> threshold
</span></span><span style=display:flex><span>                        estimator<span style=color:#ff79c6>.</span>feature_index <span style=color:#ff79c6>=</span> i
</span></span><span style=display:flex><span>                        min_error <span style=color:#ff79c6>=</span> error
</span></span><span style=display:flex><span>                        
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 2.c 计算基分类器的权重</span>
</span></span><span style=display:flex><span>            estimator<span style=color:#ff79c6>.</span>alpha <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span> <span style=color:#ff79c6>*</span> np<span style=color:#ff79c6>.</span>log((<span style=color:#bd93f9>1.0</span> <span style=color:#ff79c6>-</span> min_error) <span style=color:#ff79c6>/</span> (min_error <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1e-9</span>))
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 初始化所有预测值为1</span>
</span></span><span style=display:flex><span>            preds <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>ones(np<span style=color:#ff79c6>.</span>shape(y))
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 获取所有小于阈值的负类索引</span>
</span></span><span style=display:flex><span>            negative_idx <span style=color:#ff79c6>=</span> (estimator<span style=color:#ff79c6>.</span>label <span style=color:#ff79c6>*</span> X[:, estimator<span style=color:#ff79c6>.</span>feature_index] <span style=color:#ff79c6>&lt;</span> estimator<span style=color:#ff79c6>.</span>label <span style=color:#ff79c6>*</span> estimator<span style=color:#ff79c6>.</span>threshold)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 将负类设为 &#39;-1&#39;</span>
</span></span><span style=display:flex><span>            preds[negative_idx] <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 2.d 更新样本权重</span>
</span></span><span style=display:flex><span>            w <span style=color:#ff79c6>*=</span> np<span style=color:#ff79c6>.</span>exp(<span style=color:#ff79c6>-</span>estimator<span style=color:#ff79c6>.</span>alpha <span style=color:#ff79c6>*</span> y <span style=color:#ff79c6>*</span> preds)
</span></span><span style=display:flex><span>            w <span style=color:#ff79c6>/=</span> np<span style=color:#ff79c6>.</span>sum(w)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 保存该弱分类器</span>
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators<span style=color:#ff79c6>.</span>append(estimator)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义预测函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>predict</span>(<span style=font-style:italic>self</span>, X):
</span></span><span style=display:flex><span>        m <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(X)
</span></span><span style=display:flex><span>        y_pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>zeros((m, <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 计算每个弱分类器的预测值</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> estimator <span style=color:#ff79c6>in</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 初始化所有预测值为1</span>
</span></span><span style=display:flex><span>            predictions <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>ones(np<span style=color:#ff79c6>.</span>shape(y_pred))
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 获取所有小于阈值的负类索引</span>
</span></span><span style=display:flex><span>            negative_idx <span style=color:#ff79c6>=</span> (estimator<span style=color:#ff79c6>.</span>label <span style=color:#ff79c6>*</span> X[:, estimator<span style=color:#ff79c6>.</span>feature_index] <span style=color:#ff79c6>&lt;</span> estimator<span style=color:#ff79c6>.</span>label <span style=color:#ff79c6>*</span> estimator<span style=color:#ff79c6>.</span>threshold)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 将负类设为 &#39;-1&#39;</span>
</span></span><span style=display:flex><span>            predictions[negative_idx] <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 2.e 对每个弱分类器的预测结果进行加权</span>
</span></span><span style=display:flex><span>            y_pred <span style=color:#ff79c6>+=</span> estimator<span style=color:#ff79c6>.</span>alpha <span style=color:#ff79c6>*</span> predictions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 返回最终预测结果</span>
</span></span><span style=display:flex><span>        y_pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sign(y_pred)<span style=color:#ff79c6>.</span>flatten()
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> y_pred
</span></span></code></pre></div><h1 id=使用scikit-learn中的adaboost算法完成自定义数据集上的分类任务>使用scikit-learn中的AdaBoost算法完成自定义数据集上的分类任务</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.ensemble <span style=color:#ff79c6>import</span> AdaBoostClassifier
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.tree <span style=color:#ff79c6>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets <span style=color:#ff79c6>import</span> make_gaussian_quantiles
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 构建自定义数据集</span>
</span></span><span style=display:flex><span>X1, y1 <span style=color:#ff79c6>=</span> make_gaussian_quantiles(cov<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2.</span>,
</span></span><span style=display:flex><span>                                 n_samples<span style=color:#ff79c6>=</span><span style=color:#bd93f9>200</span>, n_features<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>,
</span></span><span style=display:flex><span>                                 n_classes<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>X2, y2 <span style=color:#ff79c6>=</span> make_gaussian_quantiles(mean<span style=color:#ff79c6>=</span>(<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>3</span>), cov<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1.5</span>,
</span></span><span style=display:flex><span>                                 n_samples<span style=color:#ff79c6>=</span><span style=color:#bd93f9>300</span>, n_features<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>,
</span></span><span style=display:flex><span>                                 n_classes<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>concatenate((X1, X2))
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>concatenate((y1, <span style=color:#ff79c6>-</span> y2 <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 基学习器使用决策树</span>
</span></span><span style=display:flex><span>bdt <span style=color:#ff79c6>=</span> AdaBoostClassifier(DecisionTreeClassifier(max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>), algorithm<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;SAMME&#34;</span>, n_estimators<span style=color:#ff79c6>=</span><span style=color:#bd93f9>200</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bdt<span style=color:#ff79c6>.</span>fit(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plot_colors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;br&#34;</span>
</span></span><span style=display:flex><span>plot_step <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.02</span>
</span></span><span style=display:flex><span>class_names <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;AB&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>figure(figsize<span style=color:#ff79c6>=</span>(<span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>5</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 绘制决策边界</span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>121</span>)
</span></span><span style=display:flex><span>x_min, x_max <span style=color:#ff79c6>=</span> X[:, <span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>min() <span style=color:#ff79c6>-</span> <span style=color:#bd93f9>1</span>, X[:, <span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>max() <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>y_min, y_max <span style=color:#ff79c6>=</span> X[:, <span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>min() <span style=color:#ff79c6>-</span> <span style=color:#bd93f9>1</span>, X[:, <span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>max() <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>xx, yy <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>meshgrid(np<span style=color:#ff79c6>.</span>arange(x_min, x_max, plot_step),
</span></span><span style=display:flex><span>                     np<span style=color:#ff79c6>.</span>arange(y_min, y_max, plot_step))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Z <span style=color:#ff79c6>=</span> bdt<span style=color:#ff79c6>.</span>predict(np<span style=color:#ff79c6>.</span>c_[xx<span style=color:#ff79c6>.</span>ravel(), yy<span style=color:#ff79c6>.</span>ravel()])
</span></span><span style=display:flex><span>Z <span style=color:#ff79c6>=</span> Z<span style=color:#ff79c6>.</span>reshape(xx<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>cs <span style=color:#ff79c6>=</span> plt<span style=color:#ff79c6>.</span>contourf(xx, yy, Z, cmap<span style=color:#ff79c6>=</span>plt<span style=color:#ff79c6>.</span>cm<span style=color:#ff79c6>.</span>Paired)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>axis(<span style=color:#f1fa8c>&#34;tight&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i, n, c <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(<span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>2</span>), class_names, plot_colors):
</span></span><span style=display:flex><span>    idx <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>where(y <span style=color:#ff79c6>==</span> i)
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter(X[idx, <span style=color:#bd93f9>0</span>], X[idx, <span style=color:#bd93f9>1</span>], c<span style=color:#ff79c6>=</span>c, cmap<span style=color:#ff79c6>=</span>plt<span style=color:#ff79c6>.</span>cm<span style=color:#ff79c6>.</span>Paired, s<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>, edgecolor<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;k&#39;</span>, label<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Class </span><span style=color:#f1fa8c>%s</span><span style=color:#f1fa8c>&#34;</span> <span style=color:#ff79c6>%</span> n)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlim(x_min, x_max)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylim(y_min, y_max)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>legend(loc<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;upper right&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Decision Boundary&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>twoclass_output <span style=color:#ff79c6>=</span> bdt<span style=color:#ff79c6>.</span>decision_function(X)
</span></span><span style=display:flex><span>plot_range <span style=color:#ff79c6>=</span> (twoclass_output<span style=color:#ff79c6>.</span>min(), twoclass_output<span style=color:#ff79c6>.</span>max())
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>122</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i, n, c <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(<span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>2</span>), class_names, plot_colors):
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>hist(twoclass_output[y <span style=color:#ff79c6>==</span> i], bins<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>, <span style=color:#8be9fd;font-style:italic>range</span><span style=color:#ff79c6>=</span>plot_range, facecolor<span style=color:#ff79c6>=</span>c, label<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;Class </span><span style=color:#f1fa8c>%s</span><span style=color:#f1fa8c>&#39;</span> <span style=color:#ff79c6>%</span> n, alpha<span style=color:#ff79c6>=</span><span style=color:#bd93f9>.5</span>, edgecolor<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;k&#39;</span>)
</span></span><span style=display:flex><span>x1, x2, y1, y2 <span style=color:#ff79c6>=</span> plt<span style=color:#ff79c6>.</span>axis()
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>axis((x1, x2, y1, y2 <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1.2</span>))
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>legend(loc<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;upper right&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#39;Samples&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#39;Score&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Decision Scores&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>tight_layout()
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplots_adjust(wspace<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.35</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=bagging与随机森林>Bagging与随机森林</h1><h2 id=bagging>Bagging</h2><p>Bagging是并行集成学习方法最著名的代表，其能在训练数据的变体上拟合多个模型。训练数据的变体使用自助采样法创建，即每个组件估计器只学习数据集中的一部分，而不是整个数据集。照这样，我们可采样出$T$个含$m$个训练样本的采样集，然后基于每个采样集训练一个基学习器，然后将这些基学习器进行结合。在对预测输出进行结合时，<strong>Bagging通常对分类任务采用简单投票法，对回归任务采用简单平均法</strong>。</p><p>假定基学习器的计算复杂度为$O(m)$，则Bagging的复杂度大致为$T(O(m)+O(s))$。考虑到采样与投票/平均过程的复杂度$O(s)$很小，而$T$通常是一个不太大的常数，因此，<strong>训练一个Bagging集成与直接使用基学习算法训练一个学习器的复杂度同阶</strong>，这说明Bagging是一个很高效的集成学习算法。另外，与AdaBoost只适用于二分类任务不同，Bagging能不经修改地用于多分类、回归等任务。</p><p>自助采样过程还给Bagging带来另一个优点：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用于验证集对泛化性能进行<strong>包外估计(out-of-bag estimate)</strong>。</p><p>事实上，包外样本还有许多其他用途。例如当基学习器是决策树时，可<strong>使用包外样本来辅助剪枝</strong>，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可<strong>使用包外样本来辅助早期停止以减小过拟合风险</strong>。</p><p>从偏差-方差分析的角度看，<strong>Bagging主要关注降低方差</strong>，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。</p><h2 id=随机森林>随机森林</h2><p><strong>随机森林(random forest, RF)</strong> 是Bagging的一个变体。<strong>RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择</strong>。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合(假设有$d$个属性)中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含$k$个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数$k$控制了<strong>随机性的引入程度</strong>：若令$k=d$，则基决策树的构建与传统决策树相同；若令$k=1$，则是随机选择一个属性用于划分；一般情况下，推荐值为$k=\log_2 d$。</p><p>随机森林中的基学习器的<strong>多样性不仅来自样本扰动，还来自属性扰动</strong>。这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步提升。下图为单个决策树和随机森林所学习到的决策边界。</p><div align=center><img src=/Kimages/2/image-20200429101757276.png style=zoom:35%></div><h1 id=结合策略>结合策略</h1><p>学习器结合可能会从三个方面带来好处：(1) 从<strong>统计</strong>的方面来看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险；(2) 从<strong>计算</strong>的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入局部极小点的风险；(3) 从<strong>表示</strong>的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大有可能学得更好的近似。学习器的三种常见结合策略如下：</p><p>(1) <strong>平均法(averaging)</strong>：对数值型输出(回归)最常见的结合策略，可以分为简单平均和加权平均等策略。</p><p>(2) <strong>投票法(voting)</strong>：对分类任务最常见的结合策略。可分为绝对多数投票法(若某标记得票超过半数，则预测为该标记，否则拒绝预测)、相对多数投票法(预测为得票最多的标记，若同时有多个标记获得最高票，则从中随机选择一个)和加权投票法等。</p><p>(3) <strong>学习法(learning)</strong>：通过另一个学习器来学习基学习器的结果，<strong>Stacking</strong>是学习法的典型代表。在学习法中，我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。</p><div align=center><img src=/Kimages/2/image-20200429110022093.png style=zoom:30%></div><p>Stacking先从初始训练集训练处初级学习器，然后生成一个新数据集用于训练次级学习器。在新数据集中，初级学习器的输出被当做样例输入特征，而初始样本标记仍被当做样例标记。</p><h1 id=提升树模型>提升树模型</h1><p><strong>提升树(boosting tree)</strong> 是弱分类器为决策树的提升方法。针对提升树模型，加性模型和前向分步算法的组合是典型的求解方式。当损失函数为平方损失和指数损失时，前向分步算法的每一步迭代都较为容易，但如果是一般的损失函数，前向分步算法的迭代并不容易。所以，有研究提出使用损失函数的负梯度在当前模型的值来求解一般的提升树模型。这种基于负梯度求解提升树前向分步迭代过程的方法也叫<strong>提升树模型(gradient boosting tree)</strong>。</p><p>本节主要介绍GBDT模型，其改进算法XGBoost、LightGBM和CatBoost等不作详细介绍，可以阅读参考资料中的链接进行学习。</p><p>梯度提升决策树GBDT(gradient boosting decision tree)是以CART决策树为基模型的提升树模型。针对分类问题的及模型为二叉分类树，对应梯度提升模型就叫GBDT；针对回归问题的基模型为二叉回归树，对应的梯度提升模型叫做<strong>GBRT(gradient boosting regression tree)</strong>。</p><p>一个提升树模型的数学表达为：</p>$$
f_M(x) = \sum_{m=1}^M T(x; \theta_m)
$$<p>其中，$T(x; \theta_m)$为决策树基模型，$\theta_m$为决策树参数，$M$为决策树个数。</p><p>以梯度提升回归树GBRT为例，当确定初始提升树模型$f_0(x) = 0$，第$m$个决策树表示为$f_m(x) = f_{m-1}(x) + T(x; \theta_m)$。根据前向分步苏纳法，可以使用经验风险最小化来确定下一棵决策树的参数$\theta_m$：</p>$$
\hat{\theta}_m = \arg \min_{\theta_m} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + T(x_i, \theta_m))
$$<p>假设回归树的损失函数为平方损失，即$L(y_i, f_{m-1}(x_i) + T(x_i, \theta_m)) = [y - f_{m-1}(x) - T(x; \theta_m)]^2$，令$r = y - f_{m-1}(x)$，则损失函数可以表示为：</p>$$
L(y_i, f_{m-1}(x_i) + T(x_i, \theta_m)) = [r - T(x; \theta_m)]^2
$$<p>可以看出，<strong>提升树模型每一次迭代都是在拟合一个残差函数</strong>。当损失函数为均方误差损失时(如上)，上式是容易求解的。但在大多数情况下，<strong>一般损失函数很难直接优化求解</strong>，因而就有了基于负梯度求解提升树模型的<strong>梯度提升树模型</strong>。梯度提升树以梯度下降的方法，使用损失函数的负梯度在当前模型的值作为回归提升树中残差的近似值：</p>$$
r_{mi} = - \left[\frac{\part L(y_i, f(x_i))}{\part f(x_i)}\right]_{f(x) = f_{m-1}(x)}
$$<p>所以，综合提升树模型、前向分步算法和梯度提升，给定训练集$D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，GBDT算法的流程可以归纳为以下步骤：</p><p>(1) 初始化提升树模型：</p>$$
f_0(x) = \arg \min_{c} \sum_{i=1}^N L(y_i, c)
$$<p>(2) 对$m = 1, 2, \cdots, M$，有</p><ul><li>对每个样本$i = 1, 2, \cdots, N$，计算负梯度拟合的残差：</li></ul>$$
r_{mi} = - \left[\frac{\part L(y_i, f(x_i))}{\part f(x_i)}\right]_{f(x) = f_{m-1}(x)}
$$<ul><li><p>将该残差作为样本新的真实值，并将数据$(x, r_{mi}), i = 1, 2, \cdots, N$作为下一棵树的训练数据，得到一棵新的提升树$f_m(x)$，其对应的叶子结点区域为$R_{mj}, j = 1, 2, \cdots, J$，其中$J$为树$T$的叶子结点的个数。</p></li><li><p>对叶子区域$j = 1, 2, ,\cdots, J$，计算最优拟合值：</p></li></ul>$$
c_{mj} = \arg \min_{c}\sum_{x_i \in R_{mj}} L(y_i, f_{m-1}(x_i) + c)
$$<ul><li>更新提升树模型：</li></ul>$$
f_m(x) = f_{m-1}(x) + \sum_{j=1}^{L}c_{mj}I(x \in R_{mj})
$$<ul><li>得到最终的梯度提升树模型：</li></ul>$$
f(x) = f_M(x) = \sum_{m=1}^{M}\sum_{j=1}^{J}c_{mj}I(x \in R_{mj})
$$<h1 id=从零开始实现一个gbdt算法系统>从零开始实现一个GBDT算法系统</h1><h2 id=cart决策树算法的实现>CART决策树算法的实现</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义二叉特征分裂函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>feature_split</span>(X, feature_i, threshold):
</span></span><span style=display:flex><span>    split_func <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(threshold, <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>or</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(threshold, <span style=color:#8be9fd;font-style:italic>float</span>):
</span></span><span style=display:flex><span>        split_func <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>lambda</span> sample: sample[feature_i] <span style=color:#ff79c6>&gt;=</span> threshold
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        split_func <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>lambda</span> sample: sample[feature_i] <span style=color:#ff79c6>==</span> threshold
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X_left <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([sample <span style=color:#ff79c6>for</span> sample <span style=color:#ff79c6>in</span> X <span style=color:#ff79c6>if</span> split_func(sample)])
</span></span><span style=display:flex><span>    X_right <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([sample <span style=color:#ff79c6>for</span> sample <span style=color:#ff79c6>in</span> X <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> split_func(sample)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> np<span style=color:#ff79c6>.</span>array([X_left, X_right])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算基尼指数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>calculate_gini</span>(y):
</span></span><span style=display:flex><span>    y <span style=color:#ff79c6>=</span> y<span style=color:#ff79c6>.</span>tolist()
</span></span><span style=display:flex><span>    probs <span style=color:#ff79c6>=</span> [y<span style=color:#ff79c6>.</span>count(i) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(y) <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> np<span style=color:#ff79c6>.</span>unique(y)]
</span></span><span style=display:flex><span>    gini <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>sum</span>([p <span style=color:#ff79c6>*</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> p) <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> probs])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> gini
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 打乱数据</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>data_shuffle</span>(X, y, seed<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> seed:
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>seed(seed)
</span></span><span style=display:flex><span>    idx <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>arange(X<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>])
</span></span><span style=display:flex><span>    np<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>shuffle(idx)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> X[idx], y[idx]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义树结点</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>TreeNode</span>():
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, feature_i<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, threshold<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, leaf_value<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, left_branch<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, right_branch<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>feature_i <span style=color:#ff79c6>=</span> feature_i  <span style=color:#6272a4># 特征索引</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>threshold <span style=color:#ff79c6>=</span> threshold  <span style=color:#6272a4># 特征划分阈值</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>leaf_value <span style=color:#ff79c6>=</span> leaf_value  <span style=color:#6272a4># 叶子节点取值</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>left_branch <span style=color:#ff79c6>=</span> left_branch  <span style=color:#6272a4># 左子树</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>right_branch <span style=color:#ff79c6>=</span> right_branch  <span style=color:#6272a4># 右子树</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义二叉决策树</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>BinaryDecisionTree</span>(<span style=color:#8be9fd;font-style:italic>object</span>):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 决策树初始参数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, min_samples_split<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, min_gini_impurity<span style=color:#ff79c6>=</span><span style=color:#bd93f9>999</span>, max_depth<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>float</span>(<span style=color:#f1fa8c>&#34;inf&#34;</span>), loss<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 根结点</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>root <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 节点最小分裂样本数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_samples_split <span style=color:#ff79c6>=</span> min_samples_split
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 节点初始化基尼不纯度</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mini_gini_impurity <span style=color:#ff79c6>=</span> min_gini_impurity
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 树最大深度</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>max_depth <span style=color:#ff79c6>=</span> max_depth
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 基尼不纯度计算函数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>gini_impurity_calculation <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 叶子节点值预测函数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_leaf_value_calculation <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 损失函数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>loss <span style=color:#ff79c6>=</span> loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 决策树拟合函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 递归构建决策树</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>root <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_build_tree(X, y)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>loss <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 决策树构建函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_build_tree</span>(<span style=font-style:italic>self</span>, X, y, current_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 初始化最小基尼不纯度</span>
</span></span><span style=display:flex><span>        init_gini_impurity <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>999</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 初始化最佳特征索引和阈值</span>
</span></span><span style=display:flex><span>        best_criteria <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 初始化数据子集</span>
</span></span><span style=display:flex><span>        best_sets <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(np<span style=color:#ff79c6>.</span>shape(y)) <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>:
</span></span><span style=display:flex><span>            y <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>expand_dims(y, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 合并输入和标签</span>
</span></span><span style=display:flex><span>        Xy <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>concatenate((X, y), axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 获取样本数和特征数</span>
</span></span><span style=display:flex><span>        n_samples, n_features <span style=color:#ff79c6>=</span> X<span style=color:#ff79c6>.</span>shape
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 设定决策树构建条件</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 训练样本数量大于节点最小分裂样本数且当前树深度小于最大深度</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> n_samples <span style=color:#ff79c6>&gt;=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_samples_split <span style=color:#ff79c6>and</span> current_depth <span style=color:#ff79c6>&lt;=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>max_depth:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 遍历计算每个特征的基尼不纯度</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> feature_i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(n_features):
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 获取第i特征的所有取值</span>
</span></span><span style=display:flex><span>                feature_values <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>expand_dims(X[:, feature_i], axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 获取第i个特征的唯一取值</span>
</span></span><span style=display:flex><span>                unique_values <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>unique(feature_values)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 遍历取值并寻找最佳特征分裂阈值</span>
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>for</span> threshold <span style=color:#ff79c6>in</span> unique_values:
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 特征节点二叉分裂</span>
</span></span><span style=display:flex><span>                    Xy1, Xy2 <span style=color:#ff79c6>=</span> feature_split(Xy, feature_i, threshold)
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 如果分裂后的子集大小都不为0</span>
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(Xy1) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0</span> <span style=color:#ff79c6>and</span> <span style=color:#8be9fd;font-style:italic>len</span>(Xy2) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>                        <span style=color:#6272a4># 获取两个子集的标签值</span>
</span></span><span style=display:flex><span>                        y1 <span style=color:#ff79c6>=</span> Xy1[:, n_features:]
</span></span><span style=display:flex><span>                        y2 <span style=color:#ff79c6>=</span> Xy2[:, n_features:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                        <span style=color:#6272a4># 计算基尼不纯度</span>
</span></span><span style=display:flex><span>                        impurity <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>impurity_calculation(y, y1, y2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                        <span style=color:#6272a4># 获取最小基尼不纯度</span>
</span></span><span style=display:flex><span>                        <span style=color:#6272a4># 最佳特征索引和分裂阈值</span>
</span></span><span style=display:flex><span>                        <span style=color:#ff79c6>if</span> impurity <span style=color:#ff79c6>&lt;</span> init_gini_impurity:
</span></span><span style=display:flex><span>                            init_gini_impurity <span style=color:#ff79c6>=</span> impurity
</span></span><span style=display:flex><span>                            best_criteria <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#34;feature_i&#34;</span>: feature_i, <span style=color:#f1fa8c>&#34;threshold&#34;</span>: threshold}
</span></span><span style=display:flex><span>                            best_sets <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>                                <span style=color:#f1fa8c>&#34;leftX&#34;</span>: Xy1[:, :n_features],
</span></span><span style=display:flex><span>                                <span style=color:#f1fa8c>&#34;lefty&#34;</span>: Xy1[:, n_features:],
</span></span><span style=display:flex><span>                                <span style=color:#f1fa8c>&#34;rightX&#34;</span>: Xy2[:, :n_features],
</span></span><span style=display:flex><span>                                <span style=color:#f1fa8c>&#34;righty&#34;</span>: Xy2[:, n_features:]
</span></span><span style=display:flex><span>                            }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 如果计算的最小不纯度小于设定的最小不纯度</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> init_gini_impurity <span style=color:#ff79c6>&lt;</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mini_gini_impurity:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 分别构建左右子树</span>
</span></span><span style=display:flex><span>            left_branch <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_build_tree(best_sets[<span style=color:#f1fa8c>&#34;leftX&#34;</span>], best_sets[<span style=color:#f1fa8c>&#34;lefty&#34;</span>], current_depth <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>            right_branch <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_build_tree(best_sets[<span style=color:#f1fa8c>&#34;rightX&#34;</span>], best_sets[<span style=color:#f1fa8c>&#34;righty&#34;</span>], current_depth <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> TreeNode(feature_i<span style=color:#ff79c6>=</span>best_criteria[<span style=color:#f1fa8c>&#34;feature_i&#34;</span>], threshold<span style=color:#ff79c6>=</span>best_criteria[<span style=color:#f1fa8c>&#34;threshold&#34;</span>],
</span></span><span style=display:flex><span>                            left_branch<span style=color:#ff79c6>=</span>left_branch, right_branch<span style=color:#ff79c6>=</span>right_branch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 计算叶子计算取值</span>
</span></span><span style=display:flex><span>        leaf_value <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_leaf_value_calculation(y)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> TreeNode(leaf_value<span style=color:#ff79c6>=</span>leaf_value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义二叉树值预测函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>predict_value</span>(<span style=font-style:italic>self</span>, x, tree<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> tree <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>            tree <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>root
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 如果叶子节点已有值，则直接返回已有值</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> tree<span style=color:#ff79c6>.</span>leaf_value <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> tree<span style=color:#ff79c6>.</span>leaf_value
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 选择特征并获取特征值</span>
</span></span><span style=display:flex><span>        feature_value <span style=color:#ff79c6>=</span> x[tree<span style=color:#ff79c6>.</span>feature_i]
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 判断落入左子树还是右子树</span>
</span></span><span style=display:flex><span>        branch <span style=color:#ff79c6>=</span> tree<span style=color:#ff79c6>.</span>right_branch
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(feature_value, <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>or</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(feature_value, <span style=color:#8be9fd;font-style:italic>float</span>):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> feature_value <span style=color:#ff79c6>&gt;=</span> tree<span style=color:#ff79c6>.</span>threshold:
</span></span><span style=display:flex><span>                branch <span style=color:#ff79c6>=</span> tree<span style=color:#ff79c6>.</span>left_branch
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> feature_value <span style=color:#ff79c6>==</span> tree<span style=color:#ff79c6>.</span>threshold:
</span></span><span style=display:flex><span>            branch <span style=color:#ff79c6>=</span> tree<span style=color:#ff79c6>.</span>right_branch
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 测试子集</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>predict_value(x, branch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 数据集预测函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>predict</span>(<span style=font-style:italic>self</span>, X):
</span></span><span style=display:flex><span>        y_pred <span style=color:#ff79c6>=</span> [<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>predict_value(sample) <span style=color:#ff79c6>for</span> sample <span style=color:#ff79c6>in</span> X]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> y_pred
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># CART分类树</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>ClassificationTree</span>(BinaryDecisionTree):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义基尼不纯度计算过程</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_calculate_gini_impurity</span>(<span style=font-style:italic>self</span>, y, y1, y2):
</span></span><span style=display:flex><span>        p <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(y1) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(y)
</span></span><span style=display:flex><span>        gini <span style=color:#ff79c6>=</span> calculate_gini(y)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 基尼不纯度</span>
</span></span><span style=display:flex><span>        gini_impurity <span style=color:#ff79c6>=</span> p <span style=color:#ff79c6>*</span> calculate_gini(y1) <span style=color:#ff79c6>+</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> p) <span style=color:#ff79c6>*</span> calculate_gini(y2)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> gini_impurity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 多数投票</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_majority_vote</span>(<span style=font-style:italic>self</span>, y):
</span></span><span style=display:flex><span>        most_common <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        max_count <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> label <span style=color:#ff79c6>in</span> np<span style=color:#ff79c6>.</span>unique(y):
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 统计多数</span>
</span></span><span style=display:flex><span>            count <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(y[y <span style=color:#ff79c6>==</span> label])
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> count <span style=color:#ff79c6>&gt;</span> max_count:
</span></span><span style=display:flex><span>                most_common <span style=color:#ff79c6>=</span> label
</span></span><span style=display:flex><span>                max_count <span style=color:#ff79c6>=</span> count
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> most_common
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 分类树拟合</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>impurity_calculation <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_calculate_gini_impurity
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_leaf_value_calculation <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_majority_vote
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(ClassificationTree, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span>fit(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># CART回归树</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>RegressionTree</span>(BinaryDecisionTree):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 计算方差减少量</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_calculate_variance_reduction</span>(<span style=font-style:italic>self</span>, y, y1, y2):
</span></span><span style=display:flex><span>        var_tot <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>var(y, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>        var_y1 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>var(y1, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>        var_y2 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>var(y2, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>        frac_1 <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(y1) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(y)
</span></span><span style=display:flex><span>        frac_2 <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(y2) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(y)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 计算方差减少量</span>
</span></span><span style=display:flex><span>        variance_reduction <span style=color:#ff79c6>=</span> var_tot <span style=color:#ff79c6>-</span> (frac_1 <span style=color:#ff79c6>*</span> var_y1 <span style=color:#ff79c6>+</span> frac_2 <span style=color:#ff79c6>*</span> var_y2)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#8be9fd;font-style:italic>sum</span>(variance_reduction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 节点值取平均</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_mean_of_y</span>(<span style=font-style:italic>self</span>, y):
</span></span><span style=display:flex><span>        value <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>mean(y, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> value <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(value) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>else</span> value[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 回归树拟合</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>impurity_calculation <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_calculate_variance_reduction
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_leaf_value_calculation <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>_mean_of_y
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(RegressionTree, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span>fit(X, y)
</span></span></code></pre></div><h2 id=gbdt算法的实现>GBDT算法的实现</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> mean_squared_error
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn <span style=color:#ff79c6>import</span> datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义回归树的平方损失</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>SquareLoss</span>:
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义平方损失</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>loss</span>(<span style=font-style:italic>self</span>, y, y_pred):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#bd93f9>0.5</span> <span style=color:#ff79c6>*</span> np<span style=color:#ff79c6>.</span>power((y <span style=color:#ff79c6>-</span> y_pred), <span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义平方损失的梯度</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>gradient</span>(<span style=font-style:italic>self</span>, y, y_pred):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>-</span>(y <span style=color:#ff79c6>-</span> y_pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># GBDT定义</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>GBDT</span>(<span style=color:#8be9fd;font-style:italic>object</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, n_estimators, learning_rate, min_samples_split,
</span></span><span style=display:flex><span>                 min_gini_impurity, max_depth, regression):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 常用超参数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators <span style=color:#ff79c6>=</span> n_estimators  <span style=color:#6272a4># 树的棵树</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>learning_rate <span style=color:#ff79c6>=</span> learning_rate  <span style=color:#6272a4># 学习率</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_samples_split <span style=color:#ff79c6>=</span> min_samples_split  <span style=color:#6272a4># 结点最小分裂样本数</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_gini_impurity <span style=color:#ff79c6>=</span> min_gini_impurity  <span style=color:#6272a4># 结点最小基尼不纯度</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>max_depth <span style=color:#ff79c6>=</span> max_depth  <span style=color:#6272a4># 最大深度</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>regression <span style=color:#ff79c6>=</span> regression  <span style=color:#6272a4># 默认为回归树</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>loss <span style=color:#ff79c6>=</span> SquareLoss()  <span style=color:#6272a4># 损失为平方损失</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 如果是分类树，需要定义分类树损失函数，这里省略，如需使用，需自定义分类损失函数</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>regression:
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>loss <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 多棵树叠加</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators):
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators<span style=color:#ff79c6>.</span>append(RegressionTree(min_samples_split<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_samples_split,
</span></span><span style=display:flex><span>                                                  min_gini_impurity<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>min_gini_impurity,
</span></span><span style=display:flex><span>                                                  max_depth<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>max_depth))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 拟合方法</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 前向分步模型初始化，第一棵树</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>fit(X, y)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 第一棵树的预测结果</span>
</span></span><span style=display:flex><span>        y_pred <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>predict(X)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 前向分步迭代训练</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>1</span>, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators):
</span></span><span style=display:flex><span>            gradient <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>loss<span style=color:#ff79c6>.</span>gradient(y, y_pred)
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[i]<span style=color:#ff79c6>.</span>fit(X, gradient)
</span></span><span style=display:flex><span>            y_pred <span style=color:#ff79c6>-=</span> np<span style=color:#ff79c6>.</span>multiply(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>learning_rate, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[i]<span style=color:#ff79c6>.</span>predict(X))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 预测方法</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>predict</span>(<span style=font-style:italic>self</span>, X):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 回归树预测</span>
</span></span><span style=display:flex><span>        y_pred <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>predict(X)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>1</span>, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_estimators):
</span></span><span style=display:flex><span>            y_pred <span style=color:#ff79c6>-=</span> np<span style=color:#ff79c6>.</span>multiply(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>learning_rate, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>estimators[i]<span style=color:#ff79c6>.</span>predict(X))
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 分类树预测</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>regression:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 将预测值转化为概率</span>
</span></span><span style=display:flex><span>            y_pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>exp(y_pred) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>expand_dims(np<span style=color:#ff79c6>.</span>sum(np<span style=color:#ff79c6>.</span>exp(y_pred), axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>), axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 转化为预测标签</span>
</span></span><span style=display:flex><span>            y_pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>argmax(y_pred, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> y_pred
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># GBDT分类树</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>GBDTClassifier</span>(GBDT):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, n_estimators<span style=color:#ff79c6>=</span><span style=color:#bd93f9>200</span>, learning_rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>.5</span>, min_samples_split<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>,
</span></span><span style=display:flex><span>                 min_info_gain<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>, max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(GBDTClassifier, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>(n_estimators<span style=color:#ff79c6>=</span>n_estimators,
</span></span><span style=display:flex><span>                                             learning_rate<span style=color:#ff79c6>=</span>learning_rate,
</span></span><span style=display:flex><span>                                             min_samples_split<span style=color:#ff79c6>=</span>min_samples_split,
</span></span><span style=display:flex><span>                                             min_gini_impurity<span style=color:#ff79c6>=</span>min_info_gain,
</span></span><span style=display:flex><span>                                             max_depth<span style=color:#ff79c6>=</span>max_depth,
</span></span><span style=display:flex><span>                                             regression<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 拟合方法</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fit</span>(<span style=font-style:italic>self</span>, X, y):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(GBDTClassifier, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span>fit(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># GBDT回归树</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>GBDTRegressor</span>(GBDT):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, n_estimators<span style=color:#ff79c6>=</span><span style=color:#bd93f9>300</span>, learning_rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>, min_samples_split<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>,
</span></span><span style=display:flex><span>                 min_var_reduction<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>, max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(GBDTRegressor, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>(n_estimators<span style=color:#ff79c6>=</span>n_estimators,
</span></span><span style=display:flex><span>                                            learning_rate<span style=color:#ff79c6>=</span>learning_rate,
</span></span><span style=display:flex><span>                                            min_samples_split<span style=color:#ff79c6>=</span>min_samples_split,
</span></span><span style=display:flex><span>                                            min_gini_impurity<span style=color:#ff79c6>=</span>min_var_reduction,
</span></span><span style=display:flex><span>                                            max_depth<span style=color:#ff79c6>=</span>max_depth,
</span></span><span style=display:flex><span>                                            regression<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># GBDT分类树</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 导入波士顿房价数据集</span>
</span></span><span style=display:flex><span>boston <span style=color:#ff79c6>=</span> datasets<span style=color:#ff79c6>.</span>load_boston()
</span></span><span style=display:flex><span><span style=color:#6272a4># 打乱数据集</span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff79c6>=</span> data_shuffle(boston<span style=color:#ff79c6>.</span>data, boston<span style=color:#ff79c6>.</span>target, seed<span style=color:#ff79c6>=</span><span style=color:#bd93f9>13</span>)
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> X<span style=color:#ff79c6>.</span>astype(np<span style=color:#ff79c6>.</span>float32)
</span></span><span style=display:flex><span>offset <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>int</span>(X<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>0.9</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 划分数据集</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(X, y, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.3</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建GBRT实例</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> GBDTRegressor()
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型训练</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型预测</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>predict(X_test)
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算模型预测的均方误差</span>
</span></span><span style=display:flex><span>mse <span style=color:#ff79c6>=</span> mean_squared_error(y_test, y_pred)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Mean Squared Error of NumPy GBRT:&#34;</span>, mse)
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li>周志华. 机器学习. 北京: 清华大学出版社, 2016.</li><li>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</li><li>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</li><li>鲁伟. 机器学习：公式推导与代码实现. 北京: 人民邮电出版社, 2022.</li><li>Géron A. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O&rsquo;Reilly Media, 2019.</li><li>XGBoost、LightGBM和CatBoost算法的对比：https://zhuanlan.zhihu.com/p/504646498</li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/ data-toggle=tooltip data-placement=top title=机器学习：决策树>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/ data-toggle=tooltip data-placement=top title=机器学习：EM算法>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>