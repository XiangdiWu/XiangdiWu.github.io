<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：EM算法"><meta property="og:title" content="机器学习：EM算法"><meta property="twitter:title" content="机器学习：EM算法"><meta name=description content="本文主要介绍EM算法，包括EM算法的引入、EM算法过程、EM算法的Python实现。"><meta property="og:description" content="本文主要介绍EM算法，包括EM算法的引入、EM算法过程、EM算法的Python实现。"><meta property="twitter:description" content="本文主要介绍EM算法，包括EM算法的引入、EM算法过程、EM算法的Python实现。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：EM算法-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：EM算法</h1><h2 class=subheading>EM Algorithm</h2><span class=meta>Posted by
XiangdiWu
on
Tuesday, October 6, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=em算法的引入>EM算法的引入</h1><p>概率模型有时既含观测变量，又含隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。<strong>EM算法</strong>就是<strong>含有隐变量的概率模型参数的极大似然估计法</strong>，或<strong>极大后验概率估计法</strong>。我们仅讨论极大似然估计，极大后验概率估计与其类似。</p><h2 id=三硬币模型>三硬币模型</h2><p>假设有3枚硬币，分别记作$A, B, C$。这些硬币正面出现的概率分别是$\pi, p, q$。进行如下投掷试验：先掷硬币$A$，根据其结果选出硬币$B$或硬币$C$，正面选硬币$B$，反而选硬币$C$；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复$n$次试验(这里$n=10$)，预测结果如下：1, 1, 0, 1, 0, 0, 1, 0, 1, 1。</p><p>假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三枚硬币分别出现正面的概率，即三硬币模型的参数。</p><p>三硬币模型可以写作：</p>$$
\begin{aligned}
P(y|\theta) &=\sum_{z} P(y, z|\theta)=\sum_{z} P(z|\theta) P(y|z, \theta) \\
&=\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}
\end{aligned}
$$<p>其中，随机变量$y$是观测变量，表示一次试验观测的结果是0或1；随机变量$z$为隐变量，表示<strong>未观测到</strong>的硬币$A$的结果；$\theta=(\pi,p,q)$是模型参数。这一模型是以上数据的生成模型。注意，随机变量$y$的数据可以观测，随机变量$z$的数据<strong>不可观测</strong>。</p><h1 id=em算法过程>EM算法过程</h1><p>将观测数据表示为$Y=(Y_1,Y_2,\cdots,Y_n)^\text T$，未观测数据表示为$Z=(Z_1,Z_2,\cdots,Z_n)^\text T$，则<strong>观测数据的似然函数</strong>为(下式可以理解为<strong>全概率公式</strong>)：</p>$$
P(Y|\theta)=\sum_Z P(Z|\theta)P(Y|Z,\theta)
$$<p>考虑到每次试验是独立的，则整个观测数据$Y$的似然函数为$Y$中<strong>所有独立重复试验的似然的乘积</strong>：</p>$$
P(Y|\theta)=\prod_{j=1}^{n}[\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}]
$$<p>考虑求模型参数$\theta=(\pi,p,q)$的极大似然估计，即</p>$$
\hat \theta=\arg\max_\theta \log P(Y|\theta)
$$<p>这个问题没有解析解，只有通过迭代的方法求解。EM算法就是可以用于求解这个问题的一种迭代算法，下面给出针对三硬币问题的EM算法：</p><p>EM算法首先选取参数的初值，记作$\theta^{(0)}=(\pi^{(0)},p^{(0)},q^{(0)})$，然后通过下面的步骤迭代计算参数的估计值，直至收敛为止。第$i$次迭代参数的估计值为$\theta^{(i)}=(\pi^{(i)},p^{(i)},q^{(i)})$。EM算法第$i+1$次迭代如下：</p><p><strong>E步</strong>：计算在模型参数$\pi^{(i)},p^{(i)},q^{(i)}$下观测数据$y_j$来自掷硬币$B$的概率</p>$$
\mu^{(i+1)}=\frac{\pi^{(i)}(p^{(i)})^{y_{j}}(1-p^{(i)})^{1-y_{j}}}{\pi^{(i)}(p^{(i)})^{y_{j}}(1-p^{(i)})^{1-y_{j}}+(1-\pi^{(i)})(q^{(i)})^{y_{j}}(1-q^{(i)})^{1-y_{j}}}
$$<p><strong>M步</strong>：计算模型参数的新估计值</p>$$
\begin{aligned}
\pi^{(i+1)}=\frac{1}{n} \sum_{j=1}^{n} \mu_{j}^{(i+1)} \\
p^{(i+1)}=\frac{\sum_{j=1}^{n} \mu_{j}^{(i+1)} y_{j}}{\sum_{j=1}^{n} \mu_{j}^{(i+1)}} \\
q^{(i+1)}=\frac{\sum_{j=1}^{n}(1-\mu_{j}^{(i+1)}) y_{j}}{\sum_{j=1}^{n}(1-\mu_{j}^{(i+1)})}
\end{aligned}
$$<p>可以算得，如果取初值$\pi^{(0)}=0.4,p^{(0)}=0.6,q^{(0)}=0.7$，那么得到的模型参数的极大似然估计中，三者的值分别为0.4064, 0.5368和0.6432。<strong>EM算法与初值的选择有关</strong>，选择不同的初值可能得到不同的参数估计值。</p><p>一般用$Y$表示观测随机变量的数据，$Z$表示隐随机变量的数据。$Y$和$Z$连在一起称为<strong>完全数据(complete data)</strong>，观测数据$Y$又称为<strong>不完全数据(incomplete data)</strong>。假设给定观测数据$Y$，其概率分布是$P(Y|\theta)$，其中$\theta$是需要估计的模型参数，那么不完全数据$Y$的似然函数是$P(Y|\theta)$，对数似然函数$L(\theta)=\log P(Y|\theta)$；假设$Y$和$Z$的联合概率分布是$P(Y,Z|\theta)$，那么完全数据的对数似然函数是$\log P(Y,Z|\theta)$。</p><p>EM算法通过迭代求$L(\theta)=\log P(Y|\theta)$的极大似然估计。每次迭代包含两步：E步，求期望；M步，求极大化。以下为<strong>EM算法的描述</strong>：</p><p>输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$；</p><p>输出：模型参数$\theta$。</p><p>(1) 选择参数的初值$\theta^{(0)}$，开始迭代；</p><p>(2) E步：记$\theta^{(i)}$为第$i$词迭代参数$\theta$的估计值，在第$i+1$次迭代的E步，计算</p>$$
\begin{aligned}
Q(\theta,\theta^{(i)})&=E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]\\
&=\sum_Z \log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
\end{aligned}
$$<p>这里，$P(Z|Y,\theta^{(i)})$是在给定观测数据$Y$和当前的参数估计$\theta^{(i)}$下隐变量数据$Z$的条件概率分布；</p><p>(3) M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，确定第$i+1$次迭代的参数的估计值$\theta^{(i+1)}$</p>$$
\theta^{(i+1)}=\arg\max_\theta Q(\theta,\theta^{(i)})
$$<p>(4) 重复(2)和(3)，直到收敛。</p><p>其中，$Q(\theta,\theta^{(i)})$为EM算法的核心，称为$Q$函数。完全数据的对数似然函数$\log P(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta^{(i)}$下对为观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望称为Q函数，即</p>$$
Q(\theta,\theta^{(i)})=E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]
$$<p>下面关于EM算法作几点说明：</p><p>步骤(1) 参数的初值可以任意选择，但需注意EM算法对初值是敏感的。</p><p>步骤(2) E步求$Q$函数时，$Q$函数式中$Z$是未观测数据，$Y$是观测数据。注意，$Q(\theta,\theta^{(i)})$的第一个变元表示<strong>要极大化的参数</strong>，第二个变元表示<strong>参数的当前估计值</strong>。每次迭代实际在求$Q$函数及其极大。</p><p>步骤(3) M步求$Q$函数的极大化，得到$\theta^{(i+1)}$，完成一次迭代。<strong>每次迭代使似然函数增大或达到局部极值</strong>。</p><p>步骤(4) 给出停止迭代的条件，一般是对较小的正数$\varepsilon_1,\varepsilon_2$，若满足</p>$$
\|\theta^{(i+1)}-\theta^{(i)}\|<\varepsilon_1
$$<p>或</p>$$
\|Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})\|<\varepsilon_2
$$<p>则停止迭代。</p><h1 id=em算法的python实现>EM算法的Python实现</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> random
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> math
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>loadData</span>(mu0, sigma0, mu1, sigma1, alpha0, alpha1):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    初始化数据集
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    这里通过服从高斯分布的随机函数来伪造数据集
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param mu0: 高斯0的均值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param sigma0: 高斯0的方差
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param mu1: 高斯1的均值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param sigma1: 高斯1的方差
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param alpha0: 高斯0的系数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param alpha1: 高斯1的系数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: 混合了两个高斯分布的数据
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    length <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1000</span>  <span style=color:#6272a4># 定义数据集长度为1000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化第一个高斯分布，生成数据，数据长度为length * alpha系数，以此来满足alpha的作用</span>
</span></span><span style=display:flex><span>    data0 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>normal(mu0, sigma0, <span style=color:#8be9fd;font-style:italic>int</span>(length <span style=color:#ff79c6>*</span> alpha0))
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 第二个高斯分布的数据</span>
</span></span><span style=display:flex><span>    data1 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>normal(mu1, sigma1, <span style=color:#8be9fd;font-style:italic>int</span>(length <span style=color:#ff79c6>*</span> alpha1))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化总数据集，两个高斯分布的数据混合后会放在该数据集中返回</span>
</span></span><span style=display:flex><span>    dataSet <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    dataSet<span style=color:#ff79c6>.</span>extend(data0)  <span style=color:#6272a4># 将第一个数据集的内容添加进去</span>
</span></span><span style=display:flex><span>    dataSet<span style=color:#ff79c6>.</span>extend(data1)  <span style=color:#6272a4># 添加第二个数据集的数据</span>
</span></span><span style=display:flex><span>    random<span style=color:#ff79c6>.</span>shuffle(dataSet)  <span style=color:#6272a4># 对总的数据集进行打乱</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> dataSet  <span style=color:#6272a4># 返回数据集</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>calcGauss</span>(dataSetArr, mu, sigmod):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    根据高斯密度函数计算值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    注：在公式中y是一个实数，但是在EM算法中，需要对每个j都求一次yjk，在本实例
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    中有1000个可观测数据，因此需要计算1000次。考虑到在E步时进行1000次高斯计
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    算，程序上比较不简洁，因此这里的y是向量，在numpy的exp中如果exp内部值为向
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    量，则对向量中每个值进行exp，输出仍是向量的形式。所以使用向量的形式1次计算
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    即可将所有计算结果得出，程序上较为简洁。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param dataSetArr: 可观测数据集
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param mu: 均值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param sigmod: 方差
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: 整个可观测数据集的高斯分布密度（向量形式）
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>/</span> (math<span style=color:#ff79c6>.</span>sqrt(<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> math<span style=color:#ff79c6>.</span>pi) <span style=color:#ff79c6>*</span> sigmod <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span>)) <span style=color:#ff79c6>*</span> np<span style=color:#ff79c6>.</span>exp(
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span> <span style=color:#ff79c6>*</span> (dataSetArr <span style=color:#ff79c6>-</span> mu) <span style=color:#ff79c6>*</span> (dataSetArr <span style=color:#ff79c6>-</span> mu) <span style=color:#ff79c6>/</span> (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> sigmod <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span>))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> result
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>E_step</span>(dataSetArr, alpha0, mu0, sigmod0, alpha1, mu1, sigmod1):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    EM算法中的E步
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    依据当前模型参数，计算分模型k对观数据y的响应度
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param dataSetArr: 可观测数据y
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param alpha0: 高斯模型0的系数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param mu0: 高斯模型0的均值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param sigmod0: 高斯模型0的方差
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param alpha1: 高斯模型1的系数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param mu1: 高斯模型1的均值
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param sigmod1: 高斯模型1的方差
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: 两个模型各自的响应度
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 计算y0的响应度</span>
</span></span><span style=display:flex><span>    gamma0 <span style=color:#ff79c6>=</span> alpha0 <span style=color:#ff79c6>*</span> calcGauss(dataSetArr, mu0, sigmod0)  <span style=color:#6272a4># 模型0的响应度的分子</span>
</span></span><span style=display:flex><span>    gamma1 <span style=color:#ff79c6>=</span> alpha1 <span style=color:#ff79c6>*</span> calcGauss(dataSetArr, mu1, sigmod1)  <span style=color:#6272a4># 模型1响应度的分子</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>sum</span> <span style=color:#ff79c6>=</span> gamma0 <span style=color:#ff79c6>+</span> gamma1  <span style=color:#6272a4># 两者相加为E步中的分布</span>
</span></span><span style=display:flex><span>    gamma0 <span style=color:#ff79c6>=</span> gamma0 <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>sum</span>  <span style=color:#6272a4># 各自相除，得到两个模型的响应度</span>
</span></span><span style=display:flex><span>    gamma1 <span style=color:#ff79c6>=</span> gamma1 <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>sum</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> gamma0, gamma1  <span style=color:#6272a4># 返回两个模型响应度</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>M_step</span>(muo, mu1, gamma0, gamma1, dataSetArr):
</span></span><span style=display:flex><span>    mu0_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>dot(gamma0, dataSetArr) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>sum(gamma0)
</span></span><span style=display:flex><span>    mu1_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>dot(gamma1, dataSetArr) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>sum(gamma1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    sigmod0_new <span style=color:#ff79c6>=</span> math<span style=color:#ff79c6>.</span>sqrt(np<span style=color:#ff79c6>.</span>dot(gamma0, (dataSetArr <span style=color:#ff79c6>-</span> muo) <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span>) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>sum(gamma0))
</span></span><span style=display:flex><span>    sigmod1_new <span style=color:#ff79c6>=</span> math<span style=color:#ff79c6>.</span>sqrt(np<span style=color:#ff79c6>.</span>dot(gamma1, (dataSetArr <span style=color:#ff79c6>-</span> mu1) <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span>) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>sum(gamma1))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    alpha0_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sum(gamma0) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(gamma0)
</span></span><span style=display:flex><span>    alpha1_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sum(gamma1) <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(gamma1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将更新的值返回</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> mu0_new, mu1_new, sigmod0_new, sigmod1_new, alpha0_new, alpha1_new
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>EM_Train</span>(dataSetList, <span style=color:#8be9fd;font-style:italic>iter</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>500</span>):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    根据EM算法进行参数估计
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param dataSetList:数据集（可观测数据）
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param iter: 迭代次数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: 估计的参数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将可观测数据y转换为数组形式，主要是为了方便后续运算</span>
</span></span><span style=display:flex><span>    dataSetArr <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array(dataSetList)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 对参数取初值，开始迭代</span>
</span></span><span style=display:flex><span>    alpha0 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span>
</span></span><span style=display:flex><span>    mu0 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    sigmod0 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>    alpha1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span>
</span></span><span style=display:flex><span>    mu1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>    sigmod1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    step <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>  <span style=color:#6272a4># 开始迭代</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>while</span> step <span style=color:#ff79c6>&lt;</span> <span style=color:#8be9fd;font-style:italic>iter</span>:
</span></span><span style=display:flex><span>        step <span style=color:#ff79c6>+=</span> <span style=color:#bd93f9>1</span>  <span style=color:#6272a4># 每次进入一次迭代后迭代次数加1</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># E步：依据当前模型参数，计算分模型k对观测数据y的响应度</span>
</span></span><span style=display:flex><span>        gamma0, gamma1 <span style=color:#ff79c6>=</span> E_step(dataSetArr, alpha0, mu0, sigmod0, alpha1, mu1, sigmod1)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># M步</span>
</span></span><span style=display:flex><span>        mu0, mu1, sigmod0, sigmod1, alpha0, alpha1 <span style=color:#ff79c6>=</span> M_step(mu0, mu1, gamma0, gamma1, dataSetArr)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> alpha0, mu0, sigmod0, alpha1, mu1, sigmod1  <span style=color:#6272a4># 迭代结束后将更新后的各参数返回</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>__name__</span> <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    start <span style=color:#ff79c6>=</span> time<span style=color:#ff79c6>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    alpha0 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.3</span>  <span style=color:#6272a4># 系数α</span>
</span></span><span style=display:flex><span>    mu0 <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>2</span>  <span style=color:#6272a4># 均值μ</span>
</span></span><span style=display:flex><span>    sigmod0 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span>  <span style=color:#6272a4># 方差σ</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    alpha1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.7</span>  <span style=color:#6272a4># 系数α</span>
</span></span><span style=display:flex><span>    mu1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span>  <span style=color:#6272a4># 均值μ</span>
</span></span><span style=display:flex><span>    sigmod1 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>  <span style=color:#6272a4># 方差σ</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化数据集</span>
</span></span><span style=display:flex><span>    dataSetList <span style=color:#ff79c6>=</span> loadData(mu0, sigmod0, mu1, sigmod1, alpha0, alpha1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印设置的参数</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;---------------------------&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;the Parameters set is:&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;alpha0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, mu0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, sigmod0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, alpha1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, mu1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, sigmod1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>&#39;</span> <span style=color:#ff79c6>%</span> (
</span></span><span style=display:flex><span>        alpha0, alpha1, mu0, mu1, sigmod0, sigmod1
</span></span><span style=display:flex><span>    ))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 开始EM算法，进行参数估计</span>
</span></span><span style=display:flex><span>    alpha0, mu0, sigmod0, alpha1, mu1, sigmod1 <span style=color:#ff79c6>=</span> EM_Train(dataSetList)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印参数预测结果</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;----------------------------&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;the Parameters predict is:&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;alpha0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, mu0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, sigmod0:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, alpha1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, mu1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>, sigmod1:</span><span style=color:#f1fa8c>%.1f</span><span style=color:#f1fa8c>&#39;</span> <span style=color:#ff79c6>%</span> (
</span></span><span style=display:flex><span>        alpha0, alpha1, mu0, mu1, sigmod0, sigmod1
</span></span><span style=display:flex><span>    ))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;----------------------------&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;time span:&#39;</span>, time<span style=color:#ff79c6>.</span>time() <span style=color:#ff79c6>-</span> start)
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</p></li><li><p>EM算法的实现：https://www.cnblogs.com/chenxiangzhen/p/10435969.html</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ data-toggle=tooltip data-placement=top title=机器学习：集成学习>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/ data-toggle=tooltip data-placement=top title=机器学习：降维>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>