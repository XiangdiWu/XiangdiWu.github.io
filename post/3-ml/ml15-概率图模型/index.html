<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：概率图模型"><meta property="og:title" content="机器学习：概率图模型"><meta property="twitter:title" content="机器学习：概率图模型"><meta name=description content="本文主要介绍概率图模型，包括概率图模型概述、模型表示、学习、推断。"><meta property="og:description" content="本文主要介绍概率图模型，包括概率图模型概述、模型表示、学习、推断。"><meta property="twitter:description" content="本文主要介绍概率图模型，包括概率图模型概述、模型表示、学习、推断。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：概率图模型-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：概率图模型</h1><h2 class=subheading>PGM-Probabilistic Graphical Model</h2><span class=meta>Posted by
XiangdiWu
on
Saturday, October 10, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=概率图模型概述>概率图模型概述</h1><div align=center><img src=/Kimages/2/image-20200818114353404.png style=zoom:40%></div><p><strong>概率图模型(probabilistic graphical model, PGM)</strong>，简称图模型(graphical model, GM)，是指一种<strong>用图结构来描述多元随机变量之间条件独立关系的概率模型</strong>，从而给研究<strong>高维空间中的概率模型</strong>带来了很大的便捷性。</p><p>对于一个$K$维随机向量$\boldsymbol X=[X_1,X_2,\cdots,X_K]^\text T$，其联合概率为高维空间中的分布，一般难以直接建模。假设每个变量为离散变量并有$m$个取值，在不作任何独立假设条件下，则需要个$m^K-1$参数才能表示其概率分布，当$m$和$K$很大时，参数量远远超出了目前计算机的存储能力。</p><p>一种有效减少参数量的方法是独立性假设。一个$K$维随机向量$\boldsymbol X$的联合概率分解为$K$个<strong>条件概率的乘积</strong>：</p>$$
\begin{aligned}
p(\boldsymbol{x}) & \triangleq P(\boldsymbol{X}=\boldsymbol{x}) \\
&=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) \cdots p\left(x_{K} \mid x_{1}, \cdots, x_{K-1}\right) \\
&=\prod_{k=1}^{K} p\left(x_{k} \mid x_{1}, \cdots, x_{k-1}\right)
\end{aligned}
$$<p>其中$x_k$表示变量$X_k$的取值。如果<strong>某些变量之间存在条件独立</strong>，其参数量就可以大幅减少。</p><p>假设有四个二值变量$X_1,X_2,X_3,X_4$，在不知道这几个变量依赖关系的情况下，可以用一个联合概率表来记录每一种取值的概率，共需要15个参数。假设在已知$X_1$时，$X_2$和$X_3$独立，即有</p>$$
p(x_{2} | x_{1}, x_{3})=p(x_{2} | x_{1}) \\
p(x_{3} | x_{1}, x_{2})=p(x_{3} | x_{1})
$$<p>在已知$X_2$和$X_3$时，$X_4$也和$X_1$独立，即有</p>$$
p(x_{4} | x_{1}, x_{2}, x_{3})=p(x_{4} | x_{2}, x_{3})
$$<p>那么其联合概率$p(\boldsymbol x)$可以分解为：</p>$$
\begin{aligned}
p(\boldsymbol{x}) &=p(x_{1}) p(x_{2} | x_{1}) p(x_{3} | x_{1}, x_{2}) p(x_{4} | x_{1}, x_{2}, x_{3}) \\
&=p(x_{1}) p(x_{2} | x_{1}) p(x_{3} | x_{1}) p(x_{4} | x_{2}, x_{3})
\end{aligned}
$$<p>是4个局部条件概率的乘积。如果分别用4个表格来记录这4个条件概率的话，只需要1 + 2 + 2 + 4 = 9个独立参数。</p><p>当概率模型中的变量数量比较多时，其条件依赖关系也比较复杂。我们可以使用<strong>图结构</strong>的方式将概率模型可视化，以一种直观、简单的方式描述随机变量之间的条件独立性的性质，并可以将一个复杂的联合概率模型分解为一些简单条件概率模型的组合。下图给出了上述例子中4个变量之间的条件独立性的图形化描述。图中每个节点表示一个变量，每条连边表示变量之间的依赖关系。<strong>对于一个非全连接的图，都存在一个或多个条件独立性假设，可以根据条件独立性将联合概率分布进行分解，表示为一组局部条件概率分布的乘积</strong>。</p><div align=center><img src=/Kimages/2/image-20200818151457264.png style=zoom:20%></div><p>图模型有以下三个基本问题。(1) <strong>表示问题</strong>：对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。(2) <strong>学习问题</strong>：图模型的学习包括图结构的学习和参数的学习。通常，我们只关注<strong>在给定图结构时的参数学习，即参数估计问题</strong>。(3) <strong>推断问题</strong>：在已知部分变量时，计算其它变量的条件概率分布。很多其他机器学习模型都可以由概率图模型进行表示。</p><h1 id=模型表示>模型表示</h1><p>图由一组<strong>节点</strong>和节点之间的<strong>边</strong>组成。在概率图模型中，<strong>每个节点都表示一个随机变量(或一组随机变量)</strong>，边表示这些随机变量之间的概率依赖关系。</p><p>常见的概率图模型可以分为两类：<strong>有向图模型和无向图模型</strong>。有向图模型的图结构为有向非循环图，如果两个节点之间有连边，表示对应的两个变量为<strong>因果关系</strong>。无向图模型使用无向图来描述变量之间的关系。<strong>每条边代表两个变量之间有概率依赖关系，但是并不一定是因果关系</strong>。</p><p>下图为有向图和无向图的示例，表示四个变量$\{X_1, X_2, X_3, X_4\}$间的依赖关系。</p><div align=center><img src=/Kimages/2/image-20200818151824792.png style=zoom:25%></div><p>图中，带阴影的节点$\{X_1, X_4\}$表示<strong>可观测到的变量</strong>，不带阴影的节点$\{X_2, X_3\}$表示<strong>隐变量</strong>，连边表示两变量间的<strong>条件依赖关系</strong>。</p><h2 id=有向图模型>有向图模型</h2><p><strong>有向图模型(directed graphcal model)</strong>，也称为<strong>贝叶斯网络(Bayesian network)<strong>或</strong>信念网络(belief network, BN)</strong>，是指用有向图来表示概率分布的图模型。假设一个有向图$G(\mathcal V,\mathcal E)$，节点集合$\mathcal V=\{X_1,X_2,\cdots,X_K\}$表示$K$个随机变量，节点$k$对应随机变量$X_k$。$\mathcal E$为边的集合，每条边表示两个变量之间<strong>因果关系</strong>。</p><p>对于一个随机向量$\boldsymbol X=[X_1,X_2,\cdots,X_K]^\text T$和一个有$K$个节点的有向非循环图$G$，$G$中的每个节点都对应一个随机变量，可以是可观测的变量、隐变量或是未知参数。$G$中的每个连接$e_{ij}$表示两个随机变量$X_i$和$X_j$之间具有非独立的因果关系。$\boldsymbol X_{\pi_k}$表示变量$X_k$的所有父节点变量集合，每个随机变量的<strong>局部条件概率分布(local conditional probability distribution)</strong> 为$|P(X_k|\boldsymbol X_{\pi_k})$。</p><p>如果$\boldsymbol X$的联合概率分布可以分解为每个随机变量$X_k$的局部条件概率连乘的形式，即</p>$$
p(\boldsymbol{x})=\prod_{k=1}^{K} p(x_{k} | \boldsymbol{x}_{\pi_{k}})
$$<p>那么$(G,\boldsymbol X)$构成了一个贝叶斯网络。</p><p>在贝叶斯网络中，如果两个节点是<strong>直接连接</strong>的，它们肯定是非条件独立的，是直接因果关系。父节点是因，子节点是果。如果两个节点<strong>不是直接连接</strong>的，但是它们之间有一条经过其它节点的路径来连接，那么这两个节点之间的条件独立性就比较复杂。以三个节点的贝叶斯网络为例，给定三个节点$X_1, X_2, X_3$，其中$X_1$和$X_3$是不直接连接的，通过节点$X_2$连接。这三个节点之间可以有四种连接关系：</p><div align=center><img src=/Kimages/2/image-20200818161143721.png style=zoom:25%></div><p>上图中，在(a)和(b)中，$X_1 \not \perp X_3|\varnothing$，但$X_{1} \perp X_{3} | X_{2}$；在(c)中，同样$X_1 \not \perp X_3|\varnothing$，但$X_{1} \perp X_{3} | X_{2}$；在(d)中，$X_1 \perp X_3|\varnothing$，但$X_{1} \not \perp X_{3} | X_{2}$。四种关系分别如下：</p><p>(1) <strong>间接因果关系</strong>：当$X_2$已知时，$X_1$和$X_3$为条件独立；</p><p>(2) <strong>间接果因关系(与(1)相同)</strong>：当$X_2$已知时，$X_1$和$X_3$为条件独立；</p><p>(3) <strong>共因关系</strong>：当$X_2$未知时，$X_1$和$X_3$是不独立的；当$X_2$已知时，$X_1$和$X_3$条件独立；</p><p>(4) <strong>共果关系</strong>：当$X_2$未知时，$X_1$和$X_3$是独立的；当$X_2$已知时，$X_1$和$X_3$不独立。</p><p>对于一个更一般的贝叶斯网络，其<strong>局部马尔可夫性质</strong>为：<strong>每个随机变量在给定父节点的情况下，条件独立于它的非后代节点</strong>，即$X_k \perp Z|X_{\pi_k}$，其中$Z$为$X_k$的非后代变量。</p><p>常见的有向图模型如下：</p><h3 id=sigmoid信念网络>sigmoid信念网络</h3><p>为了减少模型参数，可以使用参数化模型来建模有向图模型中的条件概率分布。一种简单的参数化模型为<strong>sigmoid信念网络(sigmoid belief network, SBN)</strong>。在该网络中，变量的取值为$\{0,1\}$。对于变量$X_k$和它的父节点集合$\pi_k$，其条件概率分布表示为：</p>$$
P(X_{k}=1 | \boldsymbol{x}_{\pi_{k}} ; \theta)=\sigma(\theta_{0}+\sum_{x_{i} \in \boldsymbol{x}_{\pi_{k}}} \theta_{i} x_{i})
$$<p>其中$\sigma(\cdot)$为sigmoid函数，$\theta_i$为可学习的参数。假设变量$X_k$的父节点数量为$M$，如果使用表格来记录条件概率需要$2^M$个参数，如果使用参数化模型只需要$M+1$个参数。如果对不同的变量的条件概率都共享使用一个参数化模型，其参数数量又可以大幅减少。</p><p>值得一提的是，sigmoid信念网络与logistic回归模型都采用sigmoid函数来计算条件概率。如果<strong>假设sigmoid信念网络中只有一个叶子节点</strong>，其所有的父节点之间没有连接，且取值为实数，那么<strong>sigmoid信念网络的网络结构和logistic回归模型类似</strong>。但是，两个模型区别在于<strong>logistic回归模型中</strong>的$\boldsymbol x$作为一种<strong>确定性的参数，而非变量</strong>。 因此logistic回归模型只建模条件概率$p(y|x)$，是一种<strong>判别模型</strong>；而sigmoid信念网络建模$p(x,y)$，是一种<strong>生成模型</strong>。</p><div align=center><img src=/Kimages/2/image-20200818180226925.png style=zoom:25%></div><h3 id=朴素贝叶斯分类器>朴素贝叶斯分类器</h3><p><strong>朴素贝叶斯分类器(naive Bayes classififier, NB)</strong> 是一类简单的概率分类器，在<strong>强(朴素)独立性假设</strong>的条件下运用贝叶斯公式来计算每个类别的条件概率。给定一个有$d$维特征的样本$\boldsymbol x$和类别$y$，类别$y$的条件概率为：</p>$$
\begin{aligned}
p(y | \boldsymbol{x} ; \theta) &=\frac{p(x_{1}, \cdots, x_{d} | y ; \theta) p(y ; \theta)}{p(x_{1}, \cdots, x_{d})} \\
& \propto p(x_{1}, \cdots, x_{d} | y ; \theta) p(y ; \theta)
\end{aligned}
$$<p>其中$\theta$为概率分布的参数。在朴素贝叶斯分类器中，假设在给定$Y$的情况下，$X_i$之间是条件独立的。下图给出了朴素贝叶斯分类器的图模型表示：</p><div align=center><img src=/Kimages/2/image-20200818180642241.png style=zoom:20%></div><p>条件概率分布$p(y|\boldsymbol x)$可以分解为：</p>$$
p(y | \boldsymbol{x} ; \theta) \propto p(y | \theta_{c}) \prod_{i=1}^{d} p(x_{i} | y ; \theta_{i, y})
$$<p>其中$\theta_c$是$y$的先验概率分布的参数，$\theta_{i, y}$是条件概率分布$p(x_i | y ; \theta_{i, y})$的参数。如果$x_i$为连续值，$p(x_i | y ; \theta_{i, y})$可以用<strong>高斯分布</strong>建模。如果$x_i$为离散值，$p(x_i | y ; \theta_{i, y})$可以用<strong>多项分布</strong>建模。</p><p>虽然朴素贝叶斯分类器的条件独立性假设太强，但是在实际应用中，朴素贝叶斯分类器在很多任务上也能得到很好的结果，并且模型简单，可以有效防止过拟合。</p><h3 id=隐马尔可夫模型>隐马尔可夫模型</h3><p><strong>隐马尔可夫模型(hidden Markov model, HMM)</strong> 是一种含有隐变量的马尔可夫过程。下图给出隐马尔可夫模型的图模型表示：</p><div align=center><img src=/Kimages/2/image-20200818181206735.png style=zoom:20%></div><p>隐马尔可夫模型的<strong>联合概率</strong>可以分解为：</p>$$
p(\boldsymbol{x}, \boldsymbol{y} ; \theta)=\prod_{t=1}^{T} p(y_{t} | y_{t-1}, \theta_{s}) p(x_{t} | y_{t}, \theta_{t})
$$<p>其中$p(x_t | y_t, \theta_t)$为<strong>输出概率</strong>，$p(y_t | y_{t-1}, \theta_s)$为<strong>转移概率</strong>，$\theta_s$和$\theta_t$分别表示两类条件概率的参数。</p><h2 id=无向图模型>无向图模型</h2><p>无向图模型，也称<strong>马尔可夫随机场(Markov random field, MRF)<strong>或</strong>马尔可夫网络(Markov network)</strong>，是一类用无向图来描述一组具有局部马尔可夫性质的随机向量$X$的联合概率分布的模型。</p><p>对于一个随机向量$\boldsymbol X=[X_1,\cdots,X_K]^\text T$和一个有$K$个节点的无向图$G(\mathcal V, \mathcal E)$(可以存在循环)，图$G$中的节点$k$表示随机变量$X_k,1 \leqslant k \leqslant K$。如果$(G,\boldsymbol X)$满足局部马尔可夫性质，即一个变量$X_k$在<strong>给定它的邻居</strong>的情况下<strong>独立于所有其他变量</strong>，$p(x_k|\boldsymbol x_{\setminus k})=p(x_k|\boldsymbol x_{N(k)})$，其中$N(k)$为变量$X_k$的邻居集合，$\setminus k$为除$X_k$外其他变量的集合，那么$(G,\boldsymbol X)$就构成了一个<strong>马尔可夫随机场</strong>。</p><p>无向图中的局部马尔可夫性可以表示为$X_k \perp \boldsymbol X_{\setminus N(k),\setminus k} | \boldsymbol X_{N(k)}$，其中$\boldsymbol X_{N(k), \setminus k}$表示除$\boldsymbol X_{N(k)}$和$X_k$外的其他变量。</p><h3 id=无向图模型的概率分解>无向图模型的概率分解</h3><p>由于无向图模型并不提供一个变量的拓扑顺序，因此无法用链式法则对$p(x)$进行逐一分解。无向图模型的联合概率一般以全连通子图为单位进行分解。无向图中的一个全连通子图，称为<strong>团(clique)</strong>，即团内的所有节点之间都连边。左上角的图中共有7个团，包括$\{X_1, X_2\}$，$\{X_1, X_3\}$，$\{X_2, X_3\}$，$\{X_3, X_4\}$，$\{X_2, X_4\}$，$\{X_1, X_2, X_3\}$，$\{X_2, X_3, X_4\}$。</p><p>在所有团中，如果一个团不能被其它的团包含，这个团就是一个<strong>最大团(maximal clique)</strong>。如下所示：</p><div align=center><img src=/Kimages/2/image-20200819111327135.png style=zoom:30%></div><p>无向图中的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式。</p><p><strong>Hammersley-Clifford定理</strong>：如果一个分布$p(\boldsymbol x)>0$满足无向图$G$中的局部马尔可夫性质，当且仅当$p(\boldsymbol x)$可以表示为一系列定义在最大团上的肺腑函数的乘积形式，即：</p>$$
p(\boldsymbol x)=\frac{1}{Z}\prod_{c \in \mathcal C} \phi_c(\boldsymbol x_c)
$$<p>其中，$\mathcal C$为无向图$G$中的最大团集合，$\phi_c(\boldsymbol x_c) \geqslant 0$是定义在团$c$上的<strong>势能函数(potential function)</strong>，$Z$是<strong>配分函数(partition function)</strong>，用来将乘积归一化为概率形式：</p>$$
Z=\sum_{\boldsymbol x \in \mathcal X}\prod_{c \in \mathcal C}\phi_c(\boldsymbol x_c)
$$<p>其中$\mathcal X$为随机向量$X$的取值空间。</p><p>无向图模型与有向图模型的一个重要区别是有配分函数$Z$。配分函数的计算复杂度是指数级的，因此在推断和参数学习时都需要重点考虑。</p><p>上述分布形式也称为<strong>吉布斯分布(Gibbs distribution)</strong>。根据<strong>Hammersley-Clifford定理</strong>，无向图模型和吉布斯分布是一致的。吉布斯分布一定满足马尔可夫随机场的条件独立性质，并且马尔可夫随机场的概率分布一定可以表示成吉布斯分布。</p><p>由于势能函数必须为正，因此一般定义为：</p>$$
\phi_c(\boldsymbol x_c)=\exp(-E_c(\boldsymbol x_c))
$$<p>其中$E_c(\boldsymbol x_c)$为<strong>能量函数(energy function)</strong>。因此，无向图上定义的概率分布可以表示为：</p>$$
\begin{aligned}
P(\boldsymbol{x}) &=\frac{1}{Z} \prod_{c \in \mathcal{C}} \exp (-E_{c}(\boldsymbol{x}_{c})) \\
&=\frac{1}{Z} \exp (\sum_{c \in \mathcal{C}}-E_{c}(\boldsymbol{x}_{c}))
\end{aligned}
$$<p>这种形式的分布又称为<strong>玻尔兹曼分布(Boltzmann distribution)</strong>。任何一个无向图模型都可以用上式来表示其联合概率。</p><p>常见的无向图模型如下：</p><h3 id=对数线性模型>对数线性模型</h3><p>势能函数一般定义为$\phi_c(\boldsymbol x_c|\theta_c)=\exp(\theta_c^\text T f_c(\boldsymbol x_c))$，其中函数$f_c(\boldsymbol x_c)$为定义在$\boldsymbol x_c$上的特征向量，$\theta_c$为权重向量，即可学习的参数。这样联合概率$p(\boldsymbol x)$的对数形式为：</p>$$
\log p(\boldsymbol{x} ; \theta)=\sum_{c \in \mathcal{C}} \theta_{c}^{\text{T}} f_{c}(\boldsymbol{x}_{c})-\log Z(\theta)
$$<p>其中$\theta$代表所有势能函数中的参数$\theta_c$。这种形式的无向图模型也称为<strong>对数线性模型(log-linear model)<strong>或</strong>最大熵模型(maximum entropy model)</strong>。如下图所示：</p><div align=center><img src=/Kimages/2/image-20200819155016681.png style=zoom:20%></div><p>如果用对数线性模型来建模条件概率$p(y|\boldsymbol x)$，</p>$$
p(y | \boldsymbol{x} ; \theta)=\frac{1}{Z(\boldsymbol{x} ; \theta)} \exp (\theta^{\text{T}} f(\boldsymbol{x}, y))
$$<p>其中$Z(\boldsymbol{x} ; \theta)=\sum_{y} \exp (\theta^{\mathrm{T}} f_{y}(\boldsymbol{x}, y))$。这种对数线性模型也称为<strong>条件最大熵模型</strong>或<strong>softmax回归模型</strong>。</p><h3 id=条件随机场>条件随机场</h3><p><strong>条件随机场(conditional random field, CRF)</strong> 是一种直接建模条件概率的无向图模型。和条件最大熵模型不同，条件随机场建模的条件概率$p(\boldsymbol y|\boldsymbol x)$中，$\boldsymbol y$<strong>一般为随机向量</strong>，因此需要对$p(\boldsymbol y|\boldsymbol x)$进行因子分解。假设条件随机场的最大团集合为$\mathcal C$，其条件概率为：</p>$$
p(\boldsymbol{y} | \boldsymbol{x} ; \theta)=\frac{1}{Z(\boldsymbol{x} ; \theta)} \exp (\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}(\boldsymbol{x}, \boldsymbol{y}_{c}))
$$<p>其中，$Z(\boldsymbol{x} ; \theta)=\sum_{y} \exp (\sum_{c \in \mathcal{C}} f_{c}(\boldsymbol{x}, \boldsymbol{y}_{c})^{\mathrm{T}} \theta_{c})$为归一化项。</p><p>一个最常用的条件随机场为链式结构，其条件概率为：</p>$$
p(\boldsymbol{y} | \boldsymbol{x} ; \theta)=\frac{1}{Z(\boldsymbol{x} ; \theta)} \exp (\sum_{t=1}^{T} \theta_{1}^{\mathrm{T}} f_{1}(\boldsymbol{x}, y_{t})+\sum_{t=1}^{T-1} \theta_{2}^{\mathrm{T}} f_{2}(\boldsymbol{x}, y_{t}, y_{t+1}))
$$<p>其中$f_1(\boldsymbol x,y_t)$为<strong>状态特征</strong>，一般和<strong>位置</strong>$t$相关；$f_2(\boldsymbol x,y_t,y_{t+1})$为转移特征，一般可以简化为$f_2(y_t,y_{t+1})$并使用状态转移矩阵来表示。链式结构的条件随机场示意图如下所示：</p><div align=center><img src=/Kimages/2/image-20200819161938723.png style=zoom:25%></div><h2 id=有向图和无向图之间的转换>有向图和无向图之间的转换</h2><p>有向图和无向图可以相互转换，但将无向图转为有向图通常比较困难。在实际应用中，<strong>将有向图转为无向图更加重要</strong>，这样可以利用无向图上的精确推断算法，比如<strong>联合树算法(junction tree algorithm)</strong>。无向图模型可以表示有向图模型无法表示的一些依赖关系，如循环依赖；但它不能表示有向图模型能表示的某些关系，比如因果关系。</p><p>以下图(a)中的有向图为例，其联合概率分布可以分解为$p(\boldsymbol x)=p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)$。</p><p>其中$p(x_4|x_1, x_2, x_3)$<strong>和四个变量都相关</strong>。如果要转换为无向图，需要<strong>将这四个变量都归属于一个团中</strong>。 <strong>道德化</strong>的名称来源是：<strong>有共同儿子的父节点都必须结婚(即有连边</strong>)。因此需要将$x_4$的三个父节点之间都加上连边，如下图(b)所示。这个过程称为<strong>道德化(moralization)</strong>。转换后的无向图称为<strong>道德图(moral graph)</strong>。在道德化的过程中，原来有向图的一些独立性会丢失，比如该例中$X_1 \perp X_2 \perp X_3|\varnothing$在道德图中将不再成立。</p><div align=center><img src=/Kimages/2/image-20200819162441102.png style=zoom:25%></div><h1 id=概率图模型的学习>概率图模型的学习</h1><p>图模型的学习可以分为两部分：一是<strong>网络结构学习</strong>，即寻找最优的网络结构；二是<strong>网络参数估计</strong>，即已知网络结构，估计每个条件概率分布的参数。网络结构学习一般比较困难，一般是<strong>由领域专家来构建</strong>。通常只讨论<strong>在给定网络结构条件下的参数估计问题</strong>。图模型的参数估计问题又分为<strong>不包含隐变量时的参数估计问题</strong>和<strong>包含隐变量时的参数估计问题</strong>。</p><h2 id=不含隐变量的参数估计>不含隐变量的参数估计</h2><p>如果图模型中不含隐变量，那么网络参数一般可以直接通过最大似然来进行估计。</p><p>在<strong>有向图模型</strong>中，所有变量$\boldsymbol x$的联合概率分布可以分解为每个随机变量$x_k$的<strong>局部条件概率</strong>$p(x_k|x_{\pi_k};\theta_k)$的<strong>连乘形式</strong>，其中$\theta_k$为第$k$个变量的局部条件概率的参数。给定$N$个训练样本$\mathcal D=\{\boldsymbol x^{(n)}\}_{n=1}^N$，其对数似然函数为</p>$$
\begin{aligned}
\mathcal{L}(\mathcal{D} ; \theta) &=\frac{1}{N} \sum_{n=1}^{N} \log p(\boldsymbol{x}^{(n)} ; \theta) \\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{k=1}^{K} \log p(x_{k}^{(n)} | x_{\pi_{k}}^{(n)} ; \theta_{k})
\end{aligned}
$$<p>其中$\theta_k$为模型中的所有参数。</p><p>因为所有变量都是可观测的，最大化对数似然$\mathcal{L}(\mathcal{D} ; \theta)$，只需要分别地最大化每个变量的条件似然来估计其参数。</p>$$
\theta_{k}=\arg \max \sum_{n=1}^{N} \log p(x_{k}^{(n)} | x_{\pi_{k}}^{(n)} ; \theta_{k})
$$<p>如果变量$\boldsymbol x$是离散的，直接简单的方式是在训练集上统计每个变量的条件概率表。但是条件概率表需要的参数比较多。假设条件概率的$p(x_k|x_{\pi_k};\theta_k)$父节点数量为$M$，所有变量为二值变量，其条件概率表需要$2^M$个参数。为了减少参数数量，可以使用<strong>参数化的模型</strong>，比如<strong>sigmoid信念网络</strong>。如果变量是连续的$\boldsymbol x$，可以使用<strong>高斯函数</strong>来表示条件概率分布，称为<strong>高斯信念网络</strong>。在此基础上，还可以通过让所有的条件概率分布共享使用同一组参数来进一步减少参数的数量。</p><p>在<strong>无向图模型</strong>中，所有变量$\boldsymbol x$的联合概率分布可以分解为定义在最大团上的势能函数的连乘形式。以对数线性模型为例，</p>$$
p(\boldsymbol{x} ; \theta)=\frac{1}{Z(\theta)} \exp (\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}(\boldsymbol{x}_{c}))
$$<p>其中$Z(\theta)=\sum_{\boldsymbol{x}} \exp (\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}(\boldsymbol{x}_{c}))$。</p><p>给定$N$个训练样本</p>$$\mathcal D=\{\boldsymbol x^{(n)}\}_{n=1}^N$$<p>，其对数似然函数为：</p>$$
\begin{aligned}
\mathcal{L}(\mathcal{D} ; \theta) &=\frac{1}{N} \sum_{n=1}^{N} \log p(\boldsymbol{x}^{(n)} ; \theta) \\
&=\frac{1}{N} \sum_{n=1}^{N}(\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}(\boldsymbol{x}_{c}^{(n)}))-\log Z(\theta)
\end{aligned}
$$<p>其中$\theta_c$为定义在团$c$上的势能函数的参数。采用梯度上升方法进行最大似然估计，$\mathcal{L}(\mathcal{D} ; \theta)$关于参数$\theta_c$的偏导数为</p>$$
\frac{\partial \mathcal{L}(\mathcal{D} ; \theta)}{\partial \theta_{c}}=\frac{1}{N} \sum_{n=1}^{N}(f_{c}(\boldsymbol{x}_{c}^{(n)}))-\frac{\partial \log Z(\theta)}{\partial \theta_{c}}
$$<p>其中，</p>$$
\begin{aligned}
\frac{\partial \log Z(\theta)}{\partial \theta_{c}} &=\sum_{\boldsymbol{x}} \frac{1}{Z(\theta)} \cdot \exp (\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}(\boldsymbol{x}_{c})) \cdot f_{c}\left(\boldsymbol{x}_{c}\right) \\
&=\sum_{\boldsymbol{x}} p(\boldsymbol{x} ; \theta) f_{c}(\boldsymbol{x}_{c}) \triangleq \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x} ; \theta)}[f_{c}(\boldsymbol{x}_{c})]
\end{aligned}
$$<p>因此，</p>$$
\begin{aligned}
\frac{\partial \mathcal{L}(\mathcal{D} ; \theta)}{\partial \theta_{c}} &=\frac{1}{N} \sum_{n=1}^{N} f_{c}(\boldsymbol{x}_{c}^{(n)})-\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x} ; \theta)}[f_{c}(\boldsymbol{x}_{c})] \\
&=\mathbb{E}_{\boldsymbol{x} \sim \tilde{p}(\boldsymbol{x})}[f_{c}(\boldsymbol{x}_{c})]-\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x} ; \theta)}[f_{c}(\boldsymbol{x}_{c})]
\end{aligned}
$$<p>其中$\tilde p(x)$定义为<strong>经验分布(empirical distribution)</strong>。由于在最优点时梯度为0，因此无向图的最大似然估计的优化目标等价于对于每个团$c$上的特征$f_{c}(\boldsymbol{x}_{c})$，使其在经验分布下的期望$\tilde p(x)$等于其在模型分布$p(\boldsymbol x ; \theta)$下的期望。</p><p>可以看出，无向图模型的参数估计要比有向图更为复杂。在有向图中，每个局部条件概率的参数是独立的；而在无向图中，所有的参数都是相关的，无法分解。</p><p>对于一般的无向图模型，$\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x} ; \theta)}[f_{c}(\boldsymbol{x}_{c})]$往往很难计算，因为涉及在联合概率空间计$p(\boldsymbol x ; \theta)$算期望。当模型变量比较多时，这个计算往往无法实现。因此，无向图的参数估计通常采用近似的方法。一是利用<strong>采样</strong>来近似计算这个期望；二是用<strong>坐标上升法</strong>，即固定其它参数，来优化势能函数的参数。</p><h2 id=含隐变量的参数估计>含隐变量的参数估计</h2><p>如果图模型中包含隐变量，即有部分变量是不可观测的，通常使用<strong>EM算法</strong>进行参数估计。EM算法就是<strong>含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法</strong>。</p><h1 id=概率图模型的推断>概率图模型的推断</h1><p>在图模型中，<strong>推断(inference)<strong>是指在观测到部分变量$\boldsymbol e=\{e_1,e_2,\cdots,e_m\}$的情况下，计算其他变量的某个子集$\boldsymbol q=\{q_1,q_2,\cdots,,q_n\}$的</strong>后验概率</strong>$p(\boldsymbol q|\boldsymbol e)$。</p><p>假设一个图模型中，除了变量$\boldsymbol e$和$\boldsymbol q$外，其余变量表示为$\boldsymbol z$。根据条件概率公式，有：</p>$$
\begin{aligned}
p(\boldsymbol{q} | \boldsymbol{e})&=\frac{p(\boldsymbol{q}, \boldsymbol{e})}{p(\boldsymbol{e})}\\
&=\frac{\sum_{\boldsymbol{z}} p(\boldsymbol{q}, \boldsymbol{e}, \boldsymbol{z})}{\sum_{\boldsymbol{q}, \boldsymbol{z}} p(\boldsymbol{q}, \boldsymbol{e}, \boldsymbol{z})}
\end{aligned}
$$<p>因此，图模型的推断问题可以转换为求任意一个变量子集的边际概率分布问题。在图模型中，常用的推断算法可以分为<strong>精确推断</strong>和<strong>近似推断</strong>两类。</p><h2 id=精确推断>精确推断</h2><p><strong>变量消除法</strong>是常用的精确推断算法。以下图为例，假设推断问题为计算后验概率$p(x_1|x_4)$，需计算两个边际概率$p(x_1, x_4)$和$p(x_4)$。</p><div align=center><img src=/Kimages/2/image-20200820100717427.png style=zoom:15%></div><p>根据条件独立性假设，有</p>$$
p(x_1,x_4)=\sum_{x_2,x_3}p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2,x_3)
$$<p>假设每个变量取$K$个值，计算上面的边际分布需要$K^2$次加法以及$K^2 \times 3$次乘法。根据乘法的分配律，</p>$$
ab+ac=a(b+c)
$$<p>边际概率$p(x_1,x_4)$可以写为</p>$$
p(x_1,x_4)=p(x_1)\sum_{x_3}p(x_3|x_1)\sum_{x_2}p(x_2|x_1)p(x_4|x_2,x_3)
$$<p>这样计算量可以减少到$K^2+K$次加法和$K^2+K+1$次乘法。</p><p>这种方法利用动态规划的思想，<strong>每次消除一个变量</strong>来减少计算边际分布的计算复杂度，称为<strong>变量消除法(variable elimination algorithm)</strong>。随着图模型规模的增长，变量消除法的收益越大。变量消除法可以按照不同的顺序来消除变量。比如上面的推断问题也可以按照$x_3,x_2$的消除顺序进行计算。</p><p>同理，边际概率$p(x_4)$可以通过以下方式计算：</p>$$
p(x_{4})=\sum_{x_{3}} \sum_{x_{2}} p(x_{4} | x_{2}, x_{3}) \sum_{x_{1}} p(x_{3} | x_{1}) p(x_{2} | x_{1}) p(x_{1})
$$<p>变量消除法的一个缺点是在计算多个边际分布时存在很多重复的计算。比如在上面的图模型中，计算边际概率$p(x_4)$和$p(x_3)$时很多局部的求和计算是一样的。</p><p>除了变量消除法外，<strong>信念传播算法(belief propagation, BP)</strong> 算法也是常用的精确推断算法之一。</p><h2 id=近似推断>近似推断</h2><p>精确推断一般用于结构较简单的图。当图模型的结构较复杂时，精确推断开销很大。此外，若图模型中的变量是连续的，并且其积分函数没有闭型解时，也无法使用精确推断。因此，很多情况下也常采用近似的方法来进行推断。<strong>近似推断(approximate inference)</strong> 主要有三种方法：</p><p>(1) <strong>环路信念传播</strong>：当图模型中存在环路，使用信念传播算法时，消息会在环路中一直传递，可能收敛或不收敛。<strong>环路信念传播(loopy belief propagation, LBP)</strong> 是在具有环路的图上依然使用信念传播算法，即使得到不精确解，在某些任务上也可以近似精确解。</p><p>(2) <strong>变分推断</strong>：图模型中有些变量的局部条件分布可能非常复杂，或其积分无法计算。<strong>变分推断(variational inference)</strong> 是引入一个变分分布(通常是简单的分布)来近似这些条件概率，然后通过迭代的方法进行计算。首先是更新变分分布的参数来最小化变分分布和真实分布的差异(如交叉熵或KL距离)，然后再根据变分分布来进行推断。</p><p>(3) <strong>采样法</strong>：<strong>采样法(sampling method)</strong> 是通过模拟的方式来采集符合某个分布$p(\boldsymbol x)$的一些样本，并通过这些样本来估计和这个分布有关的运算，比如期望等。</p><h1 id=基于隐马尔可夫模型的语音识别>基于隐马尔可夫模型的语音识别</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> python_speech_features <span style=color:#ff79c6>import</span> <span style=color:#ff79c6>*</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> scipy.io <span style=color:#ff79c6>import</span> wavfile
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> hmmlearn <span style=color:#ff79c6>import</span> hmm
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.externals <span style=color:#ff79c6>import</span> joblib
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> wave
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 生成wavdict，key=wavid，value=wavfile</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>gen_wavlist</span>(wavpath):
</span></span><span style=display:flex><span>    wavdict <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    labeldict <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> (dirpath, dirnames, filenames) <span style=color:#ff79c6>in</span> os<span style=color:#ff79c6>.</span>walk(wavpath):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> filename <span style=color:#ff79c6>in</span> filenames:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> filename<span style=color:#ff79c6>.</span>endswith(<span style=color:#f1fa8c>&#39;.wav&#39;</span>):
</span></span><span style=display:flex><span>                filepath <span style=color:#ff79c6>=</span> os<span style=color:#ff79c6>.</span>sep<span style=color:#ff79c6>.</span>join([dirpath, filename])
</span></span><span style=display:flex><span>                fileid <span style=color:#ff79c6>=</span> filepath
</span></span><span style=display:flex><span>                wavdict[fileid] <span style=color:#ff79c6>=</span> filepath
</span></span><span style=display:flex><span>                label <span style=color:#ff79c6>=</span> filepath<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;/&#39;</span>)[<span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>                labeldict[fileid] <span style=color:#ff79c6>=</span> label
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> wavdict, labeldict
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># MFCC特征提取</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>compute_mfcc</span>(file):
</span></span><span style=display:flex><span>    fs, audio <span style=color:#ff79c6>=</span> wavfile<span style=color:#ff79c6>.</span>read(file)
</span></span><span style=display:flex><span>    mfcc_feat <span style=color:#ff79c6>=</span> mfcc(audio, samplerate<span style=color:#ff79c6>=</span>fs, numcep<span style=color:#ff79c6>=</span><span style=color:#bd93f9>13</span>, winlen<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.025</span>, winstep<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.01</span>,
</span></span><span style=display:flex><span>                     nfilt<span style=color:#ff79c6>=</span><span style=color:#bd93f9>26</span>, nfft<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2048</span>, lowfreq<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>, highfreq<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, preemph<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.97</span>)
</span></span><span style=display:flex><span>    d_mfcc_feat <span style=color:#ff79c6>=</span> delta(mfcc_feat, <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    feature_mfcc <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>hstack((mfcc_feat, d_mfcc_feat))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> feature_mfcc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Model</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, CATEGORY<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, n_comp<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>, n_mix<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>, cov_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;full&#39;</span>, n_iter<span style=color:#ff79c6>=</span><span style=color:#bd93f9>100000</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(Model, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(CATEGORY)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>CATEGORY <span style=color:#ff79c6>=</span> CATEGORY
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>category <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(CATEGORY)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_comp <span style=color:#ff79c6>=</span> n_comp
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_mix <span style=color:#ff79c6>=</span> n_mix
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>cov_type <span style=color:#ff79c6>=</span> cov_type
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_iter <span style=color:#ff79c6>=</span> n_iter
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 初始化models，返回特定参数的模型的列表</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>category):
</span></span><span style=display:flex><span>            model <span style=color:#ff79c6>=</span> hmm<span style=color:#ff79c6>.</span>GMMHMM(n_components<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_comp, n_mix<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>n_mix, covariance_type<span style=color:#ff79c6>=</span><span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>cov_type)
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models<span style=color:#ff79c6>.</span>append(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 模型训练</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>train</span>(<span style=font-style:italic>self</span>, wavdict<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, labeldict<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>10</span>):
</span></span><span style=display:flex><span>            model <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models[k]
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> wavdict:
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>if</span> labeldict[x] <span style=color:#ff79c6>==</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>CATEGORY[k]:
</span></span><span style=display:flex><span>                    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;k=&#39;</span>, k, wavdict[x])
</span></span><span style=display:flex><span>                    mfcc_feat <span style=color:#ff79c6>=</span> compute_mfcc(wavdict[x])
</span></span><span style=display:flex><span>                    model<span style=color:#ff79c6>.</span>fit(mfcc_feat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用特定的测试集合进行测试</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>test</span>(<span style=font-style:italic>self</span>, filepath):
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>category):
</span></span><span style=display:flex><span>            subre <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>            model <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models[k]
</span></span><span style=display:flex><span>            mfcc_feat <span style=color:#ff79c6>=</span> compute_mfcc(filepath)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 生成每个数据在当前模型下的得分情况</span>
</span></span><span style=display:flex><span>            re <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>score(mfcc_feat)
</span></span><span style=display:flex><span>            subre<span style=color:#ff79c6>.</span>append(re)
</span></span><span style=display:flex><span>            result<span style=color:#ff79c6>.</span>append(subre)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 选取得分最高的种类</span>
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>vstack(result)<span style=color:#ff79c6>.</span>argmax(axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 返回种类的类别标签</span>
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>=</span> [<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>CATEGORY[label] <span style=color:#ff79c6>for</span> label <span style=color:#ff79c6>in</span> result]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> result
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>save</span>(<span style=font-style:italic>self</span>, path<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;models.pkl&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 利用external joblib保存生成的hmm模型</span>
</span></span><span style=display:flex><span>        joblib<span style=color:#ff79c6>.</span>dump(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models, path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>load</span>(<span style=font-style:italic>self</span>, path<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;models.pkl&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 导入hmm模型</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>models <span style=color:#ff79c6>=</span> joblib<span style=color:#ff79c6>.</span>load(path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>gen_dataset</span>(wavdict, labeldict):
</span></span><span style=display:flex><span>    nums <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(labeldict)
</span></span><span style=display:flex><span>    shuf_arr <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>arange(nums)
</span></span><span style=display:flex><span>    np<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>shuffle(shuf_arr)
</span></span><span style=display:flex><span>    labelarr <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> l <span style=color:#ff79c6>in</span> labeldict:
</span></span><span style=display:flex><span>        labelarr<span style=color:#ff79c6>.</span>append(l)
</span></span><span style=display:flex><span>    wavdict_test <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    labeldict_test <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    wavdict_train <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    labeldict_train <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>int</span>(nums <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>0.05</span>)):
</span></span><span style=display:flex><span>        wavdict_test[labelarr[shuf_arr[i]]] <span style=color:#ff79c6>=</span> wavdict[labelarr[shuf_arr[i]]]
</span></span><span style=display:flex><span>        labeldict_test[labelarr[shuf_arr[i]]] <span style=color:#ff79c6>=</span> labeldict[labelarr[shuf_arr[i]]]
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> labeldict:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> k <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>in</span> labeldict_test:
</span></span><span style=display:flex><span>            wavdict_train[k] <span style=color:#ff79c6>=</span> wavdict[k]
</span></span><span style=display:flex><span>            labeldict_train[k] <span style=color:#ff79c6>=</span> labeldict[k]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> wavdict_train, labeldict_train, wavdict_test, labeldict_test
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>__name__</span> <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    CATEGORY <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39;1&#39;</span>, <span style=color:#f1fa8c>&#39;2&#39;</span>, <span style=color:#f1fa8c>&#39;3&#39;</span>, <span style=color:#f1fa8c>&#39;4&#39;</span>, <span style=color:#f1fa8c>&#39;5&#39;</span>, <span style=color:#f1fa8c>&#39;6&#39;</span>, <span style=color:#f1fa8c>&#39;7&#39;</span>, <span style=color:#f1fa8c>&#39;8&#39;</span>, <span style=color:#f1fa8c>&#39;9&#39;</span>, <span style=color:#f1fa8c>&#39;10&#39;</span>]
</span></span><span style=display:flex><span>    wavdict, labeldict <span style=color:#ff79c6>=</span> gen_wavlist(<span style=color:#f1fa8c>&#39;speech_digits&#39;</span>)  <span style=color:#6272a4># 需要将数据导入当前目录</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    wavdict_train, labeldict_train, wavdict_test, labeldict_test <span style=color:#ff79c6>=</span> gen_dataset(wavdict, labeldict)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;wavdict_train&#39;</span>, labeldict_train, <span style=color:#8be9fd;font-style:italic>len</span>(wavdict_train))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;wavdict_test&#39;</span>, labeldict_test, <span style=color:#8be9fd;font-style:italic>len</span>(wavdict_test))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    models <span style=color:#ff79c6>=</span> Model(CATEGORY<span style=color:#ff79c6>=</span>CATEGORY)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;start trainging....&#39;</span>)
</span></span><span style=display:flex><span>    models<span style=color:#ff79c6>.</span>train(wavdict<span style=color:#ff79c6>=</span>wavdict, labeldict<span style=color:#ff79c6>=</span>labeldict)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;finish trainging....&#39;</span>)
</span></span><span style=display:flex><span>    models<span style=color:#ff79c6>.</span>save()
</span></span><span style=display:flex><span>    models<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    TP, FP <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> wavdict_test:
</span></span><span style=display:flex><span>        wav_path <span style=color:#ff79c6>=</span> wavdict_test[k]
</span></span><span style=display:flex><span>        res <span style=color:#ff79c6>=</span> models<span style=color:#ff79c6>.</span>test(wav_path)[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(wavdict_test[k], res, labeldict_test[k])
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> res <span style=color:#ff79c6>==</span> labeldict_test[k]:
</span></span><span style=display:flex><span>            TP <span style=color:#ff79c6>+=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>            FP <span style=color:#ff79c6>+=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(TP, FP)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;acc:&#39;</span>, TP <span style=color:#ff79c6>/</span> (TP <span style=color:#ff79c6>+</span> FP))
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</p></li><li><p>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/ data-toggle=tooltip data-placement=top title=机器学习：话题模型>&larr;
Previous Post</a></li><li class=next><a href=/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/ data-toggle=tooltip data-placement=top title=深度学习概述>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>