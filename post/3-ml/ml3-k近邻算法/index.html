<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：K近邻算法"><meta property="og:title" content="机器学习：K近邻算法"><meta property="twitter:title" content="机器学习：K近邻算法"><meta name=description content="本文主要介绍KNN算法的原理、参数、kd树、基于numpy的k近邻算法实现。"><meta property="og:description" content="本文主要介绍KNN算法的原理、参数、kd树、基于numpy的k近邻算法实现。"><meta property="twitter:description" content="本文主要介绍KNN算法的原理、参数、kd树、基于numpy的k近邻算法实现。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：K近邻算法-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：K近邻算法</h1><h2 class=subheading>KNN-K-Nearest-Neighbor</h2><span class=meta>Posted by
XiangdiWu
on
Wednesday, September 30, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=近邻算法原理>$\boldsymbol k$近邻算法原理</h1><p>$k$近邻($k$-nearest neighbor, $k$NN)算法是一种常用的监督学习方法。其基本思想为：给定一组数据，基于某种距离度量找出训练集中与其最靠近的$k$个训练样本，然后基于这$k$个邻居的信息来进行预测。通常，在<strong>分类任务</strong>中可使用<strong>投票法</strong>，即选择$k$个样本中<strong>出现最多的类别标记</strong>作为预测结果；在<strong>回归任务</strong>中可使用平均法，即将$k$个样本的实值输出标记的平均值作为预测结果。该算法属于“<strong>惰性学习(lazy learning)</strong>”方法之一，没有显式的学习过程。相应的，那些在训练阶段就对样本进行学习处理的方法，称为“<strong>急切学习(eager learning)</strong>”。</p><p>$k$近邻算法的形式化表示如下：</p>$$
y=\arg\underset{c_j}\max \sum_{x_i\in{N_k(x)}}\mathbb I(y_i=c_j),i=1,2,\cdots,N;j=1,2,\cdots,K
$$<p>其中，$x_i$为实例特征向量，$y_i=\{c_1,c_2\cdots,c_k\}$为实例的类别，$N$为实例总数，$N_k(x)$为$x$在训练集总最邻近的$k$个点，$\mathbb I$为指示函数。</p><p>$k$近邻算法的特殊情况是$k=1$的情形，称为最近邻算法。</p><h1 id=近邻算法的参数>$\boldsymbol k$近邻算法的参数</h1><p>$k$近邻算法有<strong>三个基本的超参数(super parameter)</strong>：距离度量、$k$值的选择以及分类决策规则的决定。</p><h2 id=距离度量>距离度量</h2><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。$k$近邻模型的特征空间一般是$n$维实数空间，使用的距离是欧式距离，但也可以是其他距离，例如一般的$L_p$距离，即Minkowski距离：</p>$$
L_p(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$<p>在二维空间中的Minkowski距离的等距线示意图如下：</p><div align=center><img src=/Kimages/2/image-20200417170351526.png style=zoom:30%></div><div align=center><img src=/Kimages/2/image-20200417170411153.png style=zoom:30%></div><p>当$p=2$时，称为<strong>欧氏距离(Euclidean distance)</strong>；当$p=1$时，称为<strong>曼哈顿距离(Manhattan distance)</strong>。不同的距离度量确定的最近邻点不同。</p><h2 id=值的选择>$\boldsymbol k$值的选择</h2><p>$k$值的选择会对$k$近邻算法的结果产生重大影响。如果选择较小的$k$值，就相当于用较小的邻域中的训练实例进行预测，预测结果会对邻近的实例点非常敏感。如果邻近的实例点是噪声，预测就会出错。换句话说，$k$值的减小意味着整体模型变得复杂，容易发生<strong>过拟合</strong>。</p><p>如果选择较大的$k$值，就相当于用较大邻域中的训练实例进行预测，使模型变得简单。如果$k=N$，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。通常$k$一般取一个较小的数值，通常采用<strong>交叉验证法</strong>来选取最优的$k$值。</p><div align=center><img src=/Kimages/2/image-20200416180052685.png style=zoom:35%></div><h2 id=分类决策规则>分类决策规则</h2><p>$k$近邻算法中的分类决策规则往往是<strong>多数表决</strong>，即由输入实例的$k$个近邻的训练实例中的多数类决定输入实例所属的类别。分类决策规则也可以<strong>按照距离加权</strong>，即赋予每个邻近实例点一个权重，距离给定实例更近的邻近点的权重更大。这样做考虑到了多数表决时距离带来的影响。</p><h1 id=树>$\boldsymbol k\boldsymbol d$树</h1><p>实现$k$近邻算法时，主要考虑的问题是如何对训练数据进行快速$k$近邻搜索。这点在特征空间维数大及训练数据容量大时尤为重要。</p><h2 id=构造树>构造$\boldsymbol k \boldsymbol d$树</h2><p>实现$k$近邻算法时，对数据集进行线性扫描是非常耗时的，$kd$树是一种高效的训练数据存储方式。$kd$树是<strong>二叉树</strong>，表示对$k$维空间的一个<strong>划分</strong>。构造$kd$树相当于用垂直于坐标轴的超平面将$k$维空间切分，构成一系列的$k$维超矩形区域。$kd$树的<strong>每个节点</strong>对应一个$k$维<strong>超矩形区域</strong>。</p><p>构造$kd$树的方法如下：构造根结点，使根结点对应于$k$维空间中包含所有实例点的超矩形区域。通过下面的递归方法，不断地对$k$维空间进行切分，生成子结点。在**超矩形区域(结点)**上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域(子结点)。这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止。在此过程中，将实例保存在相应的结点上。</p><p>通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数为切分点，这样得到的kd树是平衡的，但平衡的$kd$树搜索时的<strong>效率未必是最优的</strong>。</p><p><strong>构造平衡$\boldsymbol k \boldsymbol d$树的算法</strong>：</p><p>输入：$k$维空间数据集$T={x_1,x_2,\cdots,x_N}$，其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(k)})^\text T，i=1,2,\cdots,N$。</p><p>输出：$kd$树。</p><p>(1) <strong>开始</strong>：构造根结点，对应包含$T$的$k$维空间的超矩形区域。选择$x^{(1)}$为坐标轴，以T中所有实例$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根结点生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应坐标$x^{(1)}$大于切分点的子区域。</p><p>将落在切分超平面上的实例点保存在根结点。</p><p>(2) <strong>重复</strong>：对深度为$j$的结点，选择$x^{(l)}$为切分，$l=j(\text{mod}k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域划分为两个子区域。以此类推。</p><p>(3) <strong>直到两个子区域没有实例存在时停止</strong>，从而形成$kd$树的区域划分。</p><p>下图为构建$kd$树的实例。数据集为：$T=\{(2,3)^\text T,(5,4)^\text T,(9,6)^\text T,(4,7)^\text T,(8,1)^\text T,(7,2)^\text T\}$。首先选择$x^{(1)}$轴(共两个维度，$x^{(1)}$即为第一个维度)，6个数据点的$x^{(1)}$坐标的中位数是7(6是中位数，但是$x^{(1)}=6$上无数据点)，以$x^{(1)}=7$将空间分为左右两个子矩形(子结点)；接着，做矩形以$x^{(2)}=4$分为两个子矩形，右矩形以$x^{(2)}=6$分为两个子矩形，如此递归，最终得到如右图所示的$kd$树。</p><div align=center><img src=/Kimages/2/image-20200417194932684.png style=zoom:40%></div><p>三维空间内$kd$树的构造如下所示：</p><div align=center><img src=/Kimages/2/image-20200417200514114.png style=zoom:40%></div><h2 id=搜索树>搜索$\boldsymbol k \boldsymbol d$树</h2><p>使用$kd$树可以减少搜索的计算量。以搜索最近邻为例：首先找到<strong>包含目标点的叶节点</strong>，然后从该结点出发，依次退回到父结点，不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。</p><p>包含目标点的叶结点对应包含目标点的最小超矩形区域。以此叶结点的实例点作为当前最近点。目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部。然后返回当前节点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点，将此点作为新的当前最近点。算法转到更上一级的父结点，继续上述过程。若父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。</p><p><strong>使用</strong>$\boldsymbol k \boldsymbol d$<strong>树的最近邻搜索算法</strong>：</p><p>输入：已构造的$kd$树、目标点$x$。</p><p>输出：$x$的最近邻。</p><p>(1) 在$kd$树中找出包含目标点$x$的叶结点。从根结点出发，递归地向下访问$kd$树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。</p><p>(2) 以此叶结点为当前最近点。</p><p>(3) 递归地向上回退，在每个结点进行以下操作：</p><p>如果该结点保存的实例点比当前最近点距目标点更近，则以该实例点为当前最近点。</p><p>当前最近点一定存在于该结点的一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与当前最近点间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一子结点，接着，递归地搜索；如果不相交，向上回退。</p><p>(4) 当回退到根结点时，搜索结束。最后的“当前最近点”即为$x$的最近邻点。</p><p>$kd$树搜索最近邻的示例如下所示。根结点为A，共有7个实例点；令有一个输入目标实例点S，求S的最近农林的过程如下：首先在$kd$树中找到包含S的叶结点D，以D点作为近似最近邻。<strong>真正最近邻一定在以点S为中心通过点D的元的内部</strong>。然后返回结点D的父结点B，在结点B的另一子结点F的区域内搜索最近邻。结点F的区域与圆不相交，不可能有最近邻点。继续返回上一级父结点A，在结点A的另一子结点C的区域内搜索最近邻。结点C的区域与圆相交；该区域在圆内的实例点有点E，点E比点D更近，称为新的最近邻近似。最后得到点E是点S的最近邻。</p><div align=center><img src=/Kimages/2/image-20200417203154010.png style=zoom:35%></div><h1 id=基于numpy的k近邻算法实现>基于numpy的k近邻算法实现</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> collections <span style=color:#ff79c6>import</span> Counter
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn <span style=color:#ff79c6>import</span> datasets
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.utils <span style=color:#ff79c6>import</span> shuffle
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>iris <span style=color:#ff79c6>=</span> datasets<span style=color:#ff79c6>.</span>load_iris()  <span style=color:#6272a4># 导入sklearn iris数据集</span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff79c6>=</span> shuffle(iris<span style=color:#ff79c6>.</span>data, iris<span style=color:#ff79c6>.</span>target, random_state<span style=color:#ff79c6>=</span><span style=color:#bd93f9>13</span>)  <span style=color:#6272a4># 打乱数据后的数据与标签</span>
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> X<span style=color:#ff79c6>.</span>astype(np<span style=color:#ff79c6>.</span>float32)  <span style=color:#6272a4># 数据转换为float32格式</span>
</span></span><span style=display:flex><span>offset <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>int</span>(X<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>0.7</span>)  <span style=color:#6272a4># 训练集与测试集的简单划分，训练-测试比例为7：3</span>
</span></span><span style=display:flex><span>X_train, y_train <span style=color:#ff79c6>=</span> X[:offset], y[:offset]
</span></span><span style=display:flex><span>X_test, y_test <span style=color:#ff79c6>=</span> X[offset:], y[offset:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_train <span style=color:#ff79c6>=</span> y_train<span style=color:#ff79c6>.</span>reshape((<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span>y_test <span style=color:#ff79c6>=</span> y_test<span style=color:#ff79c6>.</span>reshape((<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;X_train=&#39;</span>, X_train<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;X_test=&#39;</span>, X_test<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;y_train=&#39;</span>, y_train<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;y_test=&#39;</span>, y_test<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义欧氏距离</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>compute_distances</span>(X, X_train):
</span></span><span style=display:flex><span>    num_test <span style=color:#ff79c6>=</span> X<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]  <span style=color:#6272a4># 测试实例样本量</span>
</span></span><span style=display:flex><span>    num_train <span style=color:#ff79c6>=</span> X_train<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]  <span style=color:#6272a4># 训练实例样本量</span>
</span></span><span style=display:flex><span>    dists <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>zeros((num_test, num_train))  <span style=color:#6272a4># 基于训练和测试维度的欧氏距离初始化</span>
</span></span><span style=display:flex><span>    M <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>dot(X, X_train<span style=color:#ff79c6>.</span>T)  <span style=color:#6272a4># 测试样本与训练样本的矩阵点乘</span>
</span></span><span style=display:flex><span>    te <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>square(X)<span style=color:#ff79c6>.</span>sum(axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)  <span style=color:#6272a4># 测试样本矩阵平方</span>
</span></span><span style=display:flex><span>    tr <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>square(X_train)<span style=color:#ff79c6>.</span>sum(axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)  <span style=color:#6272a4># 训练样本矩阵平方</span>
</span></span><span style=display:flex><span>    dists <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sqrt(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> M <span style=color:#ff79c6>+</span> tr <span style=color:#ff79c6>+</span> np<span style=color:#ff79c6>.</span>matrix(te)<span style=color:#ff79c6>.</span>T)  <span style=color:#6272a4># 计算欧式距离</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> dists
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dists <span style=color:#ff79c6>=</span> compute_distances(X_test, X_train)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>imshow(dists, interpolation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义预测函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>predict_labels</span>(y_train, dists, k<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>):
</span></span><span style=display:flex><span>    num_test <span style=color:#ff79c6>=</span> dists<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]  <span style=color:#6272a4># 测试样本量</span>
</span></span><span style=display:flex><span>    y_pred <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>zeros(num_test)  <span style=color:#6272a4># 初始化测试集预测结果</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(num_test):
</span></span><span style=display:flex><span>        closest_y <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 初始化最近邻列表</span>
</span></span><span style=display:flex><span>        labels <span style=color:#ff79c6>=</span> y_train[np<span style=color:#ff79c6>.</span>argsort(dists[i, :])]<span style=color:#ff79c6>.</span>flatten()
</span></span><span style=display:flex><span>        closest_y <span style=color:#ff79c6>=</span> labels[<span style=color:#bd93f9>0</span>:k]  <span style=color:#6272a4># 取最近的k个值</span>
</span></span><span style=display:flex><span>        c <span style=color:#ff79c6>=</span> Counter(closest_y)  <span style=color:#6272a4># 对最近的k个值进行计数统计</span>
</span></span><span style=display:flex><span>        y_pred[i] <span style=color:#ff79c6>=</span> c<span style=color:#ff79c6>.</span>most_common(<span style=color:#bd93f9>1</span>)[<span style=color:#bd93f9>0</span>][<span style=color:#bd93f9>0</span>]  <span style=color:#6272a4># 取计数最多的类别</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> y_pred
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 测试集预测结果</span>
</span></span><span style=display:flex><span>y_test_pred <span style=color:#ff79c6>=</span> predict_labels(y_train, dists, k<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>y_test_pred <span style=color:#ff79c6>=</span> y_test_pred<span style=color:#ff79c6>.</span>reshape((<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span><span style=color:#6272a4># 找出预测正确的实例</span>
</span></span><span style=display:flex><span>num_correct <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sum(y_test_pred <span style=color:#ff79c6>==</span> y_test)
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算准确率</span>
</span></span><span style=display:flex><span>accuracy <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>float</span>(num_correct) <span style=color:#ff79c6>/</span> X_test<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Got </span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c>/</span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c> correct=&gt;accuracy:</span><span style=color:#f1fa8c>%f</span><span style=color:#f1fa8c>&#39;</span> <span style=color:#ff79c6>%</span> (num_correct, X_test<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>], accuracy))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 5折交叉验证</span>
</span></span><span style=display:flex><span>num_folds <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>
</span></span><span style=display:flex><span>k_choices <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>5</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>12</span>, <span style=color:#bd93f9>15</span>, <span style=color:#bd93f9>20</span>, <span style=color:#bd93f9>50</span>, <span style=color:#bd93f9>100</span>]  <span style=color:#6272a4># 候选k值</span>
</span></span><span style=display:flex><span>X_train_folds <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array_split(X_train, num_folds)  <span style=color:#6272a4># 训练数据划分</span>
</span></span><span style=display:flex><span>y_train_folds <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array_split(y_train, num_folds)  <span style=color:#6272a4># 训练标签划分</span>
</span></span><span style=display:flex><span>k_to_accuracies <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span><span style=color:#6272a4># 遍历所有候选k值</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> k_choices:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> fold <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(num_folds):  <span style=color:#6272a4># 五折遍历</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 对传入的训练集单独划出一个验证集作为测试集</span>
</span></span><span style=display:flex><span>        validation_X_test <span style=color:#ff79c6>=</span> X_train_folds[fold]
</span></span><span style=display:flex><span>        validation_y_test <span style=color:#ff79c6>=</span> y_train_folds[fold]
</span></span><span style=display:flex><span>        temp_X_train <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>concatenate(X_train_folds[:fold] <span style=color:#ff79c6>+</span> X_train_folds[fold <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>:])
</span></span><span style=display:flex><span>        temp_y_train <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>concatenate(y_train_folds[:fold] <span style=color:#ff79c6>+</span> y_train_folds[fold <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>:])
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 计算距离</span>
</span></span><span style=display:flex><span>        temp_dists <span style=color:#ff79c6>=</span> compute_distances(validation_X_test, temp_X_train)
</span></span><span style=display:flex><span>        temp_y_test_pred <span style=color:#ff79c6>=</span> predict_labels(temp_y_train, temp_dists, k<span style=color:#ff79c6>=</span>k)
</span></span><span style=display:flex><span>        temp_y_test_pred <span style=color:#ff79c6>=</span> temp_y_test_pred<span style=color:#ff79c6>.</span>reshape((<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>))
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 查看分类准确率</span>
</span></span><span style=display:flex><span>        num_correct <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sum(temp_y_test_pred <span style=color:#ff79c6>==</span> validation_y_test)
</span></span><span style=display:flex><span>        num_test <span style=color:#ff79c6>=</span> validation_X_test<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        accuracy <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>float</span>(num_correct) <span style=color:#ff79c6>/</span> num_test
</span></span><span style=display:flex><span>        k_to_accuracies[k] <span style=color:#ff79c6>=</span> k_to_accuracies<span style=color:#ff79c6>.</span>get(k, []) <span style=color:#ff79c6>+</span> [accuracy]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 打印不同 k 值不同折数下的分类准确率</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>sorted</span>(k_to_accuracies):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> accuracy <span style=color:#ff79c6>in</span> k_to_accuracies[k]:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;k = </span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c>, accuracy = </span><span style=color:#f1fa8c>%f</span><span style=color:#f1fa8c>&#39;</span> <span style=color:#ff79c6>%</span> (k, accuracy))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 打印不同 k 值不同折数下的分类准确率</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> k_choices:
</span></span><span style=display:flex><span>    accuracies <span style=color:#ff79c6>=</span> k_to_accuracies[k]  <span style=color:#6272a4># 取出第k个k值的分类准确率</span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter([k] <span style=color:#ff79c6>*</span> <span style=color:#8be9fd;font-style:italic>len</span>(accuracies), accuracies)  <span style=color:#6272a4># 绘制不同k值准确率的散点图</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>accuracies_mean <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([np<span style=color:#ff79c6>.</span>mean(v) <span style=color:#ff79c6>for</span> k, v <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>sorted</span>(k_to_accuracies<span style=color:#ff79c6>.</span>items())])
</span></span><span style=display:flex><span>accuracies_std <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array([np<span style=color:#ff79c6>.</span>std(v) <span style=color:#ff79c6>for</span> k, v <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>sorted</span>(k_to_accuracies<span style=color:#ff79c6>.</span>items())])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>errorbar(k_choices, accuracies_mean, yerr<span style=color:#ff79c6>=</span>accuracies_std)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Cross-validation on k&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#39;k&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#39;Cross-validation accuracy&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=使用scikit-learn中的k近邻算法对鸢尾花数据进行分类>使用scikit-learn中的k近邻算法对鸢尾花数据进行分类</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.neighbors <span style=color:#ff79c6>import</span> KNeighborsClassifier
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets <span style=color:#ff79c6>import</span> load_iris
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> classification_report
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>plot_decision_boundary</span>(model, axis):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    在axis范围内绘制模型model的决策边界
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param model: classification model which must have &#39;predict&#39; function
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param axis: [left, right, down, up]
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x0, x1 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>meshgrid(
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>linspace(axis[<span style=color:#bd93f9>0</span>], axis[<span style=color:#bd93f9>1</span>], <span style=color:#8be9fd;font-style:italic>int</span>((axis[<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>-</span> axis[<span style=color:#bd93f9>0</span>]) <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>))<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>),
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>linspace(axis[<span style=color:#bd93f9>2</span>], axis[<span style=color:#bd93f9>3</span>], <span style=color:#8be9fd;font-style:italic>int</span>((axis[<span style=color:#bd93f9>3</span>] <span style=color:#ff79c6>-</span> axis[<span style=color:#bd93f9>2</span>]) <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>))<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    X_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>c_[x0<span style=color:#ff79c6>.</span>ravel(), x1<span style=color:#ff79c6>.</span>ravel()]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y_predict <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>predict(X_new)
</span></span><span style=display:flex><span>    zz <span style=color:#ff79c6>=</span> y_predict<span style=color:#ff79c6>.</span>reshape(x0<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> matplotlib.colors <span style=color:#ff79c6>import</span> ListedColormap
</span></span><span style=display:flex><span>    custom_cmap <span style=color:#ff79c6>=</span> ListedColormap([<span style=color:#f1fa8c>&#39;#EF9A9A&#39;</span>, <span style=color:#f1fa8c>&#39;#FFF59D&#39;</span>, <span style=color:#f1fa8c>&#39;#90CAF9&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>contourf(x0, x1, zz, linewidth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>, cmap<span style=color:#ff79c6>=</span>custom_cmap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff79c6>=</span> load_iris(return_X_y<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> X[:, :<span style=color:#bd93f9>2</span>]  <span style=color:#6272a4># 仅选择前两个特征，便于绘制决策边界</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(X, y)  <span style=color:#6272a4># 将数据划分为训练集和测试集</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(X_train<span style=color:#ff79c6>.</span>shape, X_test<span style=color:#ff79c6>.</span>shape, y_train<span style=color:#ff79c6>.</span>shape, y_test<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (112, 2) (38, 2) (112,) (38,)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i, n <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>([<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>, <span style=color:#bd93f9>30</span>]):
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>3</span>, i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    knn_cls <span style=color:#ff79c6>=</span> KNeighborsClassifier(n_neighbors<span style=color:#ff79c6>=</span>n)
</span></span><span style=display:flex><span>    knn_cls<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>    y_pred <span style=color:#ff79c6>=</span> knn_cls<span style=color:#ff79c6>.</span>predict(X_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(classification_report(y_test, y_pred))  <span style=color:#6272a4># 分类报告中包含precision/recall/f1-score</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;n_neighbors=&#39;</span> <span style=color:#ff79c6>+</span> <span style=color:#8be9fd;font-style:italic>str</span>(n))
</span></span><span style=display:flex><span>    plot_decision_boundary(knn_cls, axis<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</li><li>鲁伟. 机器学习: 公式推导与代码实现. 北京: 人民邮电出版社, 2022.</li><li>Stanford University机器学习笔记：https://stanford.edu/~shervine/teaching/</li><li>Minkowski距离维基百科：https://en.wikipedia.org/wiki/Minkowski_distance</li><li>KD树维基百科https://en.wikipedia.org/wiki/K-d_tree</li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/ data-toggle=tooltip data-placement=top title=机器学习：线性回归>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/ data-toggle=tooltip data-placement=top title=机器学习：贝叶斯分类器>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>