<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：话题模型"><meta property="og:title" content="机器学习：话题模型"><meta property="twitter:title" content="机器学习：话题模型"><meta name=description content="本文主要介绍话题模型，包括单词向量空间与话题向量空间，以及潜在语义分析算法、概率潜在语义分析模型、潜在狄利克雷分配。"><meta property="og:description" content="本文主要介绍话题模型，包括单词向量空间与话题向量空间，以及潜在语义分析算法、概率潜在语义分析模型、潜在狄利克雷分配。"><meta property="twitter:description" content="本文主要介绍话题模型，包括单词向量空间与话题向量空间，以及潜在语义分析算法、概率潜在语义分析模型、潜在狄利克雷分配。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：话题模型-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：话题模型</h1><h2 class=subheading>Topic Modeling</h2><span class=meta>Posted by
XiangdiWu
on
Friday, October 9, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=单词向量空间与话题向量空间>单词向量空间与话题向量空间</h1><p><strong>潜在语义分析(latent semantic analysis, LSA)</strong> 是一种无监督学习方法，主要用于文本的话题分析，其特点是通过<strong>矩阵分解</strong>发现<strong>文本与单词之间基于话题的语义关系</strong>。</p><p>文本信息处理中，传统的方法以<strong>单词向量</strong>表示文本的语义内容，以<strong>单词向量空间的度量</strong>表示<strong>文本之间的语义相似度</strong>。潜在语义分析旨在解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现<strong>潜在的话题</strong>，以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度。这也是<strong>话题分析(topic modeling)的基本想法</strong>。</p><p>LSA首先将文本集合表示为<strong>单词-文本矩阵</strong>，对单词-文本矩阵进行<strong>奇异值分解</strong>，从而得到<strong>话题向量空间</strong>，以及<strong>文本在话题向量空间的表示</strong>。</p><h2 id=单词向量空间>单词向量空间</h2><p>信息检索、文本数据挖掘等领域的一个核心问题是对文本的语义内容进行表示，并进行文本之间的语义相似度计算。最简单的办法是<strong>向量空间模型(vector space model)</strong>：用一个向量表示<strong>某一文本的“语义”</strong>，向量的每一维对应一个单词，其数值为单词在文本中出现的频数或权值。如此一来，文本集合中的每个文本都表示为一个向量，存在于一个向量空间，文本间的内积表示文本相似度。</p><p>严格定义：给定一个含有$n$个文本的集合$D=\{d_1, d2, \cdots , d_n\}$，以及在所有文本中出现的$m$个单词的集合$W=\{w_1, w_2, \cdots , w_m\}$。单词-文本矩阵$X$定义为：</p>$$
X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]
$$<p>其中$x_{ij}$表示<strong>单词</strong>$w_i$在<strong>文本</strong>$d_j$中再出现的频数或权值，<strong>矩阵的一列对应一个文本</strong>。由于单词的种类很多，而每个文本中出现单词的种类通常较少，因此该矩阵是一个<strong>稀疏矩阵</strong>。</p><p>权值通常用单词频率-逆文本频率TF-IDF表示，其定义是：</p>$$
T F_{w}=\frac{\text {给定词} w \text {出现的次数}}{\text {文章的总次数}} \quad I D F_{w}=\log \left(\frac{\text {语料库文章的总数}}{\text {包含关键词} w \text {的文章数量}+1}\right) \quad TF-IDF=TF \times IDF
$$<p>直观上，<strong>一个单词在一个文本中出现的频数越高，这个单词在这个文本中的重要程度就越高</strong>；<strong>一个单词在整个文本集合中出现的文本数越少，这个单词就越能表示其所在文本的特点，重要度就越高</strong>。TF-IDF是两种重要度的积，表示综合重要度。</p><p>两个文本之间的相似度可以用<strong>规格化内积(内积除以范数的积)</strong> 来度量。例如，文本$d_i$和$d_j$之间的相似度为：</p>$$
\frac{x_i \cdots x_j}{\|x_i\|\|x_j\|}
$$<p>直观上，在两个文本中共同出现的单词越多，其语义内容就越相近，这时，对应的单词向量同不为零的维度就越多，内积就越大，表示两个文本在语义内容上越相似。</p><p>单词向量空间模型的优点是模型简单，计算效率高；局限性是<strong>内积并不能准确表达文本的语义相似度</strong>，且自然语言单词具有<strong>一词多义和多词一义</strong>的特性，因而存在不精确的问题。</p><h2 id=话题向量空间>话题向量空间</h2><p>两个文本的语义相似度可以体现在两者的<strong>话题(topic)相</strong>似度上。话题<strong>没有严格的定义</strong>，就是指文本所讨论的内容或主题。<strong>话题是抽象的概念，由奇异值分解得到</strong>。</p><p>话题向量空间模型与单词向量空间模型类似。假设文本集$X$中共含有$k$个话题，假设每个话题由一个定义在单词集合$W$上的$m$维向量表示，称为话题向量。则这$k$个话题中每个话题的维度为$m$，表示<strong>每个单词在这个话题中的权重</strong>。这$k$个话题向量张成一个话<strong>题向量空间</strong>，维数为$m \times k$。注意，话题向量空间$T$是单词向量空间$X$的一个子空间。话题向量空间$T$也可以表示为一个矩阵，称为<strong>单词-话题矩阵</strong>，记作：</p>$$
T=\left[\begin{array}{cccc}
t_{11} & t_{12} & \cdots & t_{1 k} \\
t_{21} & t_{22} & \cdots & t_{2 k} \\
\vdots & \vdots & & \vdots \\
t_{m 1} & t_{m 2} & \cdots & t_{m k}
\end{array}\right]
$$<p>考虑文本集合中的一个文本$d_j$，在单词向量空间中由一个向量$x_j$表示，将$x_j$投影到话题向量空间T中，得到在话题向量空间的一个向量$y_j$，$y_j$是一个$k$维向量，其中<strong>每一个分量是一个文本在一个话题对应的权值</strong>。</p><p>矩阵$Y$表示<strong>话题在文本中出现的情况</strong>，称为<strong>话题-文本矩阵</strong>。这样一来，在单词向量空间中的文本向量可以通过它在话题空间中的向量近似表示，即$X \approx TY$，这就是潜在语义分析。</p><p>直观上，<strong>潜在语义分析是将文本在单词向量空间的表示通过线性变换转换为在话题向量空间的表示</strong>，这个线性变换由矩阵因子分解体现。</p><h1 id=潜在语义分析算法>潜在语义分析算法</h1><p>潜在语义分析算法就是根据确定的话题个数$k$对单词-文本矩阵进行<strong>截断奇异值分解</strong>，将其左矩阵作为话题向量空间，将其对角矩阵与右矩阵的乘积作为文本在话题向量空间的表示的过程。算法如下：</p><p>(1) 获取单词-文本矩阵$X$；</p><p>(2) 将单词-文本矩阵进行截断奇异值分解$X=U_k \Sigma_k V_k^{\text T}$；</p><p>(3) 矩阵$U_k$的每一个列向量表示一个话题，称为话题向量；</p><p>(4) 文本的话题空间表示：矩阵$\Sigma_k V_k^\text T$的每一个列向量是一个文本在话题向量空间的表示。</p><p><strong>潜在语义索引(latent semantic indexing, LSI)</strong> 利用SVD分解找到单词-文档矩阵的某个低秩逼近，在这个低秩逼近下能够为文档集中的每篇文档产生一个新的表示。同样，查询也可以映射到这个低秩表示空间，从而可以基于新的表示来进行查询和文档相似度的计算。</p><h1 id=概率潜在语义分析模型>概率潜在语义分析模型</h1><p><strong>概率潜在语义分析(probabilistic latent semantic analysis, PLSA)</strong> 是一种利用概率生成模型对文本集合进行话题分析的无监督学习方法。模型最大的特点是<strong>使用隐变量表示话题</strong>。整个模型表示<strong>文本生成话题，话题生成单词，从而得到单词-文本共现数据的过程</strong>；假设每个文本由一个话题分布决定，每个话题由一个单词分布决定。PLSA有生成模型以及等价的共现模型。</p><h2 id=基本想法>基本想法</h2><p>给定一个文本集合，每个文本讨论若干个话题，每个话题由若干个单词表示。对文本集合进行概率潜在语义分析，就能够发现每个文本的话题，以及每个话题的单词。<strong>话题是不能从数据中直接观察到的，是潜在的</strong>。</p><p>将文本集合转换为<strong>单词-文本矩阵</strong>，文本数据基于如下的概率模型产生(<strong>共现模型</strong>)：首先有<strong>话题的概率分布</strong>，然后有<strong>话题给定条件下文本的条件概率分布</strong>，以及<strong>话题给定条件下单词的条件概率分布</strong>。概率潜在语义分析就是发现由隐变量表示的话题，即潜在语义。直观上，语义相近的单词、语义相近的文本会被聚类到相同的“软类别”中，而话题所表示的就是这样的软类别。</p><h2 id=生成模型>生成模型</h2><p>假设有<strong>单词集合</strong>$W=\{w_1, w_2, \cdots , w_M\}$，其中$M$是单词个数；<strong>文本集合</strong>$D=\{d_1, d_2, \cdots , d_N\}$，其中$N$是文本个数；<strong>话题集合</strong>$Z=\{z_1, z_2, \cdots , z_K\}$，其中$K$是预先设定的话题个数。随机变量$w$取值于单词集合，随机变量$d$取值于文本集合，随机变量$z$取值于话题集合。概率分布$P(d)$表示生成文本$d$的概率，$P(z|d)$表示文本$d$生成话题$z$的概率，$P(w|z)$表示话题$z$生成单词$w$的概率。每个文本都有自己的话题概率的分布$P(z|d)$，每个话题$z$拥有自己的单词概率分布$P(w|z)$；即一个文本的内容由其相关话题决定，一个话题的内容由其相关单词决定。</p><p>生成模型通过以下步骤生成文本-单词共现数据：</p><p>(1) 依据分布$P(d)$，从文本集合中随机选取一个文本，重复$N$次，共生成$N$个文本。对每个文本执行以下操作；</p><p>(2) 在文本$d$给定条件下，依据条件分布$P(z|d)$，从话题集合中随机选取一个话题$z$，共生成$L$个话题，这里$L$是<strong>文本长度</strong>；</p><p>(3) 在话题$z$给定条件下，依据条件概率分布$P(w|z)$，从单词集合<strong>随机选取一个单词</strong>。</p><p>注意，这里为叙述方便，<strong>假设文本都是等长的，现实中不需要这样的假设</strong>。</p><p>生成文本中，单词变量$w$与文本变量$d$是<strong>观测变量</strong>，话题变量$z$是<strong>隐变量</strong>。也就是说模型生成的是单词-话题-文本三元组$(w, z, d)$的集合，但观测数据是单词-文本二元组$(w, d)$集合，观测数据表示为单词-文本矩阵$T$，其行表示单词，列表示文本，值为$(w, d)$的出现次数。</p><p>从数据生成过程得出矩阵$T$生成的概率为所有$(w, d)$的生成概率的乘积：</p>$$
P(T)=\prod _{(w,d)}P(w,d)^{n(w,d)}
$$<p>这里$n(w,d)$表示$(w,d)$的出现次数，单词-文本对出现的总次数是$N \times L$。每个单词-文本对$(w,d)$的生成概率由以下公式决定：</p>$$
\begin{aligned}
P(w, d)&=P(d) P(w|d) \\
&=p(d) \sum_{z} p(w, z|d) \\
&=p(d) \sum_{z} p(z|d) p(w|z)
\end{aligned}
$$<p>以上便是生成模型的定义。</p><p>生成模型假设在话题$z$给定条件下，单词$w$与文本$d$条件独立，即$P(w,z|d)=P(z|d)P(w|z)$。</p><h2 id=共现模型>共现模型</h2><p>可以定义与生成模型等价的共现模型。文本-单词共现数据$T$的生成概率依然为：</p>$$
P(T)=\prod _{(w,d)}P(w,d)^{n(w,d)}
$$<p>共现模型中每个单词-文本对$(w, d)$的生成概率如下：</p>$$
P(w,d)= \sum_{z \in Z} P(z)P(w|z)P(d|z)
$$<p>共现模型与生成模型在概率公式上基本一致，但是生成过程是由$z$生成$d$和$w$，因此<strong>与生成模型具有不同的性质</strong>。<strong>生成模型刻画文本-单词共现数据生成的过程</strong>，<strong>共现模型描述文本-单词共现数据拥有的模式</strong>。生成模型中单词变量$w$和文本变量$d$是非对称的，而共现模型中单词变量$w$和文本变量$d$是对称的。因此，<strong>前者也称为非对称模型，后者也称为对称模型</strong>。</p><p>潜在概率语义分析模型是含有隐变量的模型，其学习通常采用<strong>EM算法</strong>。</p><h1 id=潜在狄利克雷分配>潜在狄利克雷分配</h1><p><strong>潜在狄利克雷分配(latent Dirichlet allocation, LDA)</strong> 作为基于贝叶斯学习的话题模型，是LSA和PLSA的扩展，于2002年由Blei等提出。LDA在文本数据挖掘、图像处理、生物信息处理等领域被广泛使用。LDA是<strong>文本集合的生成概率模型</strong>。假设<strong>每个文本由话题的一个多项分布表示</strong>，<strong>每个话题由单词的一个多项分布表示</strong>，特别假设<strong>文本的话题分布的先验分布是狄利克雷分布</strong>，<strong>话题的单词分布的先验分布也是狄利克雷分布</strong>。先验分布的导入使LDA更好地应对过拟合现象。</p><p>LDA的文本集合的生成过程如下：首先生成一个文本的话题分布，之后在该文本的每个位置，依据该文本的话题分布生成一个话题，然后在该位置依据该话题的单词分布生成一个单词，直至文本的最后一个位置。重复以上过程生成所有文本。</p><p><strong>LDA模型是含有隐变量的概率图模型</strong>。模型中，每个话题的单词分布，每个文本的话题分布，文本每个位置的话题是隐变量；文本的每个位置的单词是观测变量。LDA模型的学习与推理无法直接求解，通常使用吉布斯抽样和变分EM算法，前者是蒙特卡罗法，后者是近似算法。</p><h2 id=gamma函数>Gamma函数</h2><p>Gamma函数的定义如下：</p>$$
\Gamma(x)=\int_0^\infty t^{x-1} e^{-t} \text dt
$$<p>通过定积分的分部积分法，可以推导出这个函数有如下的递归性质：</p>$$
\Gamma(x+1)=x\Gamma(x)
$$<p>于是很容易证明，Gamma函数可以当成是阶乘在实数集上的延拓，具有如下性质：</p>$$
\Gamma(n)=(n-1)!
$$<p>其中$n$为自然数。Gamma函数有很多妙用，它不但使得非自然数的阶乘计算有意义，还能扩展很多其他的数学概念，比如导数，原来智能型以一阶、二阶等整数阶导数，而Gamma函数可以将函数的导数的定义延拓到实数集，从而可以计算1/2阶导数。同样，积分作为导数的逆运算，也有了分数阶。</p><h2 id=多项分布>多项分布</h2><p><strong>多项分布(multinomial distribution)<strong>是一种多元离散随机变量的概率分布，是</strong>二项分布的扩展</strong>。</p><p>假设重复进行$n$次独立随机试验，每次试验可能出现的结果有$k$种，第$i$种结果出现的概率为$p_i$，第$i$种结果出现的次数为$n_i$。如果用随机变量$X=(X_1,X_2,\cdots,X_k)$表示试验所有可能结果的次数，其中$X_i$表示第$i$种结果出现的次数，那么随机变量$X$服从多项分布。</p><p>若多元离散随机变量$X=(X_1,X_2,\cdots,X_k)$的<strong>概率质量函数</strong>为：</p>$$
P(X_{1}=n_{1}, X_{2}=n_{2}, \cdots, X_{k}=n_{k})=\frac{n !}{n_{1} ! n_{2} ! \cdots n_{k} !} p_{1}^{n_{1}} p_{2}^{n_{2}} \cdots p_{k}^{n_{k}}
$$<p>则称随机变量$X$服从参数为$(n,p)$的多项分布，记作$X \sim \text{Mult}(n,p)$。</p><p>当试验的次数为1时，多项分布变为<strong>类别分布(categorical distribution)</strong>。类别分布表示试验可能出现的$k$种结果的概率。多项分布包含类别分布。</p><h2 id=狄利克雷分布>狄利克雷分布</h2><p><strong>狄利克雷分布(Dirichlet distribution)</strong> 是一种多元连续随机变量的概率分布，是<strong>Beta分布的扩展</strong>。在贝叶斯学习中，狄利克雷分布常作<strong>为多项分布的先验分布</strong>使用。</p><p>若多元连续随机变量$\theta=(\theta_1,\theta_2,\cdots,\theta_k)$的概率密度函数为：</p>$$
p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^{k} \alpha_i)}{\prod_{i=1}^{k}\Gamma(\alpha_i)} \prod_{i=1}^{k}\theta_{i}^{\alpha_i-1}
$$<p>其中$\sum_{i=1}^{k}\theta_i=1$，$\theta_i \geqslant 0$，$\alpha_i > 0$，则称随机变量$\theta$服从参数为$\alpha$的狄利克雷分布，记作$\theta \sim \text{Dir}(\alpha)$。</p><h2 id=二项分布和beta分布>二项分布和Beta分布</h2><p>二项分布是多项分布的特殊情况，Beta分布式狄利克雷分布的特殊情况。</p><p>二项分布是指如下概率分布。$X$为离散型随机变量，取值为$m$，其概率质量函数为：</p>$$
P(X=m)=\left(\begin{array}{l}
n \\
m
\end{array}\right) p^{m}(1-p)^{n-m}
$$<p>其中$n$和$p$为参数。</p><p>Beta分布是指如下概率分布，$X$为连续随机变量，取值范围为$[0,1]$，其概率密度函数为：</p>$$
p(x)=\left\{\begin{array}{ll}
\frac{1}{B(s, t)} x^{s-1}(1-x)^{t-1}, & 0 \leqslant x \leqslant 1 \\
0, & Other
\end{array}\right.
$$<p>其中$s>0$和$t>0$是参数，$B(s,t)=\frac{\Gamma(s)\Gamma(t)}{\Gamma(s+t)}$是Beta函数，定义为：</p>$$
B(s,t)=\int_0^1 x^{s-1}(1-x)^{t-1} \text{d}x
$$<p>当$s,t$是自然数时，</p>$$
B(s,t)=\frac{(s-1)!(t-1)!}{(s+t-1)!}
$$<p>当$n=1$时，二项分布变为<strong>伯努利分布(Bernoulli distribution)</strong> 或0-1分布。</p><h2 id=共轭先验>共轭先验</h2><p>贝叶斯学习中常使用<strong>共轭分布(conjugate distribution)</strong>。如果后验分布与先验分布属于同类，则先验分布与后验分布称为共轭分布。先验分布称为<strong>共轭先验(conjugate prior)</strong>。如果多项分布的先验分布是狄利克雷分布，则其后验分布也为狄利克雷分布，两者构成共轭分布。作为先验分布的狄利克雷分布的参数又称为超参数。<strong>使用共轭分布的好处是便于从先验分布计算后验分布</strong>。</p><p>狄利克雷分布是多项分布的共轭先验，Beta分布是二项分布的共轭先验。</p><h2 id=潜在狄利克雷分配模型>潜在狄利克雷分配模型</h2><p>潜在狄利克雷分配(LDA)是文本集合的生成概率模型。模型假设<strong>话题由单词的多项分布表示</strong>，<strong>文本由话题的多项分布表示</strong>，<strong>单词和话题分布的先验分布都是狄利克雷分布</strong>。</p><p><strong>LDA模型表示文本集合的自动生成过程</strong>：首先，基于单词分布的先验分布(狄利克雷分布)生成多个单词分布，即决定多个话题内容；之后，基于话题分布的先验分布(狄利克雷分布)生成多个话题分布，即决定多个文本内容；然后，基于每一个话题分布生成话题序列，针对每一个话题，基于话题的单词分布生成单词，整体构成一个单词序列，即生成文本，重复这个过程生成所有文本。文本的单词序列是观测变量，文本的话题序列是隐变量，文本的话题分布和话题的单词分布也是隐变量。</p><p>LDA模型是概率图模型，其特点是以狄利克雷分布为多项分布的先验分布，学习就是给定文本集合，通过后验概率分布的估计，推断模型的所有参数。<strong>利用LDA进行话题分析，就是对给定文本集合，学习到每个文本的话题分布，以及每个话题的单词分布</strong>。</p><p>可以认为LDA是PLSA的扩展，相同点是<strong>二者都假设话题是单词的多项分布，文本是话题的多项分布</strong>。不同点是LDA使用狄利克雷分布作为先验分布，而<strong>PLSA不使用先验分布(或者说假设先验分布是均匀分布)</strong>，两者对文本生成过程有不同假设，即LDA基于贝叶斯学习，而PLSA基于极大似然估计。<strong>LDA的优点是，使用先验概率分布，可以防止学习过程中产生过拟合</strong>。</p><p>LDA的学习(参数估计)是一个复杂的最优化问题，很难精确求解，只能近似求解。常用的近似求解方法有<strong>吉布斯抽样(Gibbs sampling)<strong>和</strong>变分推理(variational inference)</strong>。</p><h1 id=使用gensim中的潜在狄利克雷分配对题目文本进行主题分析>使用gensim中的潜在狄利克雷分配对题目文本进行主题分析</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> gensim
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> gensim <span style=color:#ff79c6>import</span> corpora, models
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(<span style=color:#f1fa8c>&#39;lda_data.txt&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>, sep<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>&#39;</span>, quoting<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>)  <span style=color:#6272a4># 需要将数据导入当前目录</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(df<span style=color:#ff79c6>.</span>head)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#ff79c6>=</span> df<span style=color:#ff79c6>.</span>dropna()
</span></span><span style=display:flex><span>docs <span style=color:#ff79c6>=</span> df[<span style=color:#f1fa8c>&#39;segs&#39;</span>]  <span style=color:#6272a4># 选出文档部分，该部分为分词后的题干</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(docs<span style=color:#ff79c6>.</span>head)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>doclist <span style=color:#ff79c6>=</span> docs<span style=color:#ff79c6>.</span>values
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(doclist[<span style=color:#bd93f9>0</span>])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(doclist[<span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(doclist[<span style=color:#bd93f9>2</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>texts <span style=color:#ff79c6>=</span> [[word <span style=color:#ff79c6>for</span> word <span style=color:#ff79c6>in</span> doc<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39; &#39;</span>) <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(word) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>for</span> doc <span style=color:#ff79c6>in</span> doclist]  <span style=color:#6272a4># 按空格切分，得到单词列表</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(texts[<span style=color:#bd93f9>0</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 词袋模型处理</span>
</span></span><span style=display:flex><span>dictionary <span style=color:#ff79c6>=</span> corpora<span style=color:#ff79c6>.</span>Dictionary(texts)
</span></span><span style=display:flex><span>corpus <span style=color:#ff79c6>=</span> [dictionary<span style=color:#ff79c6>.</span>doc2bow(text) <span style=color:#ff79c6>for</span> text <span style=color:#ff79c6>in</span> texts]
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(corpus[<span style=color:#bd93f9>2</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 开始训练LDA主题模型</span>
</span></span><span style=display:flex><span>lda <span style=color:#ff79c6>=</span> gensim<span style=color:#ff79c6>.</span>models<span style=color:#ff79c6>.</span>ldamodel<span style=color:#ff79c6>.</span>LdaModel(corpus<span style=color:#ff79c6>=</span>corpus, id2word<span style=color:#ff79c6>=</span>dictionary, num_topics<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>)  <span style=color:#6272a4># num_topics为主题数</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(lda<span style=color:#ff79c6>.</span>print_topic(<span style=color:#bd93f9>1</span>, topn<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(lda<span style=color:#ff79c6>.</span>print_topics(num_topics<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>, num_words<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 样本文档</span>
</span></span><span style=display:flex><span>bow_sample <span style=color:#ff79c6>=</span> [(<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>4</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>5</span>, <span style=color:#bd93f9>3</span>), (<span style=color:#bd93f9>6</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>7</span>, <span style=color:#bd93f9>3</span>), (<span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>9</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>11</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>12</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>13</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>14</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>15</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>16</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>17</span>, <span style=color:#bd93f9>2</span>), (<span style=color:#bd93f9>18</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>19</span>, <span style=color:#bd93f9>3</span>), (<span style=color:#bd93f9>20</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>21</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>22</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>23</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>24</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>25</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>26</span>, <span style=color:#bd93f9>2</span>), (<span style=color:#bd93f9>27</span>, <span style=color:#bd93f9>3</span>), (<span style=color:#bd93f9>28</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>29</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>30</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>31</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>32</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>33</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>34</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>35</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>36</span>, <span style=color:#bd93f9>1</span>), (<span style=color:#bd93f9>37</span>, <span style=color:#bd93f9>1</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取样本文档的主题</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(lda<span style=color:#ff79c6>.</span>get_document_topics(bow_sample))
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</p></li><li><p>靳志辉. LDA数学八卦(version 1.0).</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/ data-toggle=tooltip data-placement=top title=机器学习：特征选择>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/ data-toggle=tooltip data-placement=top title=机器学习：概率图模型>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>