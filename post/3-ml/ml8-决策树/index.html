<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="机器学习：决策树"><meta property="og:title" content="机器学习：决策树"><meta property="twitter:title" content="机器学习：决策树"><meta name=description content="本文主要介绍决策树的基本原理，包括决策树的生成和剪枝。"><meta property="og:description" content="本文主要介绍决策树的基本原理，包括决策树的生成和剪枝。"><meta property="twitter:description" content="本文主要介绍决策树的基本原理，包括决策树的生成和剪枝。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>机器学习：决策树-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/machine-learning title="Machine Learning">Machine Learning</a></div><h1>机器学习：决策树</h1><h2 class=subheading>Decision Tree</h2><span class=meta>Posted by
XiangdiWu
on
Sunday, October 4, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=决策树模型>决策树模型</h1><p><strong>决策树(decision tree)</strong> 是一种基本的<strong>分类</strong>与<strong>回归</strong>算法。决策树呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，它可以认为是<strong>if-then规则</strong>的集合，也可以认为是定义在特征空间与类空间上的<strong>条件概率分布</strong>。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用训练好的决策模型进行分类。</p><p>分类决策树模型是一种由<strong>结点(node</strong>)和<strong>有向边(directed edge)</strong> 组成的树形结构。结点有两种类型：<strong>内部节点(internal node)，表示一个特征或属性；叶结点(leaf node)，表示一个类</strong>。用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至到达叶结点。对吼将实例分到叶结点对应的类中。</p><p>决策树可以看成一个if-then规则的集合，满足<strong>互斥并且完备</strong>的性质。决策树根结点到叶结点的每一条路径中，路径上的内部结点的特征对应着规则的<strong>条件</strong>，而叶结点的类对应着规则的<strong>结论</strong>。</p><p>决策树还表示给定特征条件下类的条件概率分布，这一分布定义在特征空间的一个<strong>划分(partition)<strong>上。将特征空间划分为互不相交的</strong>单元(cell)<strong>或</strong>区域(region)</strong>，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么该条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分下单元的集合，$Y$取值于类的集合。各叶结点(单元)上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的一类去。</p><p>决策树学习的本质是从训练数据集中归纳出一组分类规则。与训练集不相矛盾的决策树(即能对训练数据进行正确分类的决策树)可能有多个，也可能一个都没有。我们需要的是一个<strong>与训练数据矛盾较小的，同时具有很好的泛化能力的决策树</strong>。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间的类的条件概率模型有无穷多个，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p><p>决策树的学习算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着特征空间的，也对应着决策树的构建。大致过程如下：</p><p>开始，构建根结点，将所有训练数据都放在根结点。选择一个<strong>最优特征</strong>，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。</p><p>以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行<strong>剪枝</strong>，使树变得简单，从而使其具有更好的泛化能力。具体地，去掉过于细分的叶结点，使其退回到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。通常，<strong>层数越多即越深的决策树的复杂性越高，层数越少即越浅的决策树的复杂性越低</strong>。</p><div align=center><img src=/Kimages/2/image-20200428151625612.png style=zoom:50%></div><h1 id=特征选择>特征选择</h1><p><strong>特征选择(feature selection)</strong> 是决定用哪个特征来划分特征空间。特征选择在于选取对训练数据具有分类能力的特征，以提高决策树的学习效率。通常特征选择的准则是信息增益或信息增益比。</p><h2 id=信息增益>信息增益</h2><p>在信息论与概率统计中，<strong>熵(entropy)</strong> 是表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为：$P\left(X=x_{i}\right)=p_{i},\ \ i=1,2,\cdots,n$，则随机变量$X$的熵的定义是：</p>$$
H(X)=\sum_{i=1}^{n}p_i\log p_i
$$<p>在上式中，若$p_i=1$，则定义$0\log 0=0$。通常，上式中的对数<strong>以2或e为底数</strong>，这时熵的单位分别称作<strong>比特(bit)或纳特(nat)</strong>。由定义可知，熵只依赖于$X$的分布，而与$X$的取值无关，因此也可将$X$的熵记作$H(p)$，即：</p>$$
H(p)=\sum_{i=1}^{n}p_i \log p_i
$$<p>熵越大，随机变量的不确定性越大。设有随机变量$(X,Y)$，其<strong>联合概率分布</strong>为：</p>$$
P(X=x_{i}, Y=y_{j})=p_{ij},\ \ i=1,2,\cdots,n;\ \ j=i=1,2,\cdots,m
$$<p><strong>条件熵(conditional entropy)</strong>$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：</p>$$
H(Y|X)=\sum_{i=1}^{n}p_i H(Y|X=x_i),\ \ p_i=P(X=x_i),\ \ i=1,2,\cdots,n
$$<p>当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为<strong>经验熵(empirical entropy)</strong> 和 <strong>经验条件熵(empirical conditional entropy)</strong>。</p><p><strong>信息增益(information gain)</strong> 表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即：</p>$$
g(D,A)=H(D)-H(D|A)
$$<p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为<strong>互信息(mutual information)</strong>，决策树中的信息增益等价于<strong>训练数据集中类与特征的互信息</strong>。</p><p>信息增益表示由于特征$A$使得对数据集$D$的分类的不确定性减少的程度。显然，对数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。根据信息增益准则的特征选择方法是：对训练数据集(或子集)$D$，计算其每个特征的信息增益，<strong>选择信息增益最大的特征</strong>。</p><p>设训练数据集为$D$，$|D|$表示其样本容量。设有$K$个类$C_k,k=1,2,\cdots,K$，$|C_k|$为术语类$C_k$的样本个数。设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i \cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数。</p><p>信息增益算法如下：</p><p>输入：训练数据集$D$和$A$；</p><p>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$</p><p>(1) 计算数据集$D$的经验熵$H(D)$：</p>$$
H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log \frac{|C_k|}{|D|}
$$<p>(2) 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：</p>$$
H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log \frac{|D_{ik}|}{|D_i|}
$$<p>(3) 计算信息增益：</p>$$
g(D,A)=H(D)-H(D|A)
$$<h2 id=信息增益比>信息增益比</h2><p>以信息增益作为划分训练数据集的特征，存在<strong>偏向于选择取值角度的特征</strong>的问题。使用信息增益比(information gain ratio)可以对这一问题进行校正，这是特征选择的另一准则。</p><p>特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的取值的熵$H_A(D)$之比，即：</p>$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
$$<p>其中，</p>$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{|D_{i}|}{|D|} \log _{2} \frac{|D_{i}|}{|D|}
$$<p>$n$是特征$A$取值的个数。</p><h2 id=基尼指数>基尼指数</h2><p>除了信息增益和信息增益比以外，<strong>CART决策树</strong>使用<strong>基尼指数(Gini index)</strong> 来选择划分属性。在分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：</p>$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$<p>对于给定的样本集合$D$，其基尼指数为：</p>$$
Gini(D)=1-\sum_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2
$$<p>$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。若样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即$D_1=\{(x,y)\in D|A(x)=a\},\ \ D_2=D-D_1$，则在特征$A$的条件下，集合$D$的基尼指数定义为：</p>$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$<p>基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p><h1 id=决策树的生成>决策树的生成</h1><h2 id=id3算法>ID3算法</h2><p>输入：训练数据集$D$，特征集$A$，阈值$\varepsilon$。</p><p>输出：决策树$T$。</p><p>(1) 若$D$中<strong>所有实例属于同一类</strong>$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$；</p><p>(2) 若$A=\varnothing$，则$T$为<strong>单节点树</strong>，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</p><p>(3) 否则，计算$A$中各特征对$D$的信息增益，选择<strong>信息增益最大的特征</strong>$A_g$。</p><p>(4) 如果$A_g$的信息增益小于<strong>阈值</strong>$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的标记，返回$T$；</p><p>(5) 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构造子结点，并由结点及其子结点构成树$T$，返回$T$；</p><p>(6) 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，<strong>递归地调用(1)~(5)</strong>，得到子树$T_i$，返回$T_i$。</p><h2 id=c45算法>C4.5算法</h2><p>C4.5算法与ID3算法相似，改进的地方是C4.5采用信息增益比作为特征选择的标准，而ID3算法是使用信息增益作为特征选择标准。</p><p>输入：训练数据集$D$，特征集$A$，阈值$\varepsilon$。</p><p>输出：决策树$T$。</p><p>(1) 若$D$中<strong>所有实例属于同一类</strong>$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$；</p><p>(2) 若$A=\varnothing$，则$T$为<strong>单节点树</strong>，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；</p><p>(3) 否则，计算$A$中各特征对$D$的信息增益比，选择<strong>信息增益比最大的特征</strong>$A_g$。</p><p>(4) 如果$A_g$的信息增益小于<strong>阈值</strong>$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的标记，返回$T$；</p><p>(5) 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干个非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构造子结点，并由结点及其子结点构成树$T$，返回$T$；</p><p>(6) 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，<strong>递归地调用(1)~(5)</strong>，得到子树$T_i$，返回$T_i$。</p><h1 id=决策树的剪枝>决策树的剪枝</h1><p><strong>剪枝(pruning)</strong> 是决策树学习算法对付过拟合的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重读，有时会造成决策树分支过多，这时就可能因训练样本学得“太好”了，以致于把训练集自身的一些特点当做所有数据都具有的一般性质而导致过拟合。</p><p>决策树剪枝的基本策略有预剪枝和后剪枝两种。**预剪枝(pre-pruning)**是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝 <strong>(post-pruning)</strong> 则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升(在验证集上准确率性能提高)，则将该子树替换为叶结点。</p><p>预剪枝通常在生成决策树的过程中<strong>使用一个验证集来决定每次划分</strong>，因此可以使得很多分支没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但另一方面，有些分支的当前划分虽不能提升泛化性能，甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高。预剪枝基于“贪心”本质禁止一些分支的展开，带来了欠拟合的风险。</p><p>后剪枝先从训练集生成一棵完整的决策树，再自底向上遍历每个内部结点，若合并某个内部结点后能够带来在验证集上的精度提升，则合并该结点。以此类推，直至无法再合并为止。</p><p>一般来说，后剪枝决策树通常比预剪枝决策树保留了更多的分支，欠拟合风险更小，<strong>泛化性能往往优于预剪枝决策树</strong>。但后剪枝过程是在完全生成决策树之后进行的，并且要<strong>自底向上地对树中的所有非叶结点进行逐一考察</strong>，因此其<strong>训练时间开销</strong>比未剪枝决策树和预剪枝决策树都要大得多。</p><h1 id=连续与缺失值>连续与缺失值</h1><h2 id=连续值处理>连续值处理</h2><p>目前为止仅讨论了基于<strong>离散属性</strong>来生成决策树。对于连续属性，不能直接根据其取值来进行结点划分，因此，<strong>连续属性离散化技术</strong>可派上用场。最简单的策略是采用<strong>二分法(bi-partition)</strong> 对连续属性进行处理。给定样本集$D$和连续属性$a$，假定$a$在$D$上出现了$n$个不同的取值，将这些值从小到大进行排序，记为$\{a^1,a^2,\cdots,a^n\}$。基于划分点$t$可将$D$分为子集$D_t^+$和$D_t^-$，其中$D_t^+$包含那些在属性$a$上取值大于$t$的样本，$D_t^-$包含那些在属性$a$上取值不大于$a$的样本。显然，对相邻的属性取值$a^i$和$a^{i+1}$来说，$t$在区间$[a^i,a^{i+1})$中取任意值所产生的划分结果相同。因此，对连续属性$a$，可考察包含$n-1$个元素的候选划分点集合：</p>$$
T_a=\left\{\frac{a^i+a^{i+1}}{2}|1 \leqslant i \leqslant n-1 \right\}
$$<p>把区间$[a^i,a^{i+1})$的中位点$\frac{a^i+a^{i+1}}{2}$作为候选划分点，然后就可以像离散属性一样来考察这些划分点。</p><h2 id=缺失值处理>缺失值处理</h2><p>现实任务中常会遇到不完整样本，即样本的某些属性值缺失。如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，显然是对数据信息极大的浪费。</p><p>考虑利用有缺失属性值的训练样例进行学习，需要解决两个问题：(1) 如何在属性值缺失的情况下进行<strong>划分属性选择</strong>？(2) 给定划分属性，若样本在该属性上的值缺失，如何<strong>对样本进行划分</strong>？</p><p>给定训练集$D$和属性$a$，令$\tilde D$表示$D$中在属性$a$上<strong>没有缺失值的样本子集</strong>，并假定我们为每一个样本$x$赋予一个权重$w_x$。对问题(1)，显然我们仅可根据$\tilde D$来判断$a$的优劣。对问题(2)，若样本$x$在划分属性$a$上取值已知，则将$x$划入与其取值对应的子结点，且样本权在子结点中保持为$w_x$。若样本$x$在划分属性$a$上取值未知，则将$x$同时划入所有子结点，且对样本在属性$a$不同取值上的权值进行一定调整。</p><h1 id=多变量决策树>多变量决策树</h1><p>若把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类意味着在这个做表空间中寻找不同样本之间的分类边界。决策树形成的分类边界有一个明显的特点：<strong>轴平行(axis-parallel)，即它的分类边界由若干个与坐标轴平行的分段组成</strong>。</p><p>这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，此时决策树会相当复杂，由于要进行大量的属性测试，预测时间开销会很大。</p><p><strong>多变量决策树(multivariate decision tree)</strong> 能够实现特征空间的“斜划分”甚至更复杂划分的决策树。以斜划分为例，在此类决策树中，非叶结点不再是针对某个属性，而是对属性的线性组合进行测试。</p><h1 id=使用scikit-learn中的决策树算法对鸢尾花数据进行分类>使用scikit-learn中的决策树算法对鸢尾花数据进行分类</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.tree <span style=color:#ff79c6>import</span> DecisionTreeClassifier
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.datasets <span style=color:#ff79c6>import</span> load_iris
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.metrics <span style=color:#ff79c6>import</span> classification_report
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>plot_decision_boundary</span>(model, axis):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    在axis范围内绘制模型model的决策边界
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param model: classification model which must have &#39;predict&#39; function
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param axis: [left, right, down, up]
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x0, x1 <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>meshgrid(
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>linspace(axis[<span style=color:#bd93f9>0</span>], axis[<span style=color:#bd93f9>1</span>], <span style=color:#8be9fd;font-style:italic>int</span>((axis[<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>-</span> axis[<span style=color:#bd93f9>0</span>]) <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>))<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>),
</span></span><span style=display:flex><span>        np<span style=color:#ff79c6>.</span>linspace(axis[<span style=color:#bd93f9>2</span>], axis[<span style=color:#bd93f9>3</span>], <span style=color:#8be9fd;font-style:italic>int</span>((axis[<span style=color:#bd93f9>3</span>] <span style=color:#ff79c6>-</span> axis[<span style=color:#bd93f9>2</span>]) <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>))<span style=color:#ff79c6>.</span>reshape(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    X_new <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>c_[x0<span style=color:#ff79c6>.</span>ravel(), x1<span style=color:#ff79c6>.</span>ravel()]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y_predict <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>predict(X_new)
</span></span><span style=display:flex><span>    zz <span style=color:#ff79c6>=</span> y_predict<span style=color:#ff79c6>.</span>reshape(x0<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> matplotlib.colors <span style=color:#ff79c6>import</span> ListedColormap
</span></span><span style=display:flex><span>    custom_cmap <span style=color:#ff79c6>=</span> ListedColormap([<span style=color:#f1fa8c>&#39;#EF9A9A&#39;</span>, <span style=color:#f1fa8c>&#39;#FFF59D&#39;</span>, <span style=color:#f1fa8c>&#39;#90CAF9&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>contourf(x0, x1, zz, linewidth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>, cmap<span style=color:#ff79c6>=</span>custom_cmap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#ff79c6>=</span> load_iris(return_X_y<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> X[:, :<span style=color:#bd93f9>2</span>]  <span style=color:#6272a4># 仅选择前两个特征，便于绘制决策边界</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#ff79c6>=</span> train_test_split(X, y)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(X_train<span style=color:#ff79c6>.</span>shape, X_test<span style=color:#ff79c6>.</span>shape, y_train<span style=color:#ff79c6>.</span>shape, y_test<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (112, 2) (38, 2) (112,) (38,)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 尝试不同限定深度的决策树，并绘制其决策边界</span>
</span></span><span style=display:flex><span>dec_tree <span style=color:#ff79c6>=</span> DecisionTreeClassifier()
</span></span><span style=display:flex><span>dec_tree<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dec_tree_10 <span style=color:#ff79c6>=</span> DecisionTreeClassifier(max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>)
</span></span><span style=display:flex><span>dec_tree_10<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dec_tree_6 <span style=color:#ff79c6>=</span> DecisionTreeClassifier(max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>6</span>)
</span></span><span style=display:flex><span>dec_tree_6<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dec_tree_4 <span style=color:#ff79c6>=</span> DecisionTreeClassifier(max_depth<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span>)
</span></span><span style=display:flex><span>dec_tree_4<span style=color:#ff79c6>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;No max depth&#39;</span>)
</span></span><span style=display:flex><span>plot_decision_boundary(dec_tree, axis<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Max depth = 10&#39;</span>)
</span></span><span style=display:flex><span>plot_decision_boundary(dec_tree_10, axis<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Max depth = 6&#39;</span>)
</span></span><span style=display:flex><span>plot_decision_boundary(dec_tree_6, axis<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>4</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>title(<span style=color:#f1fa8c>&#39;Max depth = 4&#39;</span>)
</span></span><span style=display:flex><span>plot_decision_boundary(dec_tree_4, axis<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>5</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>scatter(X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>0</span>], X_test[y_test <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;classification accuracy when max depth is with no limitation: &#39;</span>, dec_tree<span style=color:#ff79c6>.</span>score(X_test, y_test))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;classification accuracy when max depth is 10: &#39;</span>, dec_tree_10<span style=color:#ff79c6>.</span>score(X_test, y_test))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;classification accuracy when max depth is 6: &#39;</span>, dec_tree_6<span style=color:#ff79c6>.</span>score(X_test, y_test))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;classification accuracy when max depth is 4: &#39;</span>, dec_tree_4<span style=color:#ff79c6>.</span>score(X_test, y_test))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>周志华. 机器学习. 北京: 清华大学出版社, 2016.</p></li><li><p>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</p></li><li><p>决策树维基百科：https://en.wikipedia.org/wiki/Decision_tree_learning</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/ data-toggle=tooltip data-placement=top title=机器学习：支持向量机>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ data-toggle=tooltip data-placement=top title=机器学习：集成学习>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>