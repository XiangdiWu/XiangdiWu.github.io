<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="量化数学基础：信息论"><meta property="og:title" content="量化数学基础：信息论"><meta property="twitter:title" content="量化数学基础：信息论"><meta name=description content="本文主要介绍信息论的相关概念，包括熵、自信息、互信息、KL散度、JS散度等。"><meta property="og:description" content="本文主要介绍信息论的相关概念，包括熵、自信息、互信息、KL散度、JS散度等。"><meta property="twitter:description" content="本文主要介绍信息论的相关概念，包括熵、自信息、互信息、KL散度、JS散度等。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>量化数学基础：信息论-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/math title=Math>Math</a></div><h1>量化数学基础：信息论</h1><h2 class=subheading>Foundations of Quantitative Mathematics: Information Theory</h2><span class=meta>Posted by
XiangdiWu
on
Sunday, September 27, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p><strong>信息论(information theory)</strong> 是数学、物理、计算机科学等多个学科的交叉领域。信息论是由Claude Shannon最早提出的，主要研究信息的量化、存储和通信等方法。这里，<strong>“信息”是指一组消息的集合</strong>。假设在一个噪声通道上发送消息，我们需要考虑如何对每一个信息进行编码、传输以及解码，使得接收者可以尽可能准确地重构出消息。在机器学习相关领域，信息论也有着大量的应用。比如<strong>特征抽取、统计推断、自然语言处理</strong>等。</p><h1 id=熵>熵</h1><p><strong>熵(entropy)</strong> 最早是物理学的概念，用于表示一个<strong>热力学系统的无序程度</strong>。在信息论中，熵用来衡量一个随机事件的不确定性。</p><h2 id=自信息和熵>自信息和熵</h2><p><strong>自信息(self information)</strong> 表示一个随机事件所包含的信息量。一个随机事件发生的概率越高，其自信息越低。如果一个事件必然发生，则其自信息为0。对于一个随机变量$X$(取值集合为$\mathcal X$，概率分布为$p(x)$)，当$X=x$时的自信息$I(x)$定义为：$I(x)=-\log p(x)$。在自信息的定义中，对数的底可以使用2、自然常数$e$或是10。当以2为底时，自信息的单位为bit；当以$e$为底时，自信息的单位为nat。</p><p>对于分布为$p(x)$的随机变量$X$，其<strong>自信息的数学期望</strong>，即<strong>熵(entropy)</strong>$H(X)$定义为：</p>$$
\begin{aligned}
H(X) &=\mathbb{E}_{X}[\mathrm{I}(x)] \\
&=\mathbb{E}_{X}[-\log p(x)] \\
&=-\sum_{x \in \mathcal{X}} p(x) \log p(x)
\end{aligned}
$$<p>熵越高，则随机变量的信息越多；熵越低，则随机变量的信息越少。如果随机变量$X$当且仅当在$x$时$p(x)=1$，则其熵为0。也就是说，对于一个确定的信息，其熵为0，信息量也为0.如果其概率分布为一个均匀分布，则熵最大。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算不同概率分布的熵</span>
</span></span><span style=display:flex><span>p1 <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0.1</span>, <span style=color:#bd93f9>0.3</span>, <span style=color:#bd93f9>0.6</span>]
</span></span><span style=display:flex><span>p2 <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0.33</span>, <span style=color:#bd93f9>0.33</span>, <span style=color:#bd93f9>0.34</span>]
</span></span><span style=display:flex><span>p3 <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0.0</span>, <span style=color:#bd93f9>0.0</span>, <span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>entropy</span>(p):
</span></span><span style=display:flex><span>    ent <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(p)):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> p[i] <span style=color:#ff79c6>!=</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>            ent <span style=color:#ff79c6>+=</span> p[i] <span style=color:#ff79c6>*</span> math<span style=color:#ff79c6>.</span>log2(p[i])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> ent <span style=color:#ff79c6>!=</span> <span style=color:#bd93f9>0.0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>-</span>ent
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#bd93f9>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(entropy(p1))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(entropy(p2))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(entropy(p3))
</span></span></code></pre></div><p>假设一个随机变量$X$有三种可能取值$x_1,x_2,x_3$，不同概率分布对应的熵如下：</p><div align=center><img src=/Kimages/1/image-20200526195633003.png style=zoom:30%></div><h2 id=熵编码>熵编码</h2><p>信息论的研究目标之一是如何用最少的编码表示传递信息。假设我们要传递一段文本信息，这段文本中包含的符号都来自于一个字母表$\mathcal A$，我们就需要对字母表$\mathcal A$中的每个符号进行编码。以二进制编码为例，<strong>我们常用的ASCII码就是用固定的8bits来编码每个字母</strong>。但这种固定长度的编码方案不是最优的。一种高效的编码原则是，<strong>字母的出现概率越高，其编码长度越短</strong>。比如对字母$a, b, c$分别编码为0*,* 10*,* 110。给定一串要传输的文本信息，其中字母$x$的出现概率为$p(x)$，其最佳编码长度为$-\log_2p(x)$，整段文本的平均编码长度为$-\sum_{x} p(x) \log _{2} p(x)$，即底为2的熵。</p><p>在对分布$p(x)$的符号进行编码时，熵$H(p)$也是理论上<strong>最优平均编码长度</strong>，这种编码方式称为<strong>熵编码(entropy encoding)</strong>。由于每个符号的自信息通常都不是整数，因此在实际编码中很难达到理论上的最优值。<strong>霍夫曼编码(Huffman coding)和算术编码(arithmetic coding)</strong> 是两种最常见的熵编码技术。</p><h2 id=联合熵和条件熵>联合熵和条件熵</h2><p>对于两个离散型随机变量$X$和$Y$，假设$X$的取值集合为$\mathcal X$；$Y$的取值集合为$\mathcal Y$，其联合概率分布为$p(x,y)$，则：</p><p>(1) $X$和$Y$的<strong>联合熵(joint entropy)</strong> 为：</p>$$
H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
$$<p>(2) $X$和$Y$的<strong>条件熵(conditional entropy)</strong> 为：</p>$$
H(X | Y)=-\sum_{x \in \mathcal{X} \atop y \in \mathcal{Y}} p(x, y) \log p(x | y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(y)}
$$<p>根据其定义，条件熵也可写为$H(X|Y)=H(X,Y)-H(Y)$。</p><h1 id=互信息>互信息</h1><p><strong>互信息(mutual information)</strong> 是衡量已知一个变量时，另一个变量<strong>不确定性的减少程度</strong>。两个离散型随机变量$X$和$Y$的互信息定义为：</p>$$
I(X ; Y)=\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$<p>互信息的一个性质如下：</p>$$
\begin{aligned}
I(X ; Y) &=H(X)-H(X | Y) \\
&=H(Y)-H(Y | X)
\end{aligned}
$$<p>如果变量$X$和$Y$相互独立，它们的互信息为0。</p><h1 id=交叉熵和散度>交叉熵和散度</h1><h2 id=交叉熵>交叉熵</h2><p>对于分布为$p(x)$的随机变量，熵$H(p)$表示其最有编码长度。<strong>交叉熵(cross entropy)</strong> 是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码的长度，定义为：</p>$$
\begin{aligned}
H(p, q) &=\mathbb{E}_{p}[-\log q(x)] \\
&=-\sum_{x} p(x) \log q(x)
\end{aligned}
$$<p>在给定$p$的条件下，如果$q$和$p$越接近，交叉熵越小；如果$q$和$p$越远，交叉熵就越大。交叉熵常用于分类问题的损失函数，其中$p$为数据的<strong>真实标签(真实分布)</strong>，$q$为模型的<strong>预测标签(预测分布)</strong>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 交叉熵的实现(交叉熵广泛应用于机器学习中的多分类任务)</span>
</span></span><span style=display:flex><span>p_label <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>1.0</span>, <span style=color:#bd93f9>0.0</span>, <span style=color:#bd93f9>0.0</span>, <span style=color:#bd93f9>0.0</span>, <span style=color:#bd93f9>0.0</span>]
</span></span><span style=display:flex><span>p_predict <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0.78</span>, <span style=color:#bd93f9>0.11</span>, <span style=color:#bd93f9>0.02</span>, <span style=color:#bd93f9>0.06</span>, <span style=color:#bd93f9>0.03</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>cross_entropy</span>(p, q):
</span></span><span style=display:flex><span>    ent <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>assert</span> <span style=color:#8be9fd;font-style:italic>len</span>(p) <span style=color:#ff79c6>==</span> <span style=color:#8be9fd;font-style:italic>len</span>(q)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(p)):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> q[i] <span style=color:#ff79c6>!=</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>            ent <span style=color:#ff79c6>+=</span> p[i] <span style=color:#ff79c6>*</span> math<span style=color:#ff79c6>.</span>log2(q[i])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> ent <span style=color:#ff79c6>!=</span> <span style=color:#bd93f9>0.0</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>-</span>ent
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#bd93f9>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(cross_entropy(p_label, p_predict))
</span></span></code></pre></div><h2 id=kl散度>KL散度</h2><p><strong>KL散度(Kullback-Leibler divergence)</strong>，也叫KL距离或相对熵(relative entropy)，是用概率分布$q$来近似$p$时所造成的的信息损失量。KL散度是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码，其平均编码长度(即交叉熵)$H(p,q)$和$p$的最优平均编码长度(即熵)$H(p)$之间的差异。对于离散概率分布$p$和$q$，从$p$到$q$的KL散度定义为：</p>$$
\begin{aligned}
D_{\mathrm{KL}}(p \| q) &=H(p, q)-H(p) \\
&=\sum_{x} p(x) \log \frac{p(x)}{q(x)}
\end{aligned}
$$<p>为了保证连续性，定义$0\log \frac{0}{0}=0,0\log \frac{0}{q}=0$。</p><p><strong>KL散度可以衡量两个概率分布之间的距离</strong>。KL散度总是非负的，如果两个分布越接近，则KL散度越小；如果两个分布越远，KL散度就越大。但是KL散度并不是一个真正的度量或距离，一是<strong>KL散度不满足距离的对称性</strong>，二是<strong>KL散度不满足距离的三角不等式性质</strong>。</p><h2 id=js散度>JS散度</h2><p><b>JS散度(Jensen-Shannon divergence)</b>是一种对称的的衡量两个分布的相似度的度量方式，其定义为：</p>$$
D_{\mathrm{JS}}(p \| q)=\frac{1}{2} D_{\mathrm{KL}}(p \| m)+\frac{1}{2} D_{K L}(q \| m)
$$<p>其中$m=\frac{1}{2}(p+q)$。</p><p>JS散度是KL散度的一种改进，但两种散度都存在一个问题，即如果两布$p,q$<strong>没有重叠或者重叠非常少时</strong>，KL散度和JS散度都<strong>很难衡量两个分布的距离</strong>。</p><h1 id=参考资料>参考资料</h1><ul><li><p>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</p></li><li><p>交叉熵详解：https://zhuanlan.zhihu.com/p/61944055</p></li><li><p>概念链(信息量→熵→KL散度→交叉熵)：https://blog.csdn.net/tyhj_sf/article/details/84933972</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/ data-toggle=tooltip data-placement=top title=量化数学基础：概率统计>&larr;
Previous Post</a></li><li class=next><a href=/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/ data-toggle=tooltip data-placement=top title=机器学习基础>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>