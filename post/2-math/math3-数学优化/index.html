<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/6256066/pexels-photo-6256066.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/6256066/pexels-photo-6256066.jpeg"><meta name=title content="量化数学基础：数学优化"><meta property="og:title" content="量化数学基础：数学优化"><meta property="twitter:title" content="量化数学基础：数学优化"><meta name=description content="本文主要介绍数学优化问题，包括数学优化问题的类型、凸优化基础、优化算法、拉格朗日乘数法与KKT条件。"><meta property="og:description" content="本文主要介绍数学优化问题，包括数学优化问题的类型、凸优化基础、优化算法、拉格朗日乘数法与KKT条件。"><meta property="twitter:description" content="本文主要介绍数学优化问题，包括数学优化问题的类型、凸优化基础、优化算法、拉格朗日乘数法与KKT条件。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>量化数学基础：数学优化-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/6256066/pexels-photo-6256066.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/math title=Math>Math</a></div><h1>量化数学基础：数学优化</h1><h2 class=subheading>Foundations of Quantitative Mathematics: Mathematical Optimisation</h2><span class=meta>Posted by
XiangdiWu
on
Friday, September 25, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p><b>数学优化(mathematical optimization)</b>问题也叫最优化问题，指在一定约束条件下，求解一个目标函数的最大值或最小值问题。数学优化问题的定义为：给定一个目标函数(也叫代价函数)$f:A\rightarrow\mathbb R$，寻找一个变量$\boldsymbol x^* \in \mathcal D$，使得对于所有$\mathcal D$中的$\boldsymbol x$，$f(\boldsymbol{x}^{*}) \leqslant f(\boldsymbol{x})$(最小化)；或者$f(\boldsymbol{x}^{*}) \geqslant f(\boldsymbol{x})$(最大化)，其中$\mathcal D$为变量$\boldsymbol x$的<strong>约束集</strong>，也叫<strong>可行域</strong>；$\mathcal D$中的变量被称为<strong>可行解</strong>。</p><h1 id=数学优化的类型>数学优化的类型</h1><h2 id=离散优化和连续优化>离散优化和连续优化</h2><p><b>离散优化(discrete optimization)</b>问题是目标函数的输入变量为离散变量，比如为整数或有限集合中的元素。离散优化问题主要有两个分支：</p><p>(1) <strong>组合优化(combinatorial optimization)</strong>：其目标是从一个有限集合中找出使得目标函数最优的元素。在一般的组合优化问题中，集合中的元素之间存在一定的关联，可以表示为图结构。典型的组合优化问题有旅行商问题、最小生成树问题、图着色问题等。很多机器学习问题都是组合优化问题，比如特征选择、聚类问题、超参数优化问题以及<b>结构化学习(structured learning)</b>中标签预测问题等。</p><p>(2) <strong>整数规划(integer programming)</strong>：输入变量$\boldsymbol x \in \mathbb Z^d$是一个整数向量。常见的整数规划问题通常为<b>整数线性规划</b>。整数线性规划的一种最直接的求解方法是：1. 去掉输入必须为整数的限制，将原问题转换为<strong>一般的线性规划问题</strong>，这个线性规划问题为原问题的<strong>松弛问题</strong>；2. 求得相应松弛问题的解；3. 把松弛问题的解<strong>四舍五入到最接近的整数</strong>。但是这种方法得到的解<strong>一般都不是最优的</strong>，因为原问题的最优解不一定在松弛问题最优解的附近。另外，这种方法得到的解也不一定满足约束条件。</p><p>离散优化问题的求解一般都比较困难，优化算法的复杂度都比较高。</p><p><b>连续优化(continuous optimization)</b>目标函数的输入变量为连续变量$\boldsymbol x \in \mathbb R^d$，即目标函数为实函数。机器学习中的优化问题主要是连续优化问题。</p><h2 id=无约束优化和约束优化>无约束优化和约束优化</h2><p>在连续优化问题中，根据是否有变量的约束条件，可以将优化问题分为无约束优化问题和约束优化问题。</p><p><b>无约束优化(unconstrained optimization)</b>问题的可行域为整个实数域$\mathcal D=\mathbb R^d $，可以写为：</p>$$
\min _{\boldsymbol{x}} \ \ f(\boldsymbol{x})
$$<p>其中$\boldsymbol x \in \mathbb R^d$为输入变量，$f:\mathbb R \rightarrow \mathbb R $为目标函数。</p><p><strong>约束优化(constrained optimization)</strong> 问题中变量$ \boldsymbol x $需要满足一些等式或不等式的约束。约束优化问题通常使用<strong>拉格朗日乘数法</strong>来进行求解。</p><h2 id=线性优化和非线性优化>线性优化和非线性优化</h2><p>若目标函数和所有的约束函数都为线性函数，则该问题称为<b>线性规划(linear programming)</b>问题。相反，如果木变函数或任何一个约束函数为非线性函数，则该问题为 <b>非线性规划(nonlinear programming)</b>问题。</p><p>在非线性优化问题中，有一类比较特殊的问题是<strong>凸优化(convex programming)<strong>问题。在凸优化问题中，变量$\boldsymbol x $的可行域为</strong>凸集</strong>，即对于集合中任意两点，它们的连线全部位于集合内部。目标函数$f$也必须为凸函数，即满足：</p>$$
f(\alpha \boldsymbol{x}+(1-\alpha) \boldsymbol{y}) \leq \alpha f(\boldsymbol{x})+(1-\alpha) f(\boldsymbol{y}), \ \ \forall \alpha \in[0,1]
$$<p>凸优化问题是一种特殊的约束优化问题，需满足目标函数为凸函数，并且等式约束函数为线性函数，不等式约束函数为凸函数。</p><h1 id=凸优化基础>凸优化基础</h1><h2 id=凸集>凸集</h2><p>对于集合$C$，如果对任意$x,y \in C,\theta \in \mathbb R,0 \leqslant \theta \leqslant 1$，有$\theta x + (1-\theta)y \in C$，则集合$C$为<strong>凸集(convex set)</strong>。下图中，左边为凸集，右边为非凸集：</p><div align=center><img src=/Kimages/1/image-20200604220501894.png style=zoom:20%></div><p>$\theta x+(1-\theta)y$称为点$x$和$y$的凸组合。凸集有以下实例：</p><div align=center><img src=/Kimages/1/image-20200608210123071.png style=zoom:35%></div><div align=center><img src=/Kimages/1/image-20200608210306814.png style=zoom:35%></div><div align=center><img src=/Kimages/1/image-20200608210354412.png style=zoom:35%></div><div align=center><img src=/Kimages/1/image-20200608210431723.png style=zoom:35%></div><h2 id=凸函数>凸函数</h2><p>若函数$f:\mathbb R^n \rightarrow \mathbb R$为<strong>凸函数(convex function)</strong>，则其定义域$\mathcal D(f)$为凸集，且对所有$x,y \in \mathcal D(f),\theta \in [0,1]$，有$f(\theta x+(1-\theta) y) \leqslant \theta f(x)+(1-\theta) f(y)$。</p><p>若函数$f$为凸函数，则$-f$为<strong>凹函数(concave function)</strong>。若上式中$x \not = y$且$\theta \in (0,1)$，则称函数$f$为<strong>严格凸</strong>的。</p><h2 id=jensen不等式>Jensen不等式</h2><p>将凸函数的定义推广到<strong>多个变量</strong>，可以得到下式：</p>$$
f\left(\sum_{i=1}^{k} \theta_{i} x_{i}\right) \leqslant \sum_{i=1}^{k} \theta_{i} f(x_{i}) \text { for } \sum_{i=1}^{k} \theta_{i}=1, \theta_{i} \geqslant 0 \ \ \forall i
$$<p>将上式再次推广为<strong>连续型变量</strong>，可以得到：</p>$$
f\left(\int p(x) x d x\right) \leqslant \int p(x) f(x) d x \text { for } \int p(x) dx=1, p(x) \geqslant 0 \ \ \forall x
$$<p>由于$p(x)$的积分为1，可以将其当做一个<strong>概率分布</strong>。因此上式可以写作<strong>期望</strong>的形式：</p>$$
f(\mathbb{E}[x]) \leqslant \mathbb{E}[f(x)]
$$<p>上式被称为<strong>Jensen不等式(Jensen inequality)</strong>。</p><h1 id=优化算法>优化算法</h1><p>优化问题一般都是通过<strong>迭代</strong>的方式来求解，即通过猜测一个初始的估计，然后不断迭代产生新的估计，最终收敛到期望的最优解。一个好的优化算法应该能在一定的时间或空间复杂度下能够快速准确地找到最优解。同时，好的优化算法<strong>受初始猜测点的影响较小</strong>，通过迭代能稳定地找到最优解的邻域，然后迅速收敛于最优解。</p><h2 id=全局最优和局部最优>全局最优和局部最优</h2><p>对于很多非线性优化问题，会存在若干个局部的极小值。局部极小值，或局部最优解$\boldsymbol x^*$定义为：存在一个$\delta>0$，对于所有的满足$\| \boldsymbol x - \boldsymbol x^* \| \leqslant \delta$的$\boldsymbol x$，公式$f(\boldsymbol X^*) \leqslant f(\boldsymbol X)$成立。也就是说，在 $\boldsymbol x^*$的附近区域内，所有的函数值都会大于或者等于$f(\boldsymbol x^*)$。</p><p>对于所有的$\boldsymbol x \in A$，都有$f(\boldsymbol X^*) \leqslant f(\boldsymbol X)$成立，则$\boldsymbol X^*$为全局最小值，或<strong>全局最优解</strong>。求局部最优解一般是比较容易的，但很难保证其为全局最优解。<strong>对于线性规划或凸优化问题，局部最优解就是全局最优解</strong>。</p><p>要确认一个点$\boldsymbol x^*$是否为局部最优解，通过比较它的淋浴内有没有更小的函数值是不现实的。如果函数$f(\boldsymbol x)$是二次连续可微的，我们可以通过检查目标函数在点$\boldsymbol x^*$的梯度$\nabla f\left(\boldsymbol{x}^{*}\right)$和Hessian矩阵$\nabla^2f(\boldsymbol x^*)$来判断。</p><p><strong>局部最优解的一阶必要条件</strong>如下：如果$\boldsymbol x^*$为局部最优解并且函数$f$在$\boldsymbol x^*$的邻域内一阶可微，则$\nabla f\left(\boldsymbol{x}^{*}\right)=0$ 。</p><p><strong>局部最优解的二阶必要条件</strong>如下：如果$\boldsymbol x^*$为局部最优解并且函数<em>f</em>在$\boldsymbol x^*$的邻域内二阶可微，则$\nabla f\left(\boldsymbol{x}^{*}\right)=0$，且$\nabla^2f(\boldsymbol x^*)$为半正定矩阵。</p><h2 id=梯度下降法>梯度下降法</h2><p><strong>梯度下降法(gradient descent method)<strong>经常用来求解无约束优化的极小值问题。对于函数$f(\boldsymbol x)$，如果$f(\boldsymbol x)$在点$\boldsymbol x_t$附近是连续可微的，那么$f(\boldsymbol x)$下降最快的方向是$f(\boldsymbol x)$在点$\boldsymbol x_t$梯度方向的</strong>反方向</strong>。</p><p>梯度下降法从一个初始值$\boldsymbol x_0$出发，通过迭代公式</p>$$\boldsymbol{x}_{t+1}=\boldsymbol{x}_{t}-\alpha_{t} \nabla f\left(\boldsymbol{x}_{t}\right), t \geqslant 0$$<p>生成序列$\boldsymbol x_0,\boldsymbol x_1,\cdots$，使得</p>$$
f(\boldsymbol{x}_{0}) \geqslant f(\boldsymbol{x}_{1}) \geqslant f(\boldsymbol{x}_{2}) \geqslant \cdots
$$<p>如果顺利的话，序列$(\boldsymbol x_n)$收敛到<strong>局部最优解</strong>$\boldsymbol x^*$。注意，每次迭代步长$\alpha$可以改变，但其值必须合适，如果过大就不会收敛，如果过小则收敛速度太慢。</p><p>梯度下降法的过程如下图所示：</p><div align=center><img src=/Kimages/1/image-20200525174138426.png style=zoom:35%></div><p>梯度下降法为一阶收敛算法，当靠近极小值时梯度变小，收敛速度会变慢，并且可能以“之字形”的方式下降。如果目标函数为二阶连续可微，我们可以采用<strong>牛顿法</strong>。牛顿法为二阶收敛算法，收敛速度更快，但是<strong>每次迭代需要计算Hessian矩阵的逆矩阵，复杂度较高</strong>。</p><p>相反，如果我们要求解一个最大值问题，就需要向梯度正方向迭代进行搜索，逐渐接近函数的局部极大值点，这个过程则被称为<strong>梯度上升法(gradient ascent method)</strong>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># numpy实现梯度下降算法逼近一个函数的最小值</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>linspace(<span style=color:#ff79c6>-</span><span style=color:#bd93f9>80</span>, <span style=color:#bd93f9>80</span>, <span style=color:#bd93f9>5000</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>f</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 目标函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> x <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>5</span> <span style=color:#ff79c6>*</span> x <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>df</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 目标函数的导数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> x <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>alpha <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.1</span>  <span style=color:#6272a4># 学习率</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>plot(x, f(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>init_x <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>76</span>  <span style=color:#6272a4># x的初始值</span>
</span></span><span style=display:flex><span>init_value <span style=color:#ff79c6>=</span> f(init_x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>last_x <span style=color:#ff79c6>=</span> init_x
</span></span><span style=display:flex><span>last_value <span style=color:#ff79c6>=</span> init_value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#ff79c6>=</span> [init_x]
</span></span><span style=display:flex><span>values <span style=color:#ff79c6>=</span> [init_value]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 10000次迭代</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>10000</span>):
</span></span><span style=display:flex><span>    now_x <span style=color:#ff79c6>=</span> last_x <span style=color:#ff79c6>-</span> alpha <span style=color:#ff79c6>*</span> df(last_x)
</span></span><span style=display:flex><span>    now_value <span style=color:#ff79c6>=</span> f(now_x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>abs</span>(now_value <span style=color:#ff79c6>-</span> last_value) <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>1e-8</span>:  <span style=color:#6272a4># 当某一步优化进行不明显时，终止优化</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    last_x <span style=color:#ff79c6>=</span> now_x
</span></span><span style=display:flex><span>    last_value <span style=color:#ff79c6>=</span> now_value
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;The </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>th step, x=</span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, f(x)=</span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, last_x, last_value))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    xs<span style=color:#ff79c6>.</span>append(last_x)
</span></span><span style=display:flex><span>    values<span style=color:#ff79c6>.</span>append(last_value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>plot(xs, values, color<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;red&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=拉格朗日乘数法与kkt条件>拉格朗日乘数法与KKT条件</h1><p><b>拉格朗日乘数法(Lagrange multiplier)</b>是约束优化问题的一种有效求解方法。<strong>约束优化问题</strong>可以表示为：</p>$$
\begin{aligned}
& \min_{\boldsymbol x} \ \ f(x)\\
& \text{subject to} \ \ h_i(\boldsymbol x)=0,i=1,\cdots,m;g_j(\boldsymbol x) \leqslant 0,j=1,\cdots,n
\end{aligned}
$$<p>其中$h_i(\boldsymbol x)$为等式约束函数，$g_j(\boldsymbol x)$为不等式约束函数。$\boldsymbol x$的可行域为：</p>$$
\mathcal{D}=\operatorname{dom}(f) \cap \bigcap_{i=1}^{m} \operatorname{dom}\left(h_{i}\right) \cap \bigcap_{j=1}^{n} \operatorname{dom}\left(g_{j}\right) \subseteq \mathbb{R}^{d}
$$<p>其中$\operatorname{dom}(f)$是函数$f$的定义域。</p><h2 id=等式约束优化问题>等式约束优化问题</h2><p>如果<strong>约束问题只有等式约束(而没有不等式约束)</strong>，我们可以构造一个拉格朗日函数$\Lambda(\boldsymbol{x}, \lambda)$：</p>$$
\Lambda(\boldsymbol{x}, \lambda)=f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} h_{i}(\boldsymbol{x})
$$<p>其中，$\lambda$为拉格朗日乘数，可以使正数或负数。如果$f(\boldsymbol x^*)$是原始约束优化问题的局部最优值，那么存在一个 $\lambda^* $，使得$(\boldsymbol x^* ,\lambda^* )$为拉格朗日函数$\Lambda(\boldsymbol{x}, \lambda)$的<strong>平稳点(stationary point)</strong>，即一阶偏导数为0的点。因此，只需要令${\partial \Lambda(\boldsymbol{x}, \lambda)}/{\partial \boldsymbol{x}}=0$和${\partial \Lambda(\boldsymbol{x}, \lambda)}/{\partial \lambda}=0$，得到：</p>$$
\nabla f(\boldsymbol{x})+\sum_{i=1}^{m} \lambda_{i} \nabla h_{i}(\boldsymbol{x})=0, \ \ h_i(\boldsymbol x)=0,i=0,\cdots,m
$$<p>上面方程组的解即为原始问题的可能解。在实际应用中，需根据问题来验证是否为极值点。</p><p>拉格朗日乘数法是将一个有$d$个<strong>变量</strong>和$m$个<strong>等式约束条件</strong>的最优化问题转换为一个有$d+m$个变量的函数求平稳点的问题。拉格朗日乘数法所得的平稳点会包含原问题的所有极值点，但<strong>并不保证每个平稳点都是原问题的极值点</strong>。</p><p>等式约束优化问题的解释如下：假设目标函数是二维的，即$f(x,y)$，下图蓝色虚线为目标函数的等高线图。对于凸的目标函数，<strong>只有当等高线与目标函数的曲线相切时才有可能得到可行解</strong>。因此在最优解处目标函数与约束函数$g(x,y)$相切，这时两者的法向量是平行的，即：$\nabla f(x,y)+\lambda \nabla g(x,y)=0$，与上文对应。</p><div align=center><img src=/Kimages/1/image-20200526164541279.png style=zoom:35%></div><h2 id=不等式约束优化问题>不等式约束优化问题</h2><p>对于一般的约束优化问题，其<strong>广义拉格朗日函数</strong>为：</p>$$
\Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b})=f(\boldsymbol{x})+\sum_{i=1}^{m} a_{i} h_{i}(\boldsymbol{x})+\sum_{j=1}^{n} b_{j} g_{j}(\boldsymbol{x})
$$<p>其中$h_i(x)$为第$i$个等式约束，$g_j(x)$为第$j$个不等式约束($\leqslant$)，$\boldsymbol a=[a_1,\cdots,a_m]^\text{T}$为等式约束的拉格朗日乘数，$\boldsymbol b=[b_1,\cdots,b_n]^\text{T}$为不等式约束的拉格朗日乘数，$b_i \geqslant 0$。</p><p>当<strong>约束条件不满足</strong>时，$\max _{\boldsymbol{a}, \boldsymbol{b}} \Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b})=\infty$，因为若某个$i$使约束$g_i(x) > 0$，则可令$b_i \rightarrow \infty$；若某个$j$对应的$h_j(x) \not = 0$，则可令$a_i$使得$a_ih_i(x) \rightarrow \infty$；当<strong>约束条件满足</strong>并且$\boldsymbol b \geqslant 0$时，$\max _{\boldsymbol{a}, \boldsymbol{b}} \Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b})=f(\boldsymbol{x})$。因此，<strong>原始约束优化问题等价于</strong>：</p>$$
\begin{aligned}
\min _{\boldsymbol{x}} \max _{\boldsymbol{a}, \boldsymbol{b}} \ \ \ & \Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b}) \\
\text { subject to} \ \ \ & \boldsymbol{b} \geqslant 0
\end{aligned}
$$<p>这个min-max优化问题称为<strong>主问题(primal problem)</strong>。</p><p>主问题的优化一般比较困难，我们可以通过交换min-max的顺序来简化。定义拉格朗日对偶函数为：</p>$$
\Gamma(\boldsymbol{a}, \boldsymbol{b})=\min _{\boldsymbol{x} \in \mathcal{D}} \Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b})
$$<p>$\Gamma(\boldsymbol{a}, \boldsymbol{b})$是一个凸函数，即使$f(\boldsymbol x)$是非凸的。当$\boldsymbol b \geqslant 0$时，对于任意的$\tilde{x} \in \mathcal{D}$，有：</p>$$
\Gamma(\boldsymbol{a}, \boldsymbol{b})=\min _{\boldsymbol{x} \in \mathcal{D}} \Lambda(\boldsymbol{x}, \boldsymbol{a}, \boldsymbol{b}) \leqslant \Lambda(\tilde{\boldsymbol{x}}, \boldsymbol{a}, \boldsymbol{b}) \leqslant f(\tilde{\boldsymbol{x}})
$$<p>令$p^* $是原问题的最优值，则有$\Gamma(\boldsymbol{a}, \boldsymbol{b}) \leqslant p^*$，即<strong>拉格朗日对偶函数为原问题最优值的下界</strong>。优化拉格朗日对偶函数$\Gamma(\boldsymbol{a}, \boldsymbol{b})$并得到原问题的最优下界，称为<strong>拉格朗日对偶问题(Lagrange dual problem)</strong>，即：</p>$$
\begin{aligned}
\max _{\boldsymbol{a}, \boldsymbol{b}} \ \ \ & \Gamma(\boldsymbol{a}, \boldsymbol{b}) \\
\text {subject to} \ \ \ & \boldsymbol{b} \geqslant 0
\end{aligned}
$$<p>拉格朗日对偶函数为凸函数，因此拉格朗日对偶问题为<b>凸优化问题</b>。</p><p>令$d^* $ 表示拉格朗日对偶问题的最优值，则有$d^* \leqslant p^*$，这个性质称为<b>弱对偶性(weak duality)</b>。如果$d^* = p^*$，这个性质称为<strong>强对偶性(strong duality)</strong>。</p><p>当强对偶性成立时，令$\boldsymbol x^* $和$\boldsymbol a^*, \boldsymbol b^* $分别是原问题和对偶问题的最优解，那么它们满足以下条件：</p>$$
\begin{aligned}
\nabla f\left(\boldsymbol{x}^{*}\right) +\sum_{i=1}^{m} a_{i}^{*} \nabla h_{i}(\boldsymbol{x}^{*})&+\sum_{j=1}^{n} b_{j}^{*} \nabla g_{j}(\boldsymbol{x}^{*})=0 \\
h_{i}(\boldsymbol{x}^{*})=0, \ \ & i=0, \cdots, m \\
g_{j}(\boldsymbol{x}^{*}) \leqslant 0, \ \ & j=0, \cdots, n \\
b_{j}^{*} g_{j}\left(\boldsymbol{x}^{*}\right)=0, \ \ & j=0, \cdots, n \\
b_{j}^{*} \geqslant 0, \ \ & j=0, \cdots, n
\end{aligned}
$$<p>称为不等式约束优化问题的<strong>KKT条件(Karush-Kuhn-Tucker condition)</strong>。KKT条件是拉格朗日乘数法在不等式约束优化问题上的泛化。当原问题是凸优化问题时，满足KKT条件的解也是原问题和对偶问题的最优解。</p><p>KKT条件中第四个式子称为<strong>互补松弛(complementary slackness)条件</strong>。如果最优解$\boldsymbol x^* $出现在不等式约束的边界上$g_j(\boldsymbol x)=0$，则$b_j^* >0$；如果最优解$\boldsymbol x$出现在不等式约束的内部$g_j(\boldsymbol x)<0$，则$b_j^*=0$。互补松弛条件说明<strong>当最优解出现在不等式约束的内部，则约束失败</strong>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># cvxpy解决凸优化问题</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> cvxpy <span style=color:#ff79c6>as</span> cvx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义要被优化的变量</span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> cvx<span style=color:#ff79c6>.</span>Variable()
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> cvx<span style=color:#ff79c6>.</span>Variable()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义约束，可以包含等式约束和不等式约束</span>
</span></span><span style=display:flex><span>constraints <span style=color:#ff79c6>=</span> [x <span style=color:#ff79c6>+</span> y <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>, x <span style=color:#ff79c6>-</span> y <span style=color:#ff79c6>&gt;=</span> <span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义优化目标</span>
</span></span><span style=display:flex><span>obj <span style=color:#ff79c6>=</span> cvx<span style=color:#ff79c6>.</span>Minimize((x <span style=color:#ff79c6>-</span> y) <span style=color:#ff79c6>**</span> <span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 将优化目标和约束组成优化问题</span>
</span></span><span style=display:flex><span>prob <span style=color:#ff79c6>=</span> cvx<span style=color:#ff79c6>.</span>Problem(obj, constraints)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 求解</span>
</span></span><span style=display:flex><span>prob<span style=color:#ff79c6>.</span>solve()
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;status: &#34;</span>, prob<span style=color:#ff79c6>.</span>status)  <span style=color:#6272a4># optimal</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;optimal value: &#34;</span>, prob<span style=color:#ff79c6>.</span>value)  <span style=color:#6272a4># 1.0</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;optimal variables: &#34;</span>, x<span style=color:#ff79c6>.</span>value, y<span style=color:#ff79c6>.</span>value)  <span style=color:#6272a4># 1.0 1.570086213240983e-22</span>
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>李航. 统计学习方法. 北京: 清华大学出版社, 2019.</p></li><li><p>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</p></li><li><p>Stanford University机器学习课程：http://cs229.stanford.edu/</p></li><li><p>cvxpy官方网站：https://www.cvxpy.org/</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/ data-toggle=tooltip data-placement=top title=量化数学基础：微积分>&larr;
Previous Post</a></li><li class=next><a href=/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/ data-toggle=tooltip data-placement=top title=量化数学基础：概率统计>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>