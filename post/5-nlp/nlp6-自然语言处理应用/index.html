<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="自然语言处理：自然语言处理应用"><meta property="og:title" content="自然语言处理：自然语言处理应用"><meta property="twitter:title" content="自然语言处理：自然语言处理应用"><meta name=description content="本文主要介绍了自然语言处理的应用，包括文本分类、文本摘要、机器阅读理解等。"><meta property="og:description" content="本文主要介绍了自然语言处理的应用，包括文本分类、文本摘要、机器阅读理解等。"><meta property="twitter:description" content="本文主要介绍了自然语言处理的应用，包括文本分类、文本摘要、机器阅读理解等。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>自然语言处理：自然语言处理应用-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/nlp title=NLP>NLP</a></div><h1>自然语言处理：自然语言处理应用</h1><h2 class=subheading>NLP-Natural Language Processing</h2><span class=meta>Posted by
XiangdiWu
on
Sunday, October 25, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p>自然语言处理的应用（即NLP任务）分布非常广泛。本章节以任务的形式进行划分，主要介绍三类具有代表性的自然语言处理应用，包括文本分类（序列-编码-类别）、文本摘要（序列-编码-解码-序列）、以及机器阅读理解（序列-编码-同步序列）。每个部分首先介绍了应用的概念及挑战，然后介绍了一些具有代表性的论文工作。<strong>注意，从接触一个领域的具体任务开始，已经初步进入到了对该科研领域的探索之中。每个领域都有一些具体的任务，而确定一个任务往往就是着手开展一项研究工作的第一步。</strong></p><p>本节介绍的相关工作均为大语言模型流行之前的工作范式。</p><h1 id=文本分类>文本分类</h1><p><strong>文本分类(text classification)</strong> 在自然语言处理领域有着广泛的应用，例如<strong>垃圾邮件检测(spam detection)</strong>、<strong>情感分析(sentiment analysis)</strong>、<strong>语言识别(language id)</strong> 以及<strong>新闻类别标注(news classification)</strong> 等。文本分类有着非常多的实现方法，每种实现方法中文本的表示也存在着很大的不同。但总的来说，文本分类是一个<strong>监督学习问题</strong>，需要数据样本以及其对应标签。</p><p>分类问题又可以分为<strong>二分类</strong>、<strong>多分类</strong>以及<strong>多标签分类</strong>等类型。其中的<strong>多标签分类</strong>指的是为每个数据标定数量不相同的标签，其由于类标签数量不确定、<strong>类标签之间可能有相互依赖</strong>、多标签的训练集比较难以获取等原因，难度是分类问题中最大的，因此也称为了分类问题的研究热点。目前主流的多标签分类的实现方式有如下几种：</p><p>(1) <strong>不考虑标签之间的关联性</strong>：将每个标签的出现与否视为二分类任务。</p><p>(2) <strong>考虑标签的关联性</strong>：分类器链、序列生成任务、通过<strong>标签共现(组合)</strong> 的方式转换为多分类任务。</p><p>对于基于神经网络的分类器，实现多标签分类，只需<strong>将输出层的softmax激活函数改为sigmoid激活函数</strong>，对每个类别进行二分类即可。在这个过程中，每个标签之间不是独立的，其关联性也会被神经网络学习到。</p><p>分本分类通常用<strong>准确率(accuracy)、召回率(recall)、精准率(precision)和F1(又分为micro和macro)</strong> 等指标进行评价。</p><h1 id=基于朴素贝叶斯算法的文本分类>基于朴素贝叶斯算法的文本分类</h1><p>朴素贝叶斯算法是一种生成模型，其实现简单，常常用于<strong>文本分类的baseline</strong>。在朴素贝叶斯算法中，文本常用词袋模型进行表示，即每个文档表示为一个大小为$|V|$的向量，其中$|V|$为词汇表的大小，每一个维度表示对应下标的单词在文档中出现的次数：</p><div align=center><img src=/Kimages/4/image-20200417082234463.png style=zoom:35%></div><p>朴素贝叶斯分类器是一个概率模型，其理论基础是贝叶斯定理，“朴素”一词指的是<strong>条件独立性假设</strong>。对于文档$d$，朴素贝叶斯算法预测使得后验概率$P(c|d)$最大的类别$c\in C$。<strong>朴素贝叶斯算法的过程如下所示</strong>：</p><div align=center><img src=/Kimages/4/image-20200417082740628.png style=zoom:40%></div><h1 id=基于卷积神经网络的文本分类>基于卷积神经网络的文本分类</h1><p>与朴素贝叶斯算法等这类传统机器学习方法不同，使用卷积神经网络(CNN)进行文本分类需要将词使用<strong>词向量(例如Word2Vec)</strong> 进行表示。CNN最初是针对图像设计的，其利用卷积核(convolving filters)来提取图像的局部特征。然而CNN也可以用于文本的特征提取，其通常被称为<strong>TextCNN</strong>。</p><p>Yoon Kim在论文《Convolutional Neural Networks for Sentence Classification》中提出了如下CNN网络结构用于文本分类任务：</p><div align=center><img src=/Kimages/4/image-20200417084853983.png style=zoom:30%></div><p>整个网络仅使用了一个卷积层、一个池化层和一个全连接层。输入层为词向量，其可以一开始将词向量矩阵随机初始化，并在训练的过程中进行学习(作为实验的baseline)，也可以一直<strong>保持静态</strong>(即使用其他语料训练完成后便不再改变)，还可以预训练后在该网络训练的过程中进行<strong>微调</strong>(在实验中能够取得更好的效果)。该论文还将输入设置为两个通道，其均为词向量，但<strong>其中一个通道保持静态，另一个通道可以在训练过程汇中进行微调</strong>。</p><p>另一篇关于TextCNN的论文提出了如下所示的网络结构：</p><div align=center><img src=/Kimages/4/image-20200417095418695.png style=zoom:50%></div><p>该论文进行了<strong>大量的实验</strong>来优化TextCNN中的参数，其中比较重要的一些<strong>结论和建议</strong>如下：</p><p>(1) 预训练word2vec或GloVe效果好于one-hot编码形式。</p><p>(2) 卷积核大小对实验结果有较大影响，一般取1~10，文本越长，可设置卷积核大小越大。</p><p>(3) 卷积核的数量对实验结果有较大的影响，一般取100<del>600 ，一般使用Dropout比率为0</del>0.5。</p><p>(4) 选用ReLU和tanh作为激活函数优于其他的激活函数。</p><p>(5) 1-max pooling在该实验中优于其他pooling策略。</p><p>(6) 正则化对实验结果有相对小的影响。</p><p>(7)当评估一个模型的性能时，考虑模型的方差是必要的。因此，评估模型时应当使用交叉验证，同时考虑模型每次结果的方差以及范围。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># Tensorflow实现用于多标签文本分类的TextCNN结构</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>TextCNN</span>(seq_len, token_num, embed_dim, out_dim, model_img_path<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, embed_matrix<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    TextCNN模型
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    1. embedding layers
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    2. convolution layer
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    3. max-pooling
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    4. softmax layer
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x_input <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Input(shape<span style=color:#ff79c6>=</span>(max_seq_len,))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> embedd_matrix <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        x_emb <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(input_dim<span style=color:#ff79c6>=</span>token_num,
</span></span><span style=display:flex><span>                                          output_dim<span style=color:#ff79c6>=</span>embed_dim,
</span></span><span style=display:flex><span>                                          input_length<span style=color:#ff79c6>=</span>seq_len)(x_input)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:  <span style=color:#6272a4># 加载预训练好的词向量矩阵</span>
</span></span><span style=display:flex><span>        x_emb <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(input_dim<span style=color:#ff79c6>=</span>token_num,
</span></span><span style=display:flex><span>                                          output_dim<span style=color:#ff79c6>=</span>embed_dim,
</span></span><span style=display:flex><span>                                          input_length<span style=color:#ff79c6>=</span>seq_len,
</span></span><span style=display:flex><span>                                          weights<span style=color:#ff79c6>=</span>[embed_matrix],
</span></span><span style=display:flex><span>                                          trainable<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)(x_input)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># x_embed shape: (batch_size, seq_len, token_num)</span>
</span></span><span style=display:flex><span>    pool_output <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    kernel_sizes <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>4</span>]  <span style=color:#6272a4># 设置多种卷积核</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> kernel_size <span style=color:#ff79c6>in</span> kernel_sizes:
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Conv1D input shape: 形如(samples, steps, input_dim)的3D张量</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Conv1D output shape: 形如(samples，new_steps，nb_filter)的3D张量</span>
</span></span><span style=display:flex><span>        c <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>kerasConv1D(filters<span style=color:#ff79c6>=</span><span style=color:#bd93f9>100</span>, kernel_size<span style=color:#ff79c6>=</span>kernel_size, strides<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)(x_emb)
</span></span><span style=display:flex><span>        p <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>MaxPool1D(pool_size<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>int</span>(c<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>1</span>]))(c)
</span></span><span style=display:flex><span>        pool_output<span style=color:#ff79c6>.</span>append(p)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pool_output <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>concatenate([p <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> pool_output])  <span style=color:#6272a4># 将多个池化结果进行拼接</span>
</span></span><span style=display:flex><span>    x_flatten <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Flatten()(pool_output)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># output_dim个sigmoid输出，是多标签分类的实现方式，实质上是对每个标签的二分类</span>
</span></span><span style=display:flex><span>    y <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(output_dim, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;sigmoid&#39;</span>)(x_flatten)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Model([x_input], outputs<span style=color:#ff79c6>=</span>[y])
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> model
</span></span></code></pre></div><h1 id=基于fasttext的文本分类>基于FastText的文本分类</h1><p>Armand Joulin等人提出了如下FastText模型，可用于快速的文本分类， 并且准确度接近state-of-art模型。</p><div align=center><img src=/Kimages/4/image-20200423180413505.png style=zoom:30%></div><p>其中$x_1,x_2,\cdots,x_N$是一个句子中的$N$个特征，可以是词向量，也可以是<strong>N-gram特征</strong>。这些词的特征表示(word representation)在隐含层(hidden)被平均为<strong>句子的特征表示(sentence representation)</strong>，然后被送入一个线性分类器。当输出空间很大时，论文使用<strong>层次化softmax(hierarchical softmax)</strong> 对结果类别进行计算以减少计算复杂度。对于一个由$N$个文档祖晨的集合，该模型的目标是最小化如下所示的<strong>损失函数(或负对数似然函数)</strong>：</p>$$
-\frac{1}{N}\sum_{n=1}^{N}y_n\log(f(BAx_n))
$$<p>其中，$A$是一个look-up表，用于将输入特征$x_n$映射到其对应的嵌入向量；$B$是隐含层到输出层的参数矩阵，$f$是softmax激活函数。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 使用Tensorflow序列方式构建FastText文本多分类模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>build_fasttext_model</span>(vocab, embed, length, num_hidden, num_classes):
</span></span><span style=display:flex><span>    fasttext <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Sequential()
</span></span><span style=display:flex><span>    fasttext<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(input_dim<span style=color:#ff79c6>=</span>vocab,
</span></span><span style=display:flex><span>                                           output_dim<span style=color:#ff79c6>=</span>embed,
</span></span><span style=display:flex><span>                                           input_length<span style=color:#ff79c6>=</span>length,
</span></span><span style=display:flex><span>                                           mask_zero<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>))
</span></span><span style=display:flex><span>    fasttext<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>GlobalAveragePooling1D())
</span></span><span style=display:flex><span>    fasttext<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(num_hidden, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>))
</span></span><span style=display:flex><span>    fasttext<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(num_classes, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span>    fasttext<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> fasttext
</span></span></code></pre></div><h1 id=基于循环神经网络的文本分类>基于循环神经网络的文本分类</h1><p>使用循环神经网络(recurrent neural network, RNN)进行文本分类时，通常每个<strong>时间步(time step)</strong> 的输入是文本中每个单词的词向量，将最后一个单词对应的RNN的输出通过全连接层+softmax的形式映射到类别的概率分布：</p><div align=center><img src=/Kimages/4/image-20200508142405388.png style=zoom:30%></div><p>还可以使用<strong>双向结构</strong>，能够在一定程度上提升分类效果。除了将最后时刻的状态作为<strong>序列表示</strong>之外，我们还可以<strong>对整个序列的所有状态进行平均</strong>，并用这个平均状态来作为整个序列的表示：</p><div align=center><img src=/Kimages/4/image-20200508142535364.png style=zoom:30%></div><p>本质上，RNN是在产生一个<strong>句嵌入(sentence smbeeding)</strong>，然后使用全连接+softmax的方式对该句嵌入进行分类。除了单向RNN和双向RNN以外，还可以<strong>利用每个时间步的输出组合成一个新的向量</strong>作为句嵌入。除了原始的RNN以外，还可以将每一层替换为LSTM或者GRU。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 使用Tensorflow序列方式构建RNN文本多分类模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>build_rnn_model</span>(vocab, embed, length, num_hidden, num_classes):
</span></span><span style=display:flex><span>    rnn <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Sequential()
</span></span><span style=display:flex><span>    rnn<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(input_dim<span style=color:#ff79c6>=</span>vocab,
</span></span><span style=display:flex><span>                                      output_dim<span style=color:#ff79c6>=</span>embed,
</span></span><span style=display:flex><span>                                      input_length<span style=color:#ff79c6>=</span>length,
</span></span><span style=display:flex><span>                                      mask_zero<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>))
</span></span><span style=display:flex><span>    rnn<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>SimpleRNN(num_hidden, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>))  <span style=color:#6272a4># 可替换为LSTM/GRU</span>
</span></span><span style=display:flex><span>    rnn<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(num_hidden, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>))
</span></span><span style=display:flex><span>    rnn<span style=color:#ff79c6>.</span>add(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(num_classes, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span>    rnn<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> rnn
</span></span></code></pre></div><h1 id=基于预训练语言模型的文本分类>基于预训练语言模型的文本分类</h1><p>基于Transformer encoder的语言模型(如BERT)非常适合完成文本分类任务。以BERT为例，按照分类的对象，可以有<strong>句子对分类任务(sentence pair classification)、单个句子分类任务(single sentence classification)以及标记分类(token classification)任务</strong>等，其分别作用于不同场景下：句子对分类任务可以解决语义匹配问题，单个句子分类任务可以解决情感分析、垃圾邮件识别等问题，而标记分类任务可以实现<strong>命名实体识别(named entity recognition)</strong> 等任务。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调transformers中的BERT模型进行中文文本分类(新闻标题分类)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> BertTokenizer, TFBertForSequenceClassification
</span></span><span style=display:flex><span>os<span style=color:#ff79c6>.</span>environ[<span style=color:#f1fa8c>&#39;CUDA_VISIBLE_DEVICES&#39;</span>] <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;0&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 设置模型和文件路径，运行前需要将模型和文件导入当前文件夹</span>
</span></span><span style=display:flex><span>bert_model_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert-chinese&#39;</span>
</span></span><span style=display:flex><span>bert_fine_tuned_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert-cls&#39;</span>
</span></span><span style=display:flex><span>train_data_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert_classification_train.txt&#39;</span>
</span></span><span style=display:flex><span>dev_data_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert_classification_dev.txt&#39;</span>
</span></span><span style=display:flex><span>test_data_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;bert_classification_test.txt&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gpus <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>experimental<span style=color:#ff79c6>.</span>list_physical_devices(<span style=color:#f1fa8c>&#39;GPU&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(gpus)
</span></span><span style=display:flex><span><span style=color:#6272a4># 设置按需申请，multi-GPU需要for一下</span>
</span></span><span style=display:flex><span>tf<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>experimental<span style=color:#ff79c6>.</span>set_memory_growth(gpus[<span style=color:#bd93f9>0</span>], <span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># mapping</span>
</span></span><span style=display:flex><span>y_map <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#39;财经&#39;</span>: <span style=color:#bd93f9>0</span>, <span style=color:#f1fa8c>&#39;彩票&#39;</span>: <span style=color:#bd93f9>1</span>, <span style=color:#f1fa8c>&#39;房产&#39;</span>: <span style=color:#bd93f9>2</span>, <span style=color:#f1fa8c>&#39;股票&#39;</span>: <span style=color:#bd93f9>3</span>, <span style=color:#f1fa8c>&#39;家居&#39;</span>: <span style=color:#bd93f9>4</span>, <span style=color:#f1fa8c>&#39;教育&#39;</span>: <span style=color:#bd93f9>5</span>, <span style=color:#f1fa8c>&#39;科技&#39;</span>: <span style=color:#bd93f9>6</span>, 
</span></span><span style=display:flex><span>         <span style=color:#f1fa8c>&#39;社会&#39;</span>: <span style=color:#bd93f9>7</span>, <span style=color:#f1fa8c>&#39;时尚&#39;</span>: <span style=color:#bd93f9>8</span>, <span style=color:#f1fa8c>&#39;时政&#39;</span>: <span style=color:#bd93f9>9</span>, <span style=color:#f1fa8c>&#39;体育&#39;</span>: <span style=color:#bd93f9>10</span>, <span style=color:#f1fa8c>&#39;星座&#39;</span>: <span style=color:#bd93f9>11</span>, <span style=color:#f1fa8c>&#39;游戏&#39;</span>: <span style=color:#bd93f9>12</span>, <span style=color:#f1fa8c>&#39;娱乐&#39;</span>: <span style=color:#bd93f9>13</span>}
</span></span><span style=display:flex><span>y_map_reverse <span style=color:#ff79c6>=</span> {y_map[k] : k <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> y_map<span style=color:#ff79c6>.</span>keys()}
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(y_map, y_map_reverse)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>load_model</span>(bert_model_path):
</span></span><span style=display:flex><span>    tokenizer <span style=color:#ff79c6>=</span> BertTokenizer<span style=color:#ff79c6>.</span>from_pretrained(bert_model_path)
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> TFBertForSequenceClassification<span style=color:#ff79c6>.</span>from_pretrained(bert_model_path, num_labels<span style=color:#ff79c6>=</span><span style=color:#bd93f9>14</span>)  <span style=color:#6272a4># 类别数目</span>
</span></span><span style=display:flex><span>    tokenizer<span style=color:#ff79c6>.</span>pad_token <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;[PAD]&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tokenizer, model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>build_dataset</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 读取文件</span>
</span></span><span style=display:flex><span>    X_train, X_dev, y_train, y_dev <span style=color:#ff79c6>=</span> [], [], [], []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(train_data_path, <span style=color:#f1fa8c>&#39;r&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>        lines <span style=color:#ff79c6>=</span> f<span style=color:#ff79c6>.</span>read()<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(lines)):
</span></span><span style=display:flex><span>        text, label <span style=color:#ff79c6>=</span> lines[i]<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>        X_train<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>        y_train<span style=color:#ff79c6>.</span>append(y_map[label])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(dev_data_path, <span style=color:#f1fa8c>&#39;r&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>        lines <span style=color:#ff79c6>=</span> f<span style=color:#ff79c6>.</span>read()<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(lines)):
</span></span><span style=display:flex><span>        text, label <span style=color:#ff79c6>=</span> lines[i]<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>        X_dev<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>        y_dev<span style=color:#ff79c6>.</span>append(y_map[label])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> X_train, X_dev, y_train, y_dev
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fine_tuning</span>(bert_model_path, X_train, y_train, X_dev, y_dev, model_save_path, epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>, batch_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>64</span>):
</span></span><span style=display:flex><span>    tokenizer, model <span style=color:#ff79c6>=</span> load_model(bert_model_path)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#8be9fd;font-style:italic>len</span>(X_train), <span style=color:#8be9fd;font-style:italic>len</span>(y_train), <span style=color:#8be9fd;font-style:italic>len</span>(X_dev), <span style=color:#8be9fd;font-style:italic>len</span>(y_dev))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;encoding train set and dev set by tokenizer&#39;</span>)
</span></span><span style=display:flex><span>    train_encodings <span style=color:#ff79c6>=</span> tokenizer(X_train, truncation<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, padding<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, max_length<span style=color:#ff79c6>=</span><span style=color:#bd93f9>32</span>, return_tensors<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>    dev_encodings <span style=color:#ff79c6>=</span> tokenizer(X_dev, truncation<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, padding<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, max_length<span style=color:#ff79c6>=</span><span style=color:#bd93f9>32</span>, return_tensors<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(train_encodings<span style=color:#ff79c6>.</span>keys())  <span style=color:#6272a4># [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_tensor_inputs <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(train_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>    train_tensor_labels <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(y_train)
</span></span><span style=display:flex><span>    train_tensor_mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(train_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>])
</span></span><span style=display:flex><span>    train_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            {<span style=color:#f1fa8c>&#39;input_ids&#39;</span>: train_tensor_inputs, <span style=color:#f1fa8c>&#39;attention_mask&#39;</span>: train_tensor_mask},
</span></span><span style=display:flex><span>            train_tensor_labels,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dev_tensor_inputs <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(dev_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>    dev_tensor_labels <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(y_dev)
</span></span><span style=display:flex><span>    dev_tensor_mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(dev_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>])
</span></span><span style=display:flex><span>    dev_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            {<span style=color:#f1fa8c>&#39;input_ids&#39;</span>: dev_tensor_inputs, <span style=color:#f1fa8c>&#39;attention_mask&#39;</span>: dev_tensor_mask},
</span></span><span style=display:flex><span>            dev_tensor_labels,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_ds <span style=color:#ff79c6>=</span> train_ds<span style=color:#ff79c6>.</span>shuffle(<span style=color:#8be9fd;font-style:italic>len</span>(X_train))<span style=color:#ff79c6>.</span>batch(batch_size, drop_remainder<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    dev_ds <span style=color:#ff79c6>=</span> dev_ds<span style=color:#ff79c6>.</span>batch(batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(train_ds)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(dev_ds)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam(learning_rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3e-5</span>, epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-08</span>, clipnorm<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1.0</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>SparseCategoricalCrossentropy(from_logits<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>compile(optimizer<span style=color:#ff79c6>=</span>optimizer, loss<span style=color:#ff79c6>=</span>loss, metrics<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># train and save the model</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(epochs):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;training epoch&#39;</span>, i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        hist <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>fit(train_ds, epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>, validation_data<span style=color:#ff79c6>=</span>dev_ds)
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>.</span>save_pretrained(model_save_path <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_&#39;</span> <span style=color:#ff79c6>+</span> <span style=color:#8be9fd;font-style:italic>str</span>(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>) <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_epoch&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>inference</span>(bert_model_path, result_path):
</span></span><span style=display:flex><span>    tokenizer, model <span style=color:#ff79c6>=</span> load_model(bert_model_path)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(test_data_path, <span style=color:#f1fa8c>&#39;r&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>        test_data <span style=color:#ff79c6>=</span> f<span style=color:#ff79c6>.</span>read()<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)[:<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#8be9fd;font-style:italic>len</span>(test_data))
</span></span><span style=display:flex><span>    test_encodings <span style=color:#ff79c6>=</span> tokenizer(test_data, truncation<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, padding<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, max_length<span style=color:#ff79c6>=</span><span style=color:#bd93f9>32</span>, return_tensors<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tf&#39;</span>)
</span></span><span style=display:flex><span>    test_tensor_inputs <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(test_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>    test_tensor_mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(test_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>])
</span></span><span style=display:flex><span>    test_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices(({<span style=color:#f1fa8c>&#39;input_ids&#39;</span>: test_tensor_inputs, <span style=color:#f1fa8c>&#39;attention_mask&#39;</span>: test_tensor_mask}))
</span></span><span style=display:flex><span>    test_ds <span style=color:#ff79c6>=</span> test_ds<span style=color:#ff79c6>.</span>batch(<span style=color:#bd93f9>128</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(test_ds)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(result_path, <span style=color:#f1fa8c>&#39;a&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i, batch_data <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(test_ds):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> i <span style=color:#ff79c6>%</span> <span style=color:#bd93f9>10</span> <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#8be9fd;font-style:italic>print</span>(i)
</span></span><span style=display:flex><span>            out_dist <span style=color:#ff79c6>=</span> model(batch_data)[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>            out_softmax <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>nn<span style=color:#ff79c6>.</span>softmax(out_dist, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>            preds <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>argmax(out_softmax, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)<span style=color:#ff79c6>.</span>numpy())
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> pred <span style=color:#ff79c6>in</span> preds:
</span></span><span style=display:flex><span>                label <span style=color:#ff79c6>=</span> y_map_reverse[pred]
</span></span><span style=display:flex><span>                f<span style=color:#ff79c6>.</span>write(label <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>__name__</span> <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    X_train, X_dev, y_train, y_dev <span style=color:#ff79c6>=</span> build_dataset()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># fine_tuning(bert_model_path, X_train, y_train, X_dev, y_dev, bert_fine_tuned_path)</span>
</span></span><span style=display:flex><span>    inference(bert_fine_tuned_path <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_3_epoch&#39;</span>, <span style=color:#f1fa8c>&#39;result.txt&#39;</span>)
</span></span></code></pre></div><h1 id=文本摘要>文本摘要</h1><p>随着互联网产生的文本数据越来越多，文本信息过载问题日益严重，对各类文本进行一个“降 维”处理显得非常必要，<strong>文本摘要(text summarization)<strong>便是其中一个重要的手段。文本摘要旨在</strong>将文本或文本集合转换为包含关键信息的简短摘要</strong>。文本摘要按照<strong>输入类型</strong>可分为<strong>单文档摘要和多文档摘要</strong>。单文档摘要从给定的一个文档中生成摘要，多文档摘要从给定的一组主题相关的文档中生成摘要。按照<strong>输出类型</strong>可分为<strong>抽取式摘要和生成式摘要</strong>。抽取式摘要从源文档中抽取关键句和关键词组成摘要，<strong>摘要全部来源于原文</strong>；生成式摘要根据原文，允许<strong>生成新的词语、短语来组成摘要</strong>。按照有无监督数据可以分为<strong>有监督摘要和无监督摘要</strong>。</p><p>常用的<strong>文本摘要数据集</strong>有DUC数据集、New York Times数据集、CNN / Daily Mail数据集、 Gigaword数据集、LCSTS数据集等。<strong>文本摘要的结果通常使用ROUGE指标及其变体进行评价</strong>。</p><h1 id=textrank抽提式文本摘要>TextRank抽提式文本摘要</h1><p>TextRank是一种<strong>基于PageRank</strong>的<strong>无监督文本摘要</strong>算法。在TextRank算法中，句子等价于网页，使用任意两个句子的相似性等价于网页转换概率，将相似性得分存储在一个矩阵中，类似于PageRank中的概率转移矩阵。</p><p>TextRank算法步骤如下：</p><p>(1) 把所有文章整合成<strong>文本数据</strong>；</p><p>(2) 接下来把文本分割成<strong>单个句子</strong>；</p><p>(3) 将为每个句子找到向量表示，通常是先使用word2vec预训练每个单词的词向量，然后<strong>将一句话中的所有单词的word2vec进行平均</strong>，从而得到一句话的向量表示；</p><p>(4) 计算句子向量间的<strong>相似性(通常使用余弦相似度)</strong> 并存放在矩阵中；</p><p>(5) 将相似矩阵转换为以句子为节点、相似性得分为边的<strong>图结构</strong>，用于句子TextRank计算；</p><p>(6) 最后，<strong>将一定数量的排名最高的句子构成最后的摘要</strong>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 利用textrank4zh中的TextRank4Sentence实现中文抽提式文本摘要</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> textrank4zh <span style=color:#ff79c6>import</span> TextRank4Sentence
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># results用于保存摘要结果</span>
</span></span><span style=display:flex><span>tr4s <span style=color:#ff79c6>=</span> TextRank4Sentence()  <span style=color:#6272a4># TextRank4Sentence对象</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(texts)):  <span style=color:#6272a4># texts为文档集合</span>
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> texts[i]
</span></span><span style=display:flex><span>    text_divide <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;[。]&#39;</span>, text)  <span style=color:#6272a4># 每个文档可以切分为多个句子，tr4s模型负责选出重要的句子完成摘要</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    sort_dict <span style=color:#ff79c6>=</span> {j: text_divide[j] <span style=color:#ff79c6>for</span> j <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(text_divide))}  <span style=color:#6272a4># 句子按照原顺序排序的字典(次序: 句子)</span>
</span></span><span style=display:flex><span>    reverse_sort_dict <span style=color:#ff79c6>=</span> {text_divide[j]: j <span style=color:#ff79c6>for</span> j <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(text_divide))}  <span style=color:#6272a4># 反向排序字典(句子: 次序)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 利用tr4s完成摘要</span>
</span></span><span style=display:flex><span>    tr4s<span style=color:#ff79c6>.</span>analyze(text<span style=color:#ff79c6>=</span>text, lower<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, source<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;all_filters&#39;</span>)
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> tr4s<span style=color:#ff79c6>.</span>get_key_sentences(num<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>, sentence_min_len<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>)  <span style=color:#6272a4># 选取长度大于等于3的最重要的3条句子</span>
</span></span><span style=display:flex><span>    result_list <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> item <span style=color:#ff79c6>in</span> result:
</span></span><span style=display:flex><span>        result_list<span style=color:#ff79c6>.</span>append(item<span style=color:#ff79c6>.</span>sentence)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 对句子顺寻按照原文档顺序重新排序</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> j <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(result_list)):
</span></span><span style=display:flex><span>        result_list[j] <span style=color:#ff79c6>=</span> reverse_sort_dict[result_list[j]]  <span style=color:#6272a4># 变成顺序下标</span>
</span></span><span style=display:flex><span>    result_list<span style=color:#ff79c6>.</span>sort()  <span style=color:#6272a4># 对顺序下标进行排序</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> j <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(result_list)):
</span></span><span style=display:flex><span>        result_list[j] <span style=color:#ff79c6>=</span> sort_dict[result_list[j]]  <span style=color:#6272a4># 利用词典恢复成句子</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results<span style=color:#ff79c6>.</span>append(result_list)
</span></span></code></pre></div><h1 id=seq2seq与attention生成式文本摘要>seq2seq与attention生成式文本摘要</h1><p>基于seq2seq+attention结构的生成式文本摘要示意图如下：</p><div align=center><img src=/Kimages/4/image-20200803110440798.png style=zoom:33%></div><p>原始文本(source text)中的单词$w_i$被逐步输入一个编码器(论文中使用的是单层双向LSTM)，产生一个编码器隐藏层序列$h_i$，其长度与原始文本长度相同。在解码时的第$t$个时间步中，解码器接受前一个单词的词向量(在训练时是参考摘要的词向量，在测试阶段是decoder上一步输出的词向量)，并产生解码器隐含状态$s_t$。<strong>注意力分布</strong>可使用如下公式计算得出：</p>$$
\begin{array}{l}
e_{i}^{t}=v^{T} \tanh (W_{h} h_{i}+W_{s} s_{t}+b_{\mathrm{attn}})\\
a^{t}=\operatorname{softmax}(e^{t})
\end{array}
$$<p>然后，注意力分布用于计算上下文向量$h_t^*$：</p>$$
h_t^*=\sum_i a_i^t h_i
$$<p>上下文向量$h_t^*$可以看作是<strong>从源文本读取的内容的固定大小表示形式</strong>，它与解码器状态$s_t$串联在一起，并通过两个线性层以产生词汇分布$P_{\text{vocab}}$：</p>$$
P_{\text{vocab}}=\operatorname{softmax}(V^{\prime}(V[s_{t}, h_{t}^{*}]+b)+b^{\prime})
$$<p>最终预测单词的概率分布就等于在词表上所计算得到的概率分布$P_{\text{vocab}}$，即$P(w)=P_{\text{vocab}}$。</p><p>在训练过程中，时间步$t$的损失为目标单词$w_t^*$的负对数似然，即$\text{loss}_t=-\log P(w_t^*)$，整个句子的损失为：</p>$$
\operatorname{loss}=\frac{1}{T} \sum_{t=0}^{T} \operatorname{loss}_{t}
$$<p>使用如上结构进行文本摘要时，存在<strong>三个问题</strong>：(1) <strong>对事实细节的错误描述(例如2-0变为2-1)</strong>；(2) <strong>无法处理未知词汇(OOV)问题</strong>；(3) <strong>容易产生重复(repetition)</strong>。</p><h1 id=pgn生成式文本摘要>PGN生成式文本摘要</h1><p><strong>PGN(pointer-generator network)</strong> 在seq2seq+attention结构的基础上进行两个改进：(1) 引入<strong>指针网络</strong>，从原文中复制部分单词，从而<strong>提高摘要准确性，并解决OOV问题</strong>；(2) 利用<strong>coverage机制</strong>减少重复。</p><p>PGN的结构如下图所示：</p><div align=center><img src=/Kimages/4/image-20200803113112581.png style=zoom:33%></div><p>在PGN中，注意力分布$a^t$和上下文向量$h_t^*$与原始seq2seq+attention结构的计算方式一致。此外，定义在时间步$t$的<strong>生成概率</strong>$p_{\text{gen}}$：</p>$$
p_{\mathrm{gen}}=\sigma(w_{h^{*}}^{T} h_{t}^{*}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}})
$$<p>其中$\sigma$为sigmoid函数。然后，$p_{\text{gen}}$被用作一个“软开关”，来选择<strong>从词典中生成一个单词</strong>，或者根据注意力分布$a^t$<strong>从输入句子中复制一个单词</strong>。对于每一个文档，其<strong>拓展词表(extended vocabulary)</strong> 指原词表与当前文档中未在原词表中出现的词汇的并集。因此，生成单词$w$的概率分布建立在拓展词表上：</p>$$
P(w)=p_{\text{gen}} P_{\text{vocab}}(w)+(1-p_{\text{gen}})\sum_{i: w_{i}=w} a_{i}^{t}
$$<p>注意，当$w$是OOV词汇时，$P_{\text{vocab}}(w)$值为0；类似地，当$w$未出现在源文档中时，$\sum_{i: w_{i}=w} a_{i}^{t}$值也为0。</p><p>为了解决生成过程中出现的重复问题，引入coverage机制。在模型中，维持一个coverage向量$c^t$：</p>$$
c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}
$$<p>直觉上，$c_i^t$表示从$0-t$时间步中，单词$w_i$<strong>已经得到attention的程度</strong>。其中，$c^0$是一个零向量，因为在第一个时间步中，源文档中所有部分都未被覆盖。coverage向量被用于注意力机制的一个额外输入：</p>$$
e_{i}^{t}=v^{T}\tanh(W_{h} h_{i}+W_{s} s_{t}+w_{c} c_{i}^{t}+b_{\mathrm{attn}})
$$<p>上式可以确保<strong>注意力机制的当前决定(选择下一个注意力分布)考虑到其先前的决定(隐含在$c_t$中)</strong> 。 这应该使注意力机制更容易<strong>避免重复关注相同的位置，从而避免生成重复的文本</strong>。</p><p>定义coverage损失来对<strong>频繁attend相同的位置</strong>这一现象进行惩罚：</p>$$
\text{covloss}_t=\sum_i \min(a_i^t,c_i^t)
$$<p>最终模型的损失函数为：</p>$$
\operatorname{loss}_{t}=-\log P(w_{t}^{*})+\lambda \sum_{i} \min(a_{i}^{t}, c_{i}^{t})
$$<p>其中$\lambda$为超参数。</p><h1 id=基于预训练语言模型的文本摘要>基于预训练语言模型的文本摘要</h1><p>应用BERT可以实现抽提式文本摘要，模型如下图所示：</p><div align=center><img src=/Kimages/4/image-20211228190259107.png style=zoom:45%></div><p>首先将文档(document)以句子进行切分，在每个句子前加入[CLS]标记。将多种embeddings相加后用BERT编码，然后对每个[CLS]对应位置的输出进行分类：(1) 该句子作为摘要的一部分；(2) 该句子不作为摘要的一部分。</p><p>BertSum在CNN/DailyMail数据集上的效果超过了现有模型：</p><div align=center><img src=/Kimages/4/image-20211228190539245.png style=zoom:35%></div><p>类似地，微调GPT-2等生成式模型，也可以实现生成式摘要，并且结果显著优于未经过预训练的seq2seq等模型。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调transformers中的GPT-2模型进行文本摘要(汽车大师对话摘要)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> pandas <span style=color:#ff79c6>as</span> pd
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> BertTokenizer, TFGPT2LMHeadModel
</span></span><span style=display:flex><span>os<span style=color:#ff79c6>.</span>environ[<span style=color:#f1fa8c>&#39;CUDA_VISIBLE_DEVICES&#39;</span>] <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;0&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 设置模型和文件路径，运行前需要将模型和文件导入当前文件夹</span>
</span></span><span style=display:flex><span>gpt2_model_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;gpt2-chinese&#39;</span>
</span></span><span style=display:flex><span>gpt2_fine_tuned_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;gpt2-sum&#39;</span>
</span></span><span style=display:flex><span>train_data_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;gpt2_summarization_train.csv&#39;</span>
</span></span><span style=display:flex><span>test_data_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;gpt2_summarization_test.csv&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>gpus <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>experimental<span style=color:#ff79c6>.</span>list_physical_devices(<span style=color:#f1fa8c>&#39;GPU&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(gpus)
</span></span><span style=display:flex><span><span style=color:#6272a4># 设置按需申请，multi-GPU需要for一下</span>
</span></span><span style=display:flex><span>tf<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>experimental<span style=color:#ff79c6>.</span>set_memory_growth(gpus[<span style=color:#bd93f9>0</span>], <span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>load_model</span>(gpt2_model_path):
</span></span><span style=display:flex><span>    tokenizer <span style=color:#ff79c6>=</span> BertTokenizer<span style=color:#ff79c6>.</span>from_pretrained(gpt2_model_path)
</span></span><span style=display:flex><span>    tokenizer<span style=color:#ff79c6>.</span>pad_token <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;[PAD]&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 设置eos_token_id用于early stopping</span>
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> TFGPT2LMHeadModel<span style=color:#ff79c6>.</span>from_pretrained(gpt2_model_path, pad_token_id<span style=color:#ff79c6>=</span>tokenizer<span style=color:#ff79c6>.</span>pad_token_id, eos_token_id <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>pad_token_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tokenizer, model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>build_dataset</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 读取原始csv文件</span>
</span></span><span style=display:flex><span>    train_df <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(train_data_path, engine<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;python&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>)  <span style=color:#6272a4># 必须utf-8</span>
</span></span><span style=display:flex><span>    test_df <span style=color:#ff79c6>=</span> pd<span style=color:#ff79c6>.</span>read_csv(test_data_path, engine<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;python&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>)  <span style=color:#6272a4># 必须utf-8</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 空值去除</span>
</span></span><span style=display:flex><span>    train_df<span style=color:#ff79c6>.</span>dropna(subset<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;Question&#39;</span>, <span style=color:#f1fa8c>&#39;Dialogue&#39;</span>, <span style=color:#f1fa8c>&#39;Report&#39;</span>], how<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;any&#39;</span>, inplace<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    test_df<span style=color:#ff79c6>.</span>dropna(subset<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;Question&#39;</span>, <span style=color:#f1fa8c>&#39;Dialogue&#39;</span>], how<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;any&#39;</span>, inplace<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_question_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(train_df[<span style=color:#f1fa8c>&#39;Question&#39;</span>])
</span></span><span style=display:flex><span>    train_dialogue_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(train_df[<span style=color:#f1fa8c>&#39;Dialogue&#39;</span>])
</span></span><span style=display:flex><span>    train_report_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(train_df[<span style=color:#f1fa8c>&#39;Report&#39;</span>])
</span></span><span style=display:flex><span>    test_question_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(test_df[<span style=color:#f1fa8c>&#39;Question&#39;</span>])
</span></span><span style=display:flex><span>    test_dialogue_set <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(test_df[<span style=color:#f1fa8c>&#39;Dialogue&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_dataset <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    test_dataset <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(train_question_set)):
</span></span><span style=display:flex><span>        context <span style=color:#ff79c6>=</span> train_question_set[i] <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39; [CLS] &#39;</span> <span style=color:#ff79c6>+</span> train_dialogue_set[i]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(context) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>480</span>:
</span></span><span style=display:flex><span>            context <span style=color:#ff79c6>=</span> context[:<span style=color:#bd93f9>480</span>]
</span></span><span style=display:flex><span>        data <span style=color:#ff79c6>=</span> context <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39; [SEP] &#39;</span> <span style=color:#ff79c6>+</span> train_report_set[i]
</span></span><span style=display:flex><span>        train_dataset<span style=color:#ff79c6>.</span>append(data)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(test_question_set)):
</span></span><span style=display:flex><span>        context <span style=color:#ff79c6>=</span> test_question_set[i] <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39; [CLS] &#39;</span> <span style=color:#ff79c6>+</span> test_dialogue_set[i]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(context) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>480</span>:
</span></span><span style=display:flex><span>            context <span style=color:#ff79c6>=</span> context[:<span style=color:#bd93f9>480</span>]
</span></span><span style=display:flex><span>        data <span style=color:#ff79c6>=</span> context <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39; [SEP]&#39;</span>
</span></span><span style=display:flex><span>        test_dataset<span style=color:#ff79c6>.</span>append(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> train_dataset, test_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>fine_tuning</span>(gpt2_model_path, train_set, model_save_path, epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>, batch_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span>):
</span></span><span style=display:flex><span>    tokenizer, model <span style=color:#ff79c6>=</span> load_model(gpt2_model_path)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#8be9fd;font-style:italic>len</span>(train_set))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;encoding train set by tokenizer&#39;</span>)
</span></span><span style=display:flex><span>    train_encodings <span style=color:#ff79c6>=</span> tokenizer(train_set, truncation<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, padding<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, max_length<span style=color:#ff79c6>=</span><span style=color:#bd93f9>512</span>, return_tensors<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tf&#39;</span>, add_special_tokens<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_encodings[<span style=color:#f1fa8c>&#39;labels&#39;</span>] <span style=color:#ff79c6>=</span> [x[<span style=color:#bd93f9>1</span>:] <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> train_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>]]
</span></span><span style=display:flex><span>    train_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>] <span style=color:#ff79c6>=</span> [x[:<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> train_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>]]
</span></span><span style=display:flex><span>    train_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>] <span style=color:#ff79c6>=</span> [x[:<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> train_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_tensor_inputs <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(train_encodings[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>    train_tensor_labels <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(train_encodings[<span style=color:#f1fa8c>&#39;labels&#39;</span>])
</span></span><span style=display:flex><span>    train_tensor_mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>convert_to_tensor(train_encodings[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>])
</span></span><span style=display:flex><span>    train_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            {<span style=color:#f1fa8c>&#39;input_ids&#39;</span>: train_tensor_inputs, <span style=color:#f1fa8c>&#39;attention_mask&#39;</span>: train_tensor_mask},
</span></span><span style=display:flex><span>            train_tensor_labels,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_ds <span style=color:#ff79c6>=</span> train_ds<span style=color:#ff79c6>.</span>shuffle(<span style=color:#8be9fd;font-style:italic>len</span>(train_set))<span style=color:#ff79c6>.</span>batch(batch_size, drop_remainder<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(train_ds)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam(learning_rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3e-5</span>, epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-08</span>, clipnorm<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1.0</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>SparseCategoricalCrossentropy(from_logits<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>compile(optimizer<span style=color:#ff79c6>=</span>optimizer, loss<span style=color:#ff79c6>=</span>[loss, <span style=color:#ff79c6>*</span>[<span style=color:#ff79c6>None</span>] <span style=color:#ff79c6>*</span> model<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>n_layer])
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 训练并保存模型</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(epochs):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;training epoch&#39;</span>, i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>.</span>fit(train_ds, epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>.</span>save_pretrained(model_save_path <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_&#39;</span> <span style=color:#ff79c6>+</span> <span style=color:#8be9fd;font-style:italic>str</span>(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>) <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_epoch&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>generate</span>(gpt2_model_path, test_set, result_path):
</span></span><span style=display:flex><span>    tokenizer, model <span style=color:#ff79c6>=</span> load_model(gpt2_model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(test_set)):
</span></span><span style=display:flex><span>        sentence <span style=color:#ff79c6>=</span> test_set[i]
</span></span><span style=display:flex><span>        input_ids <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>encode(sentence, return_tensors<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tf&#39;</span>, add_special_tokens<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># sampling (文本摘要使用sampling会导致结果不准确)</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># outputs = model.generate(input_ids, do_sample=True, max_length=min(512, input_ids.shape[1] + 256), top_k=100, top_p=0.98, early_stopping=True, no_repeat_ngram_size=2, num_return_sequences=10)</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># greedy search</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># outputs = model.generate(input_ids, max_length=min(512, input_ids.shape[1] + 256), early_stopping=True)</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># beam search (用于摘要效果最佳)</span>
</span></span><span style=display:flex><span>        outputs <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(input_ids, max_length<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>min</span>(<span style=color:#bd93f9>512</span>, input_ids<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>256</span>), num_beams<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>, early_stopping<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        output_str <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>str</span>(tokenizer<span style=color:#ff79c6>.</span>decode(outputs[<span style=color:#bd93f9>0</span>], skip_special_tokens<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>))
</span></span><span style=display:flex><span>        output_str <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;&#39;</span><span style=color:#ff79c6>.</span>join(output_str<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39; &#39;</span>))
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, output_str)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 保存结果文件</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(result_path, <span style=color:#f1fa8c>&#39;a&#39;</span>, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;utf-8&#39;</span>) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>             f<span style=color:#ff79c6>.</span>write(<span style=color:#f1fa8c>&#39;Q&#39;</span> <span style=color:#ff79c6>+</span> <span style=color:#8be9fd;font-style:italic>str</span>(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>) <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;,&#39;</span> <span style=color:#ff79c6>+</span> output_str<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;[SEP]&#39;</span>)[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>__name__</span> <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    train_set, test_set <span style=color:#ff79c6>=</span> build_dataset()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># fine_tuning(gpt2_model_path, train_set, gpt2_fine_tuned_path + &#39;_1_epoch&#39;)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 指明使用训练了几轮的模型进行inference，此处仅使用了训练一轮的GPT-2</span>
</span></span><span style=display:flex><span>    generate(gpt2_fine_tuned_path <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39;_1_epoch&#39;</span>, test_set, <span style=color:#f1fa8c>&#39;result.csv&#39;</span>)
</span></span></code></pre></div><h1 id=机器阅读理解>机器阅读理解</h1><h2 id=mrc的形式化定义>MRC的形式化定义</h2><p>给定上下文$C$以及问题$Q$，<strong>机器阅读理解(machine reading comprehension, MRC)</strong> 任务要求模型给出问题$Q$正确的回答$A$，该过程通过学习函数$\mathcal F$得到，即$A=\mathcal F(C,Q)$。</p><h2 id=mrc中的四大任务>MRC中的四大任务</h2><p>机器阅读理解的四大任务分别为：<strong>完形填空(cloze test)、多项选择(multiple choice)、答案抽取(span extraction)以及自由问答(free answering)</strong>。</p><div align=center><img src=/Kimages/4/image-20200610151552516.png style=zoom:30%></div><p>四大任务的<strong>常用数据集</strong>如下：</p><p>(1) 完形填空：<strong>CNN & Daily Mail</strong>，The Children&rsquo;s Book Test，LAMBADA。</p><div align=center><img src=/Kimages/4/image-20200610152037046.png style=zoom:35%></div><p>(2) 多项选择：MC Test，RACE。</p><div align=center><img src=/Kimages/4/image-20200610152107786.png style=zoom:35%></div><p>(3) 答案抽取：<strong>SQuAD</strong>，<strong>NewsQA</strong>，TriviaQA。</p><div align=center><img src=/Kimages/4/image-20200610152142781.png style=zoom:35%></div><p>(4) 自由问答：<strong>bAbI</strong>，MS MARCO，SearchQA，<strong>DuReader</strong>。</p><div align=center><img src=/Kimages/4/image-20200610152327311.png style=zoom:35%></div><h2 id=mrc的四大基础模块>MRC的四大基础模块</h2><p>MRC任务有以下四大基础模块：</p><div align=center><img src=/Kimages/4/image-20200610153403730.png style=zoom:30%></div><p>(1) <strong>Embeddings</strong>：该模块将context和question嵌入到向量空间中，使用包含语义信息的向量来表示单词、句子以及段落的含义。常用的方法有one-hot、word2vec、<strong>预训练语言模型(ELMo、GPT、BERT)<strong>等，还可以</strong>融合其他特征</strong>，例如字符嵌入(character embedidng)、词性、命名实体等。</p><p>(2) <strong>Feature Extraction</strong>：该模块用于提取context和question更多上下文相关的特征。常用模型有RNN、CNN、Transformer等。</p><p>(3) <strong>Context-Question Interaction</strong>：该模块通过将context和question进行特征关联来产生好的答案。常用模型有单向注意力机制、<strong>双向注意力机制(bidirectional attention)</strong>、one-hop交互和<strong>multi-hop交互</strong>。该模块被认为是机器阅读理解领域最重要的模块。</p><p>(4) <strong>Answer Prediction</strong>：该模块生成答案。根据四大任务，产生答案的方式也对应有<strong>word predictor、option selector、span extractor和answer generator</strong>四种。</p><h2 id=mrc方法的发展>MRC方法的发展</h2><p>最早的MRC主要是基于特征工程结合传统机器学习方法进行的；第二个阶段是设计各式各样的神经网络模型(QA架构)在MRC任务上进行全监督学习；随着预训练语言模型的发展，MRC的第三个阶段主要方法是预训练+微调，并配合一些tricks进行MRC。</p><h2 id=mrc的主要评价指标>MRC的主要评价指标</h2><p>(1) Accuracy：主要用于<strong>完形填空</strong>和<strong>多项选择</strong>任务。当共有$m$个任务，有$n$道回答正确，则accuracy为：</p>$$
\text{Accuracy} =\frac{n}{m}
$$<p>(2) F1-score：主要用于<strong>答案抽取</strong>任务。在该任务中，<strong>候选答案(产生的答案)<strong>和</strong>参考答案(实际的答案)<strong>都被当做</strong>a bag of tokens</strong>，可以给出混淆矩阵：</p><div align=center><img src=/Kimages/4/image-20200610164905946.png style=zoom:30%></div><p>精准率、召回率以及F1-score的计算方式分别如下：</p>$$
\begin{aligned}
\text{precision}&=\frac{\text{TP}}{\text{TP}+\text{FP}}\\
\text{recall}&=\frac{\text{TP}}{\text{TP}+\text{FN}}\\
\text{F}_1&=\frac{2 \times P \times R}{P+R}
\end{aligned}
$$<p>(3) ROUGE-L：主要用于<strong>自由问答</strong>任务，pyrouge工具可以很容易地实现ROUGE值的计算。</p>$$
\begin{aligned}
R_{lcs}&=\frac{LCS(X, Y)}{m} \\
P_{lcs}&=\frac{LCS(X, Y)}{n} \\
F_{lcs}&=\frac{(1+\beta)^{2} R_{lcs} P_{lcs}}{R_{lcs}+\beta^{2} P_{lcs}}
\end{aligned}
$$<p>(4) BLEU：主要用于<strong>自由问答</strong>任务。BLEU的计算主要基于如下指标：</p>$$
P_{n}(C, R)=\frac{\sum_{i} \sum_{k} \min (h_{k}(c_{i}), \max (h_{k}(r_{i})))}{\sum_{i} \sum_{k} h_{k}(c_{i})}
$$<p>其中$h_k(c_i)$指<strong>候选答案</strong>$c_i$中出现的第$k$个<strong>n-gram的数量</strong>，$h_k(r_i)$指<strong>真正答案</strong>$r_i$中出现的第$k$个<strong>n-gram的数量</strong>。当答案更短时，$P_n(C,R)$往往容易取得较高的分数。因此，通常设定一个惩罚因子BP：</p>$$
\mathrm{BP}=\left\{\begin{array}{l}
1, l_{c}>l_{r} \\
e^{1-\frac{l_{r}}{l_{c}}}, l_{c} \leq l_{r}
\end{array}\right.
$$<p>完整的BLEU计算方式如下：</p>$$
\mathrm{BLEU}=\mathrm{BP} \cdot \exp (\sum_{n=1}^{N} w_{n} \log P_{n})
$$<p>其中，$N$为gram的大小，例如若$N=4$，则称其为BLEU-4；$w_i$通常取$1/N$。</p><h1 id=attentive-reader--impatient-reader>Attentive Reader & Impatient Reader</h1><p><strong>Attentive Reader & Impatient Reader</strong>由论文《Teaching Machines to Read and Comprehend》提出，是机器阅读理解<strong>一维匹配模型和二维匹配模型</strong>的开山之作。其适用于MRC中的<strong>完形填空</strong>任务。</p><p>文章提出了三种神经模型，用于估计根据文档$d$回答查询$q$单词类型$a$的概率$p(a|d, q)$：</p><div align=center><img src=/Kimages/4/image-20220104185208664.png style=zoom:40%></div><p>(1) <strong>Deep LSTM Reader</strong> (c)：将query和document输入LSTM，并用"|||&ldquo;分隔符隔开，将编码后的query和document输入模块$g$(文中采用全连接神经网络)，模型即可进行答案预测。</p><p>(2) <strong>Attentive Reader</strong> (a)：Deep LSTM Reader受到query和document长度影响，会存在信息瓶颈问题。Attentive Reader模型将document和query分开表示，分别使用一个双向LSTM来进行编码。对于query，模型将两个方向上最后一步的隐含层向量拼接实现query表示；对于document，模型使用注意力机制对document中所有token进行加权表示。权重越大表示回答query时对应document中的token越重要，因此注意力机制能够在一定程度上解决信息瓶颈问题。与Deep LSTM Reader类似，模型最后用query和document的表示做分类，选出最合适的答案$a$。
(3) Impatient Reader (c)：Attentive Reader可以集中注意到document中最有可能得到答案的段落。进一步，在Impatient Reader中，当query中的每个词被阅读的时候，模型都会重新阅读document。</p><p>在CNN & Daily Mail数据上的实验结果如下：</p><div align=center><img src=/Kimages/4/image-20220104192743148.png style=zoom:40%></div><p>该论文引出了MRC领域<strong>一维匹配模型</strong>和<strong>二维匹配模型</strong>的概念：一维匹配模型将query编码为固定长度的向量，计算document每个词在特定问题上下文向量中作为答案的概率；二维匹配模型将query每一个词编码，计算document中每一个词对query中每一个词的注意力，形成词与词的二维匹配结构，模型效果要稍优于一维匹配模型。</p><h1 id=bi-daf>Bi-DAF</h1><p>论文《Bidirectional Attention Flow for Machine Comprehension》提出使用双向注意力流机制实现机器阅读理解，确立了编码层-交互层-输出层的MRC结构。所提出的模型如下图所示：</p><div align=center><img src=/Kimages/4/image-20220105101023855.png style=zoom:45%></div><p>(1) 在三个<strong>嵌入层(embedding layer)</strong> 中，首先对每个token分别使用字符级别和词级别的嵌入进行标识，然后用LSTM进行编码。经过LSTM后得到两个矩阵：$H \in \mathbb R^{2d \times T}$以及$U \in \mathbb R^{2d \times J}$，其中$d$是词嵌入和字符嵌入得到的表示的长度，$T$和$J$分别表示上下文(context)和查询(query)的长度。到这一步，查询和上下文是分开并行处理的。</p><p>(2) <strong>注意力流层(attention flow layer)</strong>：该层进行context-to-query和query-to-context双向注意力的计算。首先计算出一个相似度矩阵$S \in \mathbb R^{T \times J}$，其中$S^{t, j} = \alpha(H_{:,t}, U_{:,j})$，$\alpha$是一个计算两个向量之间相似度的可训练的标量函数，文中选用$\alpha(h, u) = w_S^T [h;u;h \cdot u]$，其中&rdquo;$;$&ldquo;表示向量拼接，"$\cdot$&ldquo;表示向量按位乘法。对于context-to-query注意力的计算，计算query中每个word与context中word的相似度；对于query-to-context注意力的计算，取出每一行的最大值，然后经过softmax函数计算得到。</p><p>(3) <strong>建模层(modeling layer)</strong>：将注意力流层的两个注意力输出和嵌入曾context对应的输出进行拼接，就构成了context的query-aware表示，将该表示经过建模层即可获取进一步的高层表示。</p><p>(4) <strong>输出层(output layer)</strong>：这一层是问题相关的。在QA任务中，需要从文章的内部摘出一段作为回答，即给出一个开始位置和一个结束位置，分别表示为$p_1$和$p_2$。在计算$p_1$时，将建模层的输出通过一个全连接层并通过softmax函数得到概率，对于$p_2$的操作也类似，但作者又加入了一个LSTM函数。对于损失函数，作者将$p_1$和$p_2$的预测定义为分类任务，并将损失函数定义为交叉熵相加的形式进行优化。</p><h1 id=qa-net>QA-Net</h1><p>基于RNN结构的模型训练和推断速度较慢。论文《QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension》提出了一种基于卷积和自注意力操作的网络用于机器阅读理解，其能够在准确率不下降的情况下，以3-13倍的训练增速以及4-9倍的推断增速实现加速。模型的整体架构如下图所示：</p><div align=center><img src=/Kimages/4/image-20220105150906555.png style=zoom:55%></div>其输入层设计为300维的GloVe词向量和200维的字符嵌入。对于context-query attention层，其采用的策略与Bi-DAF一致。该论文的重点在于将Bi-DAF中的LSTM编码层替换为了多个Encoder Blocks，如上图右侧所示。在这些blocks中，卷积操作可以对局部相互作用建模(捕获文本的局部结构)，而使用自注意力机制则可以对全局交互进行建模(学习每对单词之间的相互作用)。对于SQuAD 1.1数据集，模型输出层预测的是各个位置作为开始和结束位置的概率，因此与Bi-DAF类似，将起始位置和结束位置交叉熵之和作为损失函数进行优化。<h1 id=基于预训练语言模型的mrc>基于预训练语言模型的MRC</h1><p>包括BERT、GPT在内，许多预训练语言模型如Transformer-XL、RoBERTa、ALBERT等，均能够通过简单改动去适配MRC任务。</p><p>当数据集(如SQuAD 2.0)中包含了无法回答的问题时，除了基本的encoder-decoder结构，还需要在解码器端加上一个验证模块(verifier)。先前的工作一般都用一个强大的预训练模型作为encoder来完成阅读的任务。论文《Retrospective Reader for Machine Reading Comprehension》提出了Retro-Reader的模型，为MRC中无法回答的问题设计了更好的verifier。这个模型包含两阶段的阅读(reading)和验证(verification)策略：(1) <strong>粗读阶段</strong>，先整体阅读文段和问题，得到一个基本的判断。(2) <strong>细读阶段</strong>：再次验证答案并给出最终的预测。该模型在SQuAD2.0和NewsQA数据集上达到了新的SOTA。</p><p>在下图中，[a]展示了最直接的阅读理解模型，即不存在验证模块的模型；[b]和[c]两种方式分别在模型encoder端和decoder端中加入了验证模块，并在训练时采用多任务学习的策略，从而使模型能够判断不可回答的问题并进行忽略。该论文主要提出两个模块：泛读器(sketchy reading module)和精读器(intensive reading module)，通过两阶段的方式完成阅读。下图中[d]和[e]展示了该论文所提出的结构：</p><div align=center><img src=/Kimages/4/image-20220105153105939.png style=zoom:45%></div>#### Sketchy Reading Module<p>泛读器由以下部分组成：</p><p>(1) Embedding：将question和passage拼接在一起，通过tokenizer切割为token，每一个token都是三个部分的加和：token embedding + positon embedding + token-type embedding。</p><p>(2) Interaction：通过一个多层Transformer的encoder部分来实现，得到输入的上下文表示。</p><p>(3) External Front Verification(E-FV)：机器给出一个粗糙的判断，通过E-FV来构造一个二分类器，让机器来初步判断此时的问题是不是可回答的。这里的输入是上一层输出的隐藏层的值，将上一层[CLS]对应的输出作为整个序列的表示，传入一个全连接层来判断此时的问题是否是可以回答的。给出的输出是预测概率，并使用交叉熵损失函数进行训练。E-FV同时计算了一个external verification score，在后面的部分会用到这个值：</p>$$
score_{ext} = logit_{na} - logit_{an}
$$<p>这里的na表示问题不能回答，ans表示可以回答。</p><h2 id=intensive-reading-module>Intensive Reading Module</h2><p>(1) Embedding + interaction：和泛读器的Embedding以及Interaction部分一样，模型通过一个多层Transformer的encoder部分来实现，得到输入的上下文表示$H$。之前的BERT、XLNET、ALBERT等预训练模型都是直接将$H$直接送进线性层生成预测结果。</p><p>(2) Question-aware Matching：根据位置信息，将表示$H$分为$H^Q$和$H^P$，然后提出交叉注意力(cross attention)和匹配注意力(matching attention)两种机制供选择。其中，前者将$H$和$H^Q$送入一个<strong>revised</strong> one-layer multi-head attention layer ($Q, K, V$)，将$Q$替换为$H$，将$K$和$V$替换为$H^Q$，得到最后的表示$H'$。后者将$H$和$H^Q$送入一个传统的matching attention层中，得到$H'$。</p><p>(3) Span Prediction：以$H'$为输入，用一个全连接层+softmax函数学习开始和结束位置，使用交叉熵损失函数进行训练。</p><p>(4) Internal Front Verification(I-FV)：再来进一步确认这个问题是不是可以回答的，输入为$H'$的[CLS]对应表示$h_1$，将$h_1$送入一个全连接层，再通过基于分类方法或基于回归方法得到最终的概率。训练时将Span Prediction和I-FV的损失加权进行多任务学习。</p><p>(5) Threshold-based Answerable Verification：与先前工作类似，令问题对应的文章中各个token为开始或结束位置的概率的向量分别为$s$和$e$，则分别计算存在答案的得分$score_{has}$和不存在答案的得分$score_{null}$：</p>$$
\begin{aligned}
score_{has} &=\max (s_{k}+e_{l}), 1<k \leq l \leq n, \\ score_{null} &=s_{1}+e_{1}, \end{aligned} $$<p>
继续计算上面两个概率的差值：$score_{diff}=score_{has}-score_{diff}$。</p><h2 id=rear-verification>Rear Verification</h2><p>Rear Verification模块结合泛读器和精读器的结果，最终对是否存在答案做出判断。定义$v=\beta_{1} score_{diff}+\beta_{2} score_{ext}$，其中$\beta_1$和$\beta_2$代表权重。将$v$认为是最终判断有答案的概率，如果$v$大于预先设定的阈值$\delta$，此时认为问题存在答案，返回Span Prediction的结果；反之不存在，返回空字符串。</p><h2 id=summary>Summary</h2><p>Retro-Reader利用了人类"先泛读，再精读"的习惯作为核心思想，训练一个泛读器粗略地学习问题的答案是否存在答案，再训练一个泛读器以细粒度的注意力机制来同时学习问题的答案是否存在，以及问题答案的具体范围。最终，将泛读器和精读器的结果进行融合从而判定问题的答案是否存在。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调transformers中的BERT模型实现SQuAD 1.1阅读理解任务</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> re
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> json
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> string
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> tensorflow <span style=color:#ff79c6>import</span> keras
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> tensorflow.keras <span style=color:#ff79c6>import</span> layers
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> tokenizers <span style=color:#ff79c6>import</span> BertWordPieceTokenizer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TFBertModel, BertConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>max_len <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>384</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># BertWordPieceTokenizer比transformers库中的BertTokenizer速度更快</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> BertWordPieceTokenizer(<span style=color:#f1fa8c>&#39;bert-base-uncased/vocab.txt&#39;</span>, lowercase<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 读取数据，需要将数据导入当前目录</span>
</span></span><span style=display:flex><span>pretrained_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;bert-base-uncased/&#34;</span>
</span></span><span style=display:flex><span>train_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;squad-v1.1-train.json&#39;</span>
</span></span><span style=display:flex><span>eval_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;squad-v1.1-dev.json&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>SquadExample</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, question, context, start_char_idx, answer_text, all_answers):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>question <span style=color:#ff79c6>=</span> question
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>context <span style=color:#ff79c6>=</span> context
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>start_char_idx <span style=color:#ff79c6>=</span> start_char_idx
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>answer_text <span style=color:#ff79c6>=</span> answer_text
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>all_answers <span style=color:#ff79c6>=</span> all_answers
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>preprocess</span>(<span style=font-style:italic>self</span>):
</span></span><span style=display:flex><span>        context <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>context
</span></span><span style=display:flex><span>        question <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>question
</span></span><span style=display:flex><span>        answer_text <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>answer_text
</span></span><span style=display:flex><span>        start_char_idx <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>start_char_idx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Clean context, answer and question</span>
</span></span><span style=display:flex><span>        context <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(<span style=color:#8be9fd;font-style:italic>str</span>(context)<span style=color:#ff79c6>.</span>split())
</span></span><span style=display:flex><span>        question <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(<span style=color:#8be9fd;font-style:italic>str</span>(question)<span style=color:#ff79c6>.</span>split())
</span></span><span style=display:flex><span>        answer <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(<span style=color:#8be9fd;font-style:italic>str</span>(answer_text)<span style=color:#ff79c6>.</span>split())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Find end character index of answer in context</span>
</span></span><span style=display:flex><span>        end_char_idx <span style=color:#ff79c6>=</span> start_char_idx <span style=color:#ff79c6>+</span> <span style=color:#8be9fd;font-style:italic>len</span>(answer)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> end_char_idx <span style=color:#ff79c6>&gt;=</span> <span style=color:#8be9fd;font-style:italic>len</span>(context):
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Mark the character indexes in context that are in answer</span>
</span></span><span style=display:flex><span>        is_char_in_ans <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> <span style=color:#8be9fd;font-style:italic>len</span>(context)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> idx <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(start_char_idx, end_char_idx):
</span></span><span style=display:flex><span>            is_char_in_ans[idx] <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Tokenize context</span>
</span></span><span style=display:flex><span>        tokenized_context <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>encode(context)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Find tokens that were created from answer characters</span>
</span></span><span style=display:flex><span>        ans_token_idx <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> idx, (start, end) <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(tokenized_context<span style=color:#ff79c6>.</span>offsets):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>sum</span>(is_char_in_ans[start:end]) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>                ans_token_idx<span style=color:#ff79c6>.</span>append(idx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(ans_token_idx) <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Find start and end token index for tokens from answer</span>
</span></span><span style=display:flex><span>        start_token_idx <span style=color:#ff79c6>=</span> ans_token_idx[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        end_token_idx <span style=color:#ff79c6>=</span> ans_token_idx[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Tokenize question</span>
</span></span><span style=display:flex><span>        tokenized_question <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>encode(question)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Create inputs</span>
</span></span><span style=display:flex><span>        input_ids <span style=color:#ff79c6>=</span> tokenized_context<span style=color:#ff79c6>.</span>ids <span style=color:#ff79c6>+</span> tokenized_question<span style=color:#ff79c6>.</span>ids[<span style=color:#bd93f9>1</span>:]
</span></span><span style=display:flex><span>        token_type_ids <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> <span style=color:#8be9fd;font-style:italic>len</span>(tokenized_context<span style=color:#ff79c6>.</span>ids) <span style=color:#ff79c6>+</span> [<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>*</span> <span style=color:#8be9fd;font-style:italic>len</span>(
</span></span><span style=display:flex><span>            tokenized_question<span style=color:#ff79c6>.</span>ids[<span style=color:#bd93f9>1</span>:]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        attention_mask <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>1</span>] <span style=color:#ff79c6>*</span> <span style=color:#8be9fd;font-style:italic>len</span>(input_ids)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Pad and create attention masks.</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># Skip if truncation is needed</span>
</span></span><span style=display:flex><span>        padding_length <span style=color:#ff79c6>=</span> max_len <span style=color:#ff79c6>-</span> <span style=color:#8be9fd;font-style:italic>len</span>(input_ids)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> padding_length <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0</span>:  <span style=color:#6272a4># pad</span>
</span></span><span style=display:flex><span>            input_ids <span style=color:#ff79c6>=</span> input_ids <span style=color:#ff79c6>+</span> ([<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> padding_length)
</span></span><span style=display:flex><span>            attention_mask <span style=color:#ff79c6>=</span> attention_mask <span style=color:#ff79c6>+</span> ([<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> padding_length)
</span></span><span style=display:flex><span>            token_type_ids <span style=color:#ff79c6>=</span> token_type_ids <span style=color:#ff79c6>+</span> ([<span style=color:#bd93f9>0</span>] <span style=color:#ff79c6>*</span> padding_length)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> padding_length <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>0</span>:  <span style=color:#6272a4># skip</span>
</span></span><span style=display:flex><span>            <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>input_ids <span style=color:#ff79c6>=</span> input_ids
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>token_type_ids <span style=color:#ff79c6>=</span> token_type_ids
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>attention_mask <span style=color:#ff79c6>=</span> attention_mask
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>start_token_idx <span style=color:#ff79c6>=</span> start_token_idx
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>end_token_idx <span style=color:#ff79c6>=</span> end_token_idx
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>context_token_to_char <span style=color:#ff79c6>=</span> tokenized_context<span style=color:#ff79c6>.</span>offsets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(train_path) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>    raw_train_data <span style=color:#ff79c6>=</span> json<span style=color:#ff79c6>.</span>load(f)
</span></span><span style=display:flex><span><span style=color:#ff79c6>with</span> <span style=color:#8be9fd;font-style:italic>open</span>(eval_path) <span style=color:#ff79c6>as</span> f:
</span></span><span style=display:flex><span>    raw_eval_data <span style=color:#ff79c6>=</span> json<span style=color:#ff79c6>.</span>load(f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_squad_examples</span>(raw_data):
</span></span><span style=display:flex><span>    squad_examples <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> item <span style=color:#ff79c6>in</span> raw_data[<span style=color:#f1fa8c>&#39;data&#39;</span>]:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> para <span style=color:#ff79c6>in</span> item[<span style=color:#f1fa8c>&#39;paragraphs&#39;</span>]:
</span></span><span style=display:flex><span>            context <span style=color:#ff79c6>=</span> para[<span style=color:#f1fa8c>&#39;context&#39;</span>]
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> qa <span style=color:#ff79c6>in</span> para[<span style=color:#f1fa8c>&#39;qas&#39;</span>]:
</span></span><span style=display:flex><span>                question <span style=color:#ff79c6>=</span> qa[<span style=color:#f1fa8c>&#39;question&#39;</span>]
</span></span><span style=display:flex><span>                answer_text <span style=color:#ff79c6>=</span> qa[<span style=color:#f1fa8c>&#39;answers&#39;</span>][<span style=color:#bd93f9>0</span>][<span style=color:#f1fa8c>&#39;text&#39;</span>]
</span></span><span style=display:flex><span>                all_answers <span style=color:#ff79c6>=</span> [_[<span style=color:#f1fa8c>&#39;text&#39;</span>] <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> qa[<span style=color:#f1fa8c>&#39;answers&#39;</span>]]
</span></span><span style=display:flex><span>                start_char_idx <span style=color:#ff79c6>=</span> qa[<span style=color:#f1fa8c>&#39;answers&#39;</span>][<span style=color:#bd93f9>0</span>][<span style=color:#f1fa8c>&#39;answer_start&#39;</span>]
</span></span><span style=display:flex><span>                squad_eg <span style=color:#ff79c6>=</span> SquadExample(
</span></span><span style=display:flex><span>                    question, context, start_char_idx, answer_text, all_answers
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>                squad_eg<span style=color:#ff79c6>.</span>preprocess()
</span></span><span style=display:flex><span>                squad_examples<span style=color:#ff79c6>.</span>append(squad_eg)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> squad_examples
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_inputs_targets</span>(squad_examples):
</span></span><span style=display:flex><span>    dataset_dict <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;input_ids&#39;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;token_type_ids&#39;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;attention_mask&#39;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;start_token_idx&#39;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;end_token_idx&#39;</span>: [],
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> item <span style=color:#ff79c6>in</span> squad_examples:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> item<span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>==</span> <span style=color:#ff79c6>False</span>:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> key <span style=color:#ff79c6>in</span> dataset_dict:
</span></span><span style=display:flex><span>                dataset_dict[key]<span style=color:#ff79c6>.</span>append(<span style=color:#8be9fd;font-style:italic>getattr</span>(item, key))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> key <span style=color:#ff79c6>in</span> dataset_dict:
</span></span><span style=display:flex><span>        dataset_dict[key] <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>array(dataset_dict[key])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        dataset_dict[<span style=color:#f1fa8c>&#39;input_ids&#39;</span>],
</span></span><span style=display:flex><span>        dataset_dict[<span style=color:#f1fa8c>&#39;token_type_ids&#39;</span>],
</span></span><span style=display:flex><span>        dataset_dict[<span style=color:#f1fa8c>&#39;attention_mask&#39;</span>],
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    y <span style=color:#ff79c6>=</span> [dataset_dict[<span style=color:#f1fa8c>&#39;start_token_idx&#39;</span>], dataset_dict[<span style=color:#f1fa8c>&#39;end_token_idx&#39;</span>]]
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> x, y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_squad_examples <span style=color:#ff79c6>=</span> create_squad_examples(raw_train_data)
</span></span><span style=display:flex><span>x_train, y_train <span style=color:#ff79c6>=</span> create_inputs_targets(train_squad_examples)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>{</span><span style=color:#8be9fd;font-style:italic>len</span>(train_squad_examples)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> training points created.&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>eval_squad_examples <span style=color:#ff79c6>=</span> create_squad_examples(raw_eval_data)
</span></span><span style=display:flex><span>x_eval, y_eval <span style=color:#ff79c6>=</span> create_inputs_targets(eval_squad_examples)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>{</span><span style=color:#8be9fd;font-style:italic>len</span>(eval_squad_examples)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> evaluation points created.&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_model</span>():
</span></span><span style=display:flex><span>    encoder <span style=color:#ff79c6>=</span> TFBertModel<span style=color:#ff79c6>.</span>from_pretrained(pretrained_path)  <span style=color:#6272a4># BERT encoder</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    input_ids <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Input(shape<span style=color:#ff79c6>=</span>(max_len,), dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>int32)
</span></span><span style=display:flex><span>    token_type_ids <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Input(shape<span style=color:#ff79c6>=</span>(max_len,), dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>int32)
</span></span><span style=display:flex><span>    attention_mask <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Input(shape<span style=color:#ff79c6>=</span>(max_len,), dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embedding <span style=color:#ff79c6>=</span> encoder(input_ids, token_type_ids<span style=color:#ff79c6>=</span>token_type_ids, attention_mask<span style=color:#ff79c6>=</span>attention_mask)[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_logits <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>1</span>, name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;start_logit&#39;</span>, use_bias<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)(embedding)
</span></span><span style=display:flex><span>    start_logits <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Flatten()(start_logits)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    end_logits <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>1</span>, name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;end_logit&#39;</span>, use_bias<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)(embedding)
</span></span><span style=display:flex><span>    end_logits <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Flatten()(end_logits)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_probs <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Activation(keras<span style=color:#ff79c6>.</span>activations<span style=color:#ff79c6>.</span>softmax)(start_logits)
</span></span><span style=display:flex><span>    end_probs   <span style=color:#ff79c6>=</span> layers<span style=color:#ff79c6>.</span>Activation(keras<span style=color:#ff79c6>.</span>activations<span style=color:#ff79c6>.</span>softmax)(end_logits)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> keras<span style=color:#ff79c6>.</span>Model(
</span></span><span style=display:flex><span>        inputs<span style=color:#ff79c6>=</span>[input_ids, token_type_ids, attention_mask],
</span></span><span style=display:flex><span>        outputs<span style=color:#ff79c6>=</span>[start_probs, end_probs],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    loss <span style=color:#ff79c6>=</span> keras<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>SparseCategoricalCrossentropy(from_logits<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>    optimizer <span style=color:#ff79c6>=</span> keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam(lr<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5e-5</span>)
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>compile(optimizer<span style=color:#ff79c6>=</span>optimizer, loss<span style=color:#ff79c6>=</span>[loss, loss])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> create_model()
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>normalize_text</span>(text):
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> text<span style=color:#ff79c6>.</span>lower()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Remove punctuations</span>
</span></span><span style=display:flex><span>    exclude <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>set</span>(string<span style=color:#ff79c6>.</span>punctuation)
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;&#39;</span><span style=color:#ff79c6>.</span>join(ch <span style=color:#ff79c6>for</span> ch <span style=color:#ff79c6>in</span> text <span style=color:#ff79c6>if</span> ch <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>in</span> exclude)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Remove articles</span>
</span></span><span style=display:flex><span>    regex <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>compile(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#39;\b(a|an|the)\b&#39;</span>, re<span style=color:#ff79c6>.</span>UNICODE)
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>sub(regex, <span style=color:#f1fa8c>&#39; &#39;</span>, text)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Remove extra white space</span>
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39; &#39;</span><span style=color:#ff79c6>.</span>join(text<span style=color:#ff79c6>.</span>split())
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 构造回调函数，在每个epoch结束后通过验证数据计算出当前模型效果</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>ExactMatch</span>(keras<span style=color:#ff79c6>.</span>callbacks<span style=color:#ff79c6>.</span>Callback):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, x_eval, y_eval):
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>x_eval <span style=color:#ff79c6>=</span> x_eval
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>y_eval <span style=color:#ff79c6>=</span> y_eval
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_epoch_end</span>(<span style=font-style:italic>self</span>, epoch, logs<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        pred_start, pred_end <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>model<span style=color:#ff79c6>.</span>predict(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>x_eval)
</span></span><span style=display:flex><span>        count <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>        eval_examples_no_skip <span style=color:#ff79c6>=</span> [_ <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> eval_squad_examples <span style=color:#ff79c6>if</span> _<span style=color:#ff79c6>.</span>skip <span style=color:#ff79c6>==</span> <span style=color:#ff79c6>False</span>]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> idx, (start, end) <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(<span style=color:#8be9fd;font-style:italic>zip</span>(pred_start, pred_end)):
</span></span><span style=display:flex><span>            squad_eg <span style=color:#ff79c6>=</span> eval_examples_no_skip[idx]
</span></span><span style=display:flex><span>            offsets <span style=color:#ff79c6>=</span> squad_eg<span style=color:#ff79c6>.</span>context_token_to_char
</span></span><span style=display:flex><span>            start <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>argmax(start)
</span></span><span style=display:flex><span>            end <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>argmax(end)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> start <span style=color:#ff79c6>&gt;=</span> <span style=color:#8be9fd;font-style:italic>len</span>(offsets):
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>continue</span>
</span></span><span style=display:flex><span>            pred_char_start <span style=color:#ff79c6>=</span> offsets[start][<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> end <span style=color:#ff79c6>&lt;</span> <span style=color:#8be9fd;font-style:italic>len</span>(offsets):
</span></span><span style=display:flex><span>                pred_char_end <span style=color:#ff79c6>=</span> offsets[end][<span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>                pred_ans <span style=color:#ff79c6>=</span> squad_eg<span style=color:#ff79c6>.</span>context[pred_char_start:pred_char_end]
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>                pred_ans <span style=color:#ff79c6>=</span> squad_eg<span style=color:#ff79c6>.</span>context[pred_char_start:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            normalized_pred_ans <span style=color:#ff79c6>=</span> normalize_text(pred_ans)
</span></span><span style=display:flex><span>            normalized_true_ans <span style=color:#ff79c6>=</span> [normalize_text(_) <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> squad_eg<span style=color:#ff79c6>.</span>all_answers]
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> normalized_pred_ans <span style=color:#ff79c6>in</span> normalized_true_ans:
</span></span><span style=display:flex><span>                count <span style=color:#ff79c6>+=</span> <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>        acc <span style=color:#ff79c6>=</span> count <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>y_eval[<span style=color:#bd93f9>0</span>])
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>epoch=</span><span style=color:#f1fa8c>{</span>epoch<span style=color:#ff79c6>+</span><span style=color:#bd93f9>1</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>, exact match score=</span><span style=color:#f1fa8c>{</span>acc<span style=color:#f1fa8c>:</span><span style=color:#f1fa8c>.2f</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练模型，这里仅使用部分数据做演示</span>
</span></span><span style=display:flex><span>exact_match_callback <span style=color:#ff79c6>=</span> ExactMatch(x_eval, y_eval)
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>fit(x_train[:<span style=color:#bd93f9>1000</span>], y_train[:<span style=color:#bd93f9>1000</span>], epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>, batch_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>16</span>, callbacks<span style=color:#ff79c6>=</span>[exact_match_callback])
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>Dan Jurafsky, H. Martin. Speech and Language Processing(3rd ed. draft).</p></li><li><p>Kim Y. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.</p></li><li><p>Zhang Y, Wallace B. A sensitivity analysis of (and practitioners&rsquo; guide to) convolutional neural networks for sentence classification. arXiv preprint arXiv:1510.03820, 2015.</p></li><li><p>Joulin A, Grave E, Bojanowski P, et al. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016.</p></li><li><p>Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p></li><li><p>Mihalcea R, Tarau P. Textrank: Bringing order into text. Proceedings of the 2004 conference on empirical methods in natural language processing. 2004: 404-411.</p></li><li><p>See A, Liu P J, Manning C D. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.</p></li><li><p>Liu Y. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318, 2019.</p></li><li><p>文本摘要简述：https://www.jiqizhixin.com/articles/2019-03-25-7</p></li><li><p>TextRank算法详细讲解与代码实现：https://www.cnblogs.com/motohq/p/11887420.html</p></li><li><p>百度AI问答摘要与推理常规赛官网：https://aistudio.baidu.com/aistudio/competition/detail/80/0/introduction</p></li><li><p>Liu S, Zhang X, Zhang S, et al. Neural machine reading comprehension: Methods and trends. Applied Sciences, 2019, 9(18): 3698.</p></li><li><p>Hermann K M, Kocisky T, Grefenstette E, et al. Teaching machines to read and comprehend. Advances in neural information processing systems, 2015, 28: 1693-1701.</p></li><li><p>Seo M, Kembhavi A, Farhadi A, et al. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.</p></li><li><p>Yu A W, Dohan D, Luong M T, et al. QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension. International Conference on Learning Representations. 2018.</p></li><li><p>Zhang Z, Yang J, Zhao H. Retrospective Reader for Machine Reading Comprehension. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(16): 14506-14514.</p></li><li><p>Liu S, Zhang X, Zhang S, et al. Neural machine reading comprehension: Methods and trends. Applied Sciences, 2019, 9(18): 3698.</p></li><li><p>Hermann K M, Kocisky T, Grefenstette E, et al. Teaching machines to read and comprehend. Advances in neural information processing systems, 2015, 28: 1693-1701.</p></li><li><p>Seo M, Kembhavi A, Farhadi A, et al. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.</p></li><li><p>Yu A W, Dohan D, Luong M T, et al. QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension. International Conference on Learning Representations. 2018.</p></li><li><p>Zhang Z, Yang J, Zhao H. Retrospective Reader for Machine Reading Comprehension. Proceedings of the AAAI Conference on Artificial Intelligence. 2021, 35(16): 14506-14514.</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/ data-toggle=tooltip data-placement=top title=自然语言处理：预训练模型>&larr;
Previous Post</a></li><li class=next><a href=/2021/05/13/reflections-on-witnessing-imbalance/ data-toggle=tooltip data-placement=top title=《见证失衡》读后感>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>