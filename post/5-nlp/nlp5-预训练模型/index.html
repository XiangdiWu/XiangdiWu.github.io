<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="自然语言处理：预训练模型"><meta property="og:title" content="自然语言处理：预训练模型"><meta property="twitter:title" content="自然语言处理：预训练模型"><meta name=description content="本文主要介绍了ELMo和Transformer的预训练模型，以及其在机器翻译和其他NLP任务中的应用。"><meta property="og:description" content="本文主要介绍了ELMo和Transformer的预训练模型，以及其在机器翻译和其他NLP任务中的应用。"><meta property="twitter:description" content="本文主要介绍了ELMo和Transformer的预训练模型，以及其在机器翻译和其他NLP任务中的应用。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>自然语言处理：预训练模型-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/nlp title=NLP>NLP</a></div><h1>自然语言处理：预训练模型</h1><h2 class=subheading>ELMo和Transformer</h2><span class=meta>Posted by
XiangdiWu
on
Saturday, October 24, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=elmo>ELMo</h1><p>预训练词向量(如word2vec和GloVe等)通常只能为一个单词产生一个特定的词向量，而忽略了该单词的<strong>上下文(context)</strong> 关系，因而无法解决<strong>一词多义</strong>或<strong>一义多词</strong>的问题。<strong>ELMo(embeddings from language models)</strong> 本质上是一个深度双向LSTM模型，用于为一个句子中的每个单词生成上下文相关的词向量。将这些上下文相关词向量编码了单词的深层次语义和句法信息，因此当ELMo应用到许多NLP任务中，这些任务的效果相对于使用静态的词向量往往能得到很大的提升。</p><p>ELMo是整个输入句子的函数，其输出为句子中每个单词的上下文相关词向量。给定一个含有$N$个标记的序列$(t_1,t_2,\cdots,t_N)$，<strong>前向语言模型(forward language model)</strong> 通过建模在给定之前的标记序列$(t_1,\cdots,t_{k-1})$下$t_k$的概率来计算该句子(标记序列)的概率：</p>$$
p\left(t_{1}, t_{2}, \cdots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \cdots, t_{k-1}\right)
$$<p>在ELMo之前的语言模型通常为第$k$个位置的单词(通过embedding等方式)计算出一个上下文无关的词表示$\mathbf x_k^{LM}$，然后将其送入一个$L$层的前向LSTM。在每个位置$k$，每一层LSTM会输出一个上下文相关的表示为$\overrightarrow{\mathbf h}_{k,j}^{LM}$，其中$j=1,2,\cdots,L$。最顶层的LSTM输出$\overrightarrow{\mathbf h}_{k,L}^{LM}$在下游任务被用来预测下一个标记(通过sottmax层等方式)，即$t_{k+1}$。</p><p><strong>反向语言模型(backward language model)</strong> 与前向语言模型的计算方向正好相反：</p>$$
p\left(t_{1}, t_{2}, \cdots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \cdots, t_{N}\right)
$$<p>因此在第$k$个位置，第$j$层LSTM的输出表示为$\overleftarrow{\mathbf h}_{k,j}^{LM}$。</p><p><strong>双向语言模型(biLM)</strong> 可以结合前向和反向的语言模型。其可以形式化表示为<strong>最大化前向和反向语言模型的对数似然函数之和</strong>：</p>$$
\sum_{k=1}^{N} (\log p(t_{k} | t_{1}, \cdots, t_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{LSTM}, \Theta_{s})+\log p(t_{k} | t_{k+1}, \cdots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}))
$$<p>在ELMo中，将第$k$ 个位置的标记一个$L$层的双向语言模型，可以得到$2L+1$个词的表示：</p>$$
\begin{aligned}
R_{k} &=\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} | j=1, \cdots, L\} \\
&=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \cdots, L\right\}
\end{aligned}
$$<p>其中，${\mathbf h}_{k,0}^{LM}$代表$\mathbf{x}_{k}^{L M}$，${\mathbf h}_{k,0}^{LM}=[\overrightarrow{\mathbf h}_{k,j}^{LM};\overleftarrow{\mathbf h}_{k,j}^{LM}]$。</p><p>对于下游任务，ELMo将上述词的表示的向量集合使用一个单独的词向量$\mathbf{ELMo}_k=E(R_k;\Theta_e)$进行表示。最简单的情况是直接选出<strong>最上层的词表示</strong>作为最终结果，即$E(R_k)=\mathbf h^{LM}_{k,L}$，TagLM以及CoVe便是使用了这样的方法。更一般的方法是使用参数学习的方式来表示最终的上下文相关词向量：</p>$$
\mathbf{ELMo}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}
$$<p>其中，$s^{task}$用于调节每一层的词表示$\mathbf h^{LM}_{k,j}$的权重，$\gamma^{task}$用于调节整个ELMo向量的权重。考虑到每一层的激活值会有不同的分布，可以在每一层后面添加一个层归一化(layer normalization)。</p><p>给定一个目标NLP任务的预训练biLM和一个有监督的体系结构，使用biLM改进任务模型是一个简单的过程。我们只需运行biLM并记录每个单词在所有层中的表示，然后将每个层产生的词表示合并为最终单一的词表示向量$\mathbf{ELMo}_k^{task}$。在这之后，便可以直接将该单一的词表示向量送入下游任务中，如将一句话中的单词逐个输入RNN中进行文本分类、问答等任务。</p><p>作为一种双向语言模型，ELMo<strong>预训练的方式</strong>是根据一个单词左右两边的单词来预测当前单词。</p><p>ELMo在多个NLP任务及数据集上的实验结果如下所示：</p><img src=/Kimages/4/image-20200420174149047.png style=zoom:33%><h1 id=transformer>Transformer</h1><p>在Transformer之前的序列模型均采用了<strong>循环层(recurrent layer)和卷积层(convolution layer)</strong>，其中效果最好的模型均采用了<strong>注意力(attention)</strong> 机制。Transformer摒弃了全部循环层和卷积层，只基于注意力机制，实现了<strong>计算的并行化</strong>。原论文采用机器翻译以及其他NLP任务来验证Transformer的有效性。Transformer的模型结构如下所示，其中左半部分是多个编码器，右半部分是多个解码器：</p><img src=/Kimages/4/image-20200420153815269.png style=zoom:40%><h2 id=encoder-and-decoder-stacks>Encoder and Decoder Stacks</h2><p><strong>编码器(encoder)</strong> 是由$N=6$个编码器层组成的栈式结构，其中每个编码器层中有两个sub-layers，即<strong>多头注意力机制(multi-head attentin)</strong> 层和<strong>全连接前馈神经网络</strong>层。每一个sub-layer都采用了<strong>残差连接(residual connection)<strong>以及</strong>层归一化(layer normalization)</strong>。因此，每个sub-layer的输出为$\text{LayerNorm}(x+\text{Sublayer}(x))$。为了方便进行残差连接，模型中所有sub-layers的输出维度，以及embedding层的输出维度，均为$d_{model}=512$。<strong>解码器(decoder)</strong> 也是由$N=6$个解码器层组成的栈式结构，但解码器层比编码器层多一个sub-layer，即掩码多头注意力机制，保证了在位置$i$的预测仅仅依赖于在$i$之前已知的预测。</p><h2 id=attention>Attention</h2><p>注意力函数可以描述为<strong>将查询(query)和一组键(key)-值(value)对映射到输出</strong>，其中查询、键、值和输出都是向量。<strong>输出被计算为值的加权和</strong>，其中分配给每个值的权重由查询的兼容函数和相应的键计算。</p><p>论文中的attention机制称为“scaled dot-product attention”，如下图所示：</p><div align=center><img src=/Kimages/4/image-20200420184046759.png style=zoom:30%></div><p>其输入包括queries(维度为$d_k$)、keys(维度为$d_k$)以及values(维度为$d_v$)。当并行处理多个输入的时候，可以将其组成三个矩阵$Q,K,V$，并且以下式来计算注意力输出：</p>$$
\text{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$<p>两种最常用的注意力函数分别是加性注意力(additive attention)和点积注意力(dot-product attention)。论文中使用了点积注意力，并且除以$\sqrt{d_k}$。与加性注意力相比，点积注意力计算更快，并且在实际应用中更加节省空间。值得注意的是，$QK^T$除以$\sqrt{d_k}$的原因如下：当$d_k$的值比较大时，点积的增长速度很快，导致sotfmax函数来到梯度非常小的区域。因此，因子$\sqrt{d_k}$的作用是<strong>控制sotfmax函数的值的范围，减少梯度消失的问题</strong>。</p><p>论文提出，将输入线性映射到$h$个不同的空间内并且使用多个并行的注意力机制能够提高模型性能。该方式称为多头注意力机制，如下图所示：</p><div align=center><img src=/Kimages/4/image-20200420201802904.png style=zoom:30%></div><p>多头注意力机制可以形式化地表示为：</p>$$
\begin{aligned}
\text {MultiHead}(Q, K, V) &=\text {Concat}\left(\text {head}_{1}, \ldots, \text {head}_{\mathrm{h}}\right) W^{O} \\
\text {where head}_{\mathrm{i}} &=\text {Attention}(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V})
\end{aligned}
$$<p>该式可以利用下图进行理解：</p><div align=center><img src=/Kimages/4/image-20200420203933069.png style=zoom:45%></div><p>假设模块的输入为$X$(即原始句子的词嵌入序列，维度为$(len,d_{model})$)或者$R$(即上一个编码器模块传来的输入，维度也为$(len,d_{model})$)。图中的$X$或$R$等价于原文公式中的$Q,K,V$，首先分别乘以$3\times h$个不同的权重矩阵(维度分别为$(d_{model},d_k),(d_{model},d_k),(d_{model},d_v)$)以映射到$3\times h$个不同的空间，然后分别按照每一个注意力头的$Q,K,V$进行单头注意力计算，产生每个注意力头的结果$Z_i$，维度为$(len,d_v)$。将$h$个$Z_i$连接在一起，得到维度为$(len,hd_v)$大小的矩阵。将连接后的矩阵$Z$与参数矩阵$W^O$(维度为$(hd_v,d_{model})$)相乘，得到最终的结果为$Z$，维度为$(len,d_{model})$。原文取$h=8,d_k=d_v=d_{model}/h=64$。</p><p>Transformer中的多头注意力机制有如下三种不同的方式：</p><p>(1) 在encoder-decoder attention中，queries来自于<strong>上一个解码器层的输出</strong>，keys和values来自于<strong>每一个编码器层的输出</strong>。这种方式使得解码器输出的每一步都能关注编码器输入的每一个位置。</p><p>(2) 在编码器中，<strong>所有的keys、values和queries都来自于同一个矩阵</strong>，即最初的词向量序列输入$X$或者上一个编码器层的输出$R$。</p><p>(3) 在解码器中，每一步解码器能够关注当前步以及当前步之前的向量。我们需要保留解码器中输出句子每一步左边的信息流来维持其<strong>自回归(auto-regression)</strong> 的特性。该方法通过<strong>遮盖(mask)</strong> 实现。</p><h2 id=position-wise-feed-forward-networks>Position-wise Feed-Forward Networks</h2><p>在多头注意力层后，加入一层全连接前馈神经网络层，该网络层由两次线性映射以及一个ReLU非线性映射组成，即：</p>$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$<p>在该网络中，输入和输出的维度均为$d_{model}=512$，中间层的维度为$d_{ff}=2048$。</p><h2 id=embeddings-and-softmax>Embeddings and Softmax</h2><p>与其他序列模型相似，Transformer编码器的inputs和解码器的outputs(都是输入)均是学习得到的词向量，维度均为$d_{model}$。sotfmax层在解码器之后，结合一个线性变换层用于预测当前单词的概率分布。</p><h2 id=positional-encoding>Positional Encoding</h2><p>由于模型中不含循环层以及卷积层，为了使模型得以利用句子中的顺序信息，必须向输入中加入相对或绝对的位置信息。文章通过给编码器和解码器的输入词向量$X$加入<strong>位置编码(positional encodings)</strong> 来实现顺序信息的引入。位置编码的维度与输入词向量相同，均为$d_{model}$。最终输入编码器和解码器的向量为<strong>原始词向量与其对应位置编码之和</strong>。</p><p>在论文中，位置编码采用如下形式：</p>$$
\begin{aligned}
P E_{(p o s, 2 i)} &=\sin (pos / 10000^{2 i / d_{\text{model}}}) \\
P E_{(\text {pos}, 2 i+1)} &=\cos (pos / 10000^{2 i / d_{\text{model}}})
\end{aligned}
$$<p>其中，$pos$是每个单词在句子中的位置(position)，$i$代表每个词向量的第$i$个维度(dimension)。</p><p>下表分析了self-attention与循环层和卷积层相比时间复杂度的差异：</p><div align=center><img src=/Kimages/4/image-20200420222402891.png style=zoom:33%></div><p>实验结果如下：</p><div align=center><img src=/Kimages/4/image-20200420222424199.png style=zoom:33%></div><h2 id=tensorflow实现transformer用于机器翻译>Tensorflow实现Transformer用于机器翻译</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> unicodedata
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> re
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> io
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> time
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.ticker <span style=color:#ff79c6>as</span> ticker
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sklearn.model_selection <span style=color:#ff79c6>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据路径，需要将数据导入当前目录</span>
</span></span><span style=display:flex><span>path_to_file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;transformer_pt_en_data.txt&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据预处理，将unicode文件转换为ascii</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>unicode_to_ascii</span>(s):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#f1fa8c>&#39;&#39;</span><span style=color:#ff79c6>.</span>join(c <span style=color:#ff79c6>for</span> c <span style=color:#ff79c6>in</span> unicodedata<span style=color:#ff79c6>.</span>normalize(<span style=color:#f1fa8c>&#39;NFD&#39;</span>, s) <span style=color:#ff79c6>if</span> unicodedata<span style=color:#ff79c6>.</span>category(c) <span style=color:#ff79c6>!=</span> <span style=color:#f1fa8c>&#39;Mn&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>preprocess_sentence</span>(w):
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> unicode_to_ascii(w<span style=color:#ff79c6>.</span>lower()<span style=color:#ff79c6>.</span>strip())
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 在单词与跟在其后的标点符号之间插入一个空格，例如： &#34;he is a boy.&#34; =&gt; &#34;he is a boy .&#34;</span>
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>sub(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#34;([?.!,¿])&#34;</span>, <span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#34; \1 &#34;</span>, w)
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>sub(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#39;[&#34; ]+&#39;</span>, <span style=color:#f1fa8c>&#34; &#34;</span>, w)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 除了 (a-z, A-Z, &#34;.&#34;, &#34;?&#34;, &#34;!&#34;, &#34;,&#34;)，将所有字符替换为空格</span>
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>sub(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#34;[^a-zA-Z?.!,¿]+&#34;</span>, <span style=color:#f1fa8c>&#34; &#34;</span>, w)
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> w<span style=color:#ff79c6>.</span>rstrip()<span style=color:#ff79c6>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 给句子加上开始和结束标记，以便模型知道何时开始和结束预测</span>
</span></span><span style=display:flex><span>    w <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;&lt;start&gt; &#39;</span> <span style=color:#ff79c6>+</span> w <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#39; &lt;end&gt;&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> w
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 测试句子预处理是否正确</span>
</span></span><span style=display:flex><span>en_sentence <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;May I borrow this book?&#34;</span>
</span></span><span style=display:flex><span>sp_sentence <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;¿Puedo tomar prestado este libro?&#34;</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;句子的预处理: &#39;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(en_sentence, <span style=color:#f1fa8c>&#39;  -&gt;  &#39;</span>, preprocess_sentence(en_sentence))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(sp_sentence, <span style=color:#f1fa8c>&#39;  -&gt;  &#39;</span>, preprocess_sentence(sp_sentence))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 构造数据集，返回这样格式的单词对：[英语, 葡萄牙语]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_dataset</span>(path, num_examples):
</span></span><span style=display:flex><span>    lines <span style=color:#ff79c6>=</span> io<span style=color:#ff79c6>.</span>open(path, encoding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;UTF-8&#39;</span>)<span style=color:#ff79c6>.</span>read()<span style=color:#ff79c6>.</span>strip()<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>    word_pairs <span style=color:#ff79c6>=</span> [[preprocess_sentence(w) <span style=color:#ff79c6>for</span> w <span style=color:#ff79c6>in</span> l<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\t</span><span style=color:#f1fa8c>&#39;</span>)[:<span style=color:#bd93f9>2</span>]] <span style=color:#ff79c6>for</span> l <span style=color:#ff79c6>in</span> lines[:num_examples]]
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#8be9fd;font-style:italic>zip</span>(<span style=color:#ff79c6>*</span>word_pairs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>max_length</span>(tensor):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#8be9fd;font-style:italic>max</span>(<span style=color:#8be9fd;font-style:italic>len</span>(t) <span style=color:#ff79c6>for</span> t <span style=color:#ff79c6>in</span> tensor)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>tokenize</span>(lang):
</span></span><span style=display:flex><span>    lang_tokenizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>preprocessing<span style=color:#ff79c6>.</span>text<span style=color:#ff79c6>.</span>Tokenizer(filters<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;&#39;</span>)
</span></span><span style=display:flex><span>    lang_tokenizer<span style=color:#ff79c6>.</span>fit_on_texts(lang)
</span></span><span style=display:flex><span>    tensor <span style=color:#ff79c6>=</span> lang_tokenizer<span style=color:#ff79c6>.</span>texts_to_sequences(lang)
</span></span><span style=display:flex><span>    tensor <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>preprocessing<span style=color:#ff79c6>.</span>sequence<span style=color:#ff79c6>.</span>pad_sequences(tensor, padding<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;post&#39;</span>)  <span style=color:#6272a4># 后部补0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tensor, lang_tokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>load_dataset</span>(path, num_examples<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 创建清理过的输入输出对</span>
</span></span><span style=display:flex><span>    targ_lang, inp_lang <span style=color:#ff79c6>=</span> create_dataset(path, num_examples)  <span style=color:#6272a4># 英语和西班牙语</span>
</span></span><span style=display:flex><span>    input_tensor, inp_lang_tokenizer <span style=color:#ff79c6>=</span> tokenize(inp_lang)
</span></span><span style=display:flex><span>    target_tensor, targ_lang_tokenizer <span style=color:#ff79c6>=</span> tokenize(targ_lang)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 限制数据集的大小以加快实验速度</span>
</span></span><span style=display:flex><span>num_examples <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>20000</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 读取input(葡萄牙语)向量、output(英语)向量以及两个语言的tokenizer</span>
</span></span><span style=display:flex><span>input_tensor, target_tensor, tokenizer_pt, tokenizer_en <span style=color:#ff79c6>=</span> load_dataset(path_to_file, num_examples)
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算目标张量的最大长度 （max_length）</span>
</span></span><span style=display:flex><span>max_length_inp, max_length_targ <span style=color:#ff79c6>=</span> max_length(input_tensor), max_length(target_tensor)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;前20000条语料中，葡萄牙语的最大长度为: &#39;</span>, max_length_inp, <span style=color:#f1fa8c>&#39;英语的最大长度为: &#39;</span>, max_length_targ)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 采用80-20的比例切分训练集和验证集</span>
</span></span><span style=display:flex><span>input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val <span style=color:#ff79c6>=</span> train_test_split(input_tensor, target_tensor, test_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.2</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 显示长度</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;训练和测试集的长度(均为ndarray)，每一条代表一条句子index序列): &#39;</span>, <span style=color:#8be9fd;font-style:italic>len</span>(input_tensor_train), <span style=color:#8be9fd;font-style:italic>len</span>(target_tensor_train), <span style=color:#8be9fd;font-style:italic>len</span>(input_tensor_val), <span style=color:#8be9fd;font-style:italic>len</span>(target_tensor_val))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建tf.data.Data数据集(这里仅构建了训练集)</span>
</span></span><span style=display:flex><span>train_dataset <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices((input_tensor_train, target_tensor_train))
</span></span><span style=display:flex><span>train_dataset <span style=color:#ff79c6>=</span> train_dataset<span style=color:#ff79c6>.</span>batch(<span style=color:#bd93f9>64</span>, drop_remainder<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;数据集: &#39;</span>, train_dataset)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>example_input_batch, example_target_batch <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>next</span>(<span style=color:#8be9fd;font-style:italic>iter</span>(train_dataset))
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;一个批次的训练数据和标签的shape: &#39;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(example_input_batch<span style=color:#ff79c6>.</span>shape, <span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>, example_target_batch<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;至此，数据集处理完毕</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 位置编码(Positional encoding)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_angles</span>(pos, i, d_model):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># pos为单词在句子中的绝对位置，i指的是单词embedding向量的第i维</span>
</span></span><span style=display:flex><span>    angle_rates <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>power(<span style=color:#bd93f9>10000</span>, (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> (i <span style=color:#ff79c6>//</span> <span style=color:#bd93f9>2</span>)) <span style=color:#ff79c6>/</span> np<span style=color:#ff79c6>.</span>float32(d_model))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> pos <span style=color:#ff79c6>*</span> angle_rates
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>positional_encoding</span>(position, d_model):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># d_model是位置编码的维度，也是embedding向量的维度</span>
</span></span><span style=display:flex><span>    angle_rads <span style=color:#ff79c6>=</span> get_angles(np<span style=color:#ff79c6>.</span>arange(position)[:, np<span style=color:#ff79c6>.</span>newaxis], np<span style=color:#ff79c6>.</span>arange(d_model)[np<span style=color:#ff79c6>.</span>newaxis, :], d_model)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将sin应用于数组中的偶数索引2i</span>
</span></span><span style=display:flex><span>    angle_rads[:, <span style=color:#bd93f9>0</span>::<span style=color:#bd93f9>2</span>] <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>sin(angle_rads[:, <span style=color:#bd93f9>0</span>::<span style=color:#bd93f9>2</span>])
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将cos应用于数组中的奇数索引2i+1</span>
</span></span><span style=display:flex><span>    angle_rads[:, <span style=color:#bd93f9>1</span>::<span style=color:#bd93f9>2</span>] <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>cos(angle_rads[:, <span style=color:#bd93f9>1</span>::<span style=color:#bd93f9>2</span>])
</span></span><span style=display:flex><span>    pos_encoding <span style=color:#ff79c6>=</span> angle_rads[np<span style=color:#ff79c6>.</span>newaxis, <span style=color:#ff79c6>...</span>]
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>cast(pos_encoding, dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pos_encoding <span style=color:#ff79c6>=</span> positional_encoding(<span style=color:#bd93f9>50</span>, <span style=color:#bd93f9>512</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;位置编码的shape: &#39;</span>, pos_encoding<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 填充遮挡(padding mask)遮挡一批序列中所有的填充标记。这确保了模型不会将填充作为输入。该mask表明填充值0出现的位置：在这些位置mask输出1，否则输出0</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_padding_mask</span>(seq):
</span></span><span style=display:flex><span>    seq <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>cast(tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>equal(seq, <span style=color:#bd93f9>0</span>), tf<span style=color:#ff79c6>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 添加额外的维度来将填充加到注意力对数(logits)</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> seq[:, tf<span style=color:#ff79c6>.</span>newaxis, tf<span style=color:#ff79c6>.</span>newaxis, :]  <span style=color:#6272a4># (batch_size, 1, 1, seq_len)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>constant([[<span style=color:#bd93f9>7</span>, <span style=color:#bd93f9>6</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>1</span>], [<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>4</span>, <span style=color:#bd93f9>5</span>]])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;[[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]对0进行mask后: &#39;</span>, create_padding_mask(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 前瞻遮挡(look-ahead mask)用于遮挡一个序列中的后续标记，用于Transformer解码器</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这意味着要预测第三个词，将仅使用第一个和第二个词。与此类似，预测第四个词，仅使用第一个，第二个和第三个词，依此类推</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_look_ahead_mask</span>(size):
</span></span><span style=display:flex><span>    mask <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>-</span> tf<span style=color:#ff79c6>.</span>linalg<span style=color:#ff79c6>.</span>band_part(tf<span style=color:#ff79c6>.</span>ones((size, size)), <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> mask  <span style=color:#6272a4># (seq_len, seq_len)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>3</span>))
</span></span><span style=display:flex><span>temp <span style=color:#ff79c6>=</span> create_look_ahead_mask(x<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;tf.random.uniform((1, 3))的前瞻遮挡: &#39;</span>, temp)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>scaled_dot_product_attention</span>(q, k, v, mask):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;计算注意力权重。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    q, k, v必须具有匹配的后置维度，k, v必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    虽然mask根据其类型（填充或前瞻）有不同的形状，但是mask必须能进行广播转换以便求和。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    参数:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>      q: 请求的形状 == (..., seq_len_q, depth)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>      k: 主键的形状 == (..., seq_len_k, depth)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>      v: 数值的形状 == (..., seq_len_v, depth)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>      mask: Float 张量，其形状能转换成(..., seq_len_q, seq_len_k)，默认为None。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    返回值:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>      输出，注意力权重
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    matmul_qk <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>matmul(q, k, transpose_b<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)  <span style=color:#6272a4># (..., seq_len_q, seq_len_k)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 缩放matmul_qk</span>
</span></span><span style=display:flex><span>    dk <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>cast(tf<span style=color:#ff79c6>.</span>shape(k)[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>], tf<span style=color:#ff79c6>.</span>float32)
</span></span><span style=display:flex><span>    scaled_attention_logits <span style=color:#ff79c6>=</span> matmul_qk <span style=color:#ff79c6>/</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>sqrt(dk)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将mask加入到缩放后的张量上</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> mask <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        scaled_attention_logits <span style=color:#ff79c6>+=</span> (mask <span style=color:#ff79c6>*</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1e9</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># softmax在最后一个轴（seq_len_k）上归一化，因此分数相加等于1</span>
</span></span><span style=display:flex><span>    attention_weights <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>nn<span style=color:#ff79c6>.</span>softmax(scaled_attention_logits, axis<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>)  <span style=color:#6272a4># (..., seq_len_q, seq_len_k)</span>
</span></span><span style=display:flex><span>    output <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>matmul(attention_weights, v)  <span style=color:#6272a4># (..., seq_len_q, depth)</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> output, attention_weights
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 当softmax在K上进行归一化后，它的值决定了分配到Q的重要程度</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 输出表示注意力权重和 V(数值)向量的乘积。这确保了要关注的词保持原样，而无关的词将被清除掉</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>print_out</span>(q, k, v):
</span></span><span style=display:flex><span>    temp_out, temp_attn <span style=color:#ff79c6>=</span> scaled_dot_product_attention(q, k, v, <span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Attention weights are:&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(temp_attn)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Output is:&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(temp_out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>np<span style=color:#ff79c6>.</span>set_printoptions(suppress<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>temp_k <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>constant([[<span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>10</span>], [<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>10</span>]], dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>float32)  <span style=color:#6272a4># (4, 3)</span>
</span></span><span style=display:flex><span>temp_v <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>constant([[<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>0</span>], [<span style=color:#bd93f9>100</span>, <span style=color:#bd93f9>5</span>], [<span style=color:#bd93f9>1000</span>, <span style=color:#bd93f9>6</span>]], dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>float32)  <span style=color:#6272a4># (4, 2)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这条query符合第二个key，因此返回了第二个value</span>
</span></span><span style=display:flex><span>temp_q <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>constant([[<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>10</span>, <span style=color:#bd93f9>0</span>]], dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>float32)  <span style=color:#6272a4># (1, 3)</span>
</span></span><span style=display:flex><span>print_out(temp_q, temp_k, temp_v)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 多头注意力(Multi-head attention)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>MultiHeadAttention</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Layer):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, d_model, num_heads):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(MultiHeadAttention, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_heads <span style=color:#ff79c6>=</span> num_heads
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model <span style=color:#ff79c6>=</span> d_model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>assert</span> d_model <span style=color:#ff79c6>%</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_heads <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>depth <span style=color:#ff79c6>=</span> d_model <span style=color:#ff79c6>//</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_heads
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wq <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(d_model)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wk <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(d_model)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wv <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dense <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>split_heads</span>(<span style=font-style:italic>self</span>, x, batch_size):
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        输入维度: (batch_size, seq_len, d_model)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        分拆最后一个维度到(num_heads, depth)，转置结果使得输出维度为(batch_size, num_heads, seq_len, depth)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>reshape(x, (batch_size, <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_heads, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>depth))
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>transpose(x, perm<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>3</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, v, k, q, mask):  <span style=color:#6272a4># seq_len_q == seq_len_k == seq_len_v</span>
</span></span><span style=display:flex><span>        batch_size <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>shape(q)[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        q <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wq(q)  <span style=color:#6272a4># (batch_size, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        k <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wk(k)  <span style=color:#6272a4># (batch_size, seq_len, d_model)</span>
</span></span><span style=display:flex><span>        v <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>wv(v)  <span style=color:#6272a4># (batch_size, seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        q <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>split_heads(q, batch_size)  <span style=color:#6272a4># (batch_size, num_heads, seq_len_q, depth)</span>
</span></span><span style=display:flex><span>        k <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>split_heads(k, batch_size)  <span style=color:#6272a4># (batch_size, num_heads, seq_len_k, depth)</span>
</span></span><span style=display:flex><span>        v <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>split_heads(v, batch_size)  <span style=color:#6272a4># (batch_size, num_heads, seq_len_v, depth)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
</span></span><span style=display:flex><span>        scaled_attention, attention_weights <span style=color:#ff79c6>=</span> scaled_dot_product_attention(q, k, v, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        scaled_attention <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>transpose(scaled_attention, perm<span style=color:#ff79c6>=</span>[<span style=color:#bd93f9>0</span>, <span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>3</span>])
</span></span><span style=display:flex><span>        <span style=color:#6272a4># scaled_attention shape after transpose: (batch_size, seq_len_q, num_heads, depth)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 将多个attention head进行concat，由于是在一个数组中的，只需reshape即可</span>
</span></span><span style=display:flex><span>        concat_attention <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>reshape(scaled_attention, (batch_size, <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model))
</span></span><span style=display:flex><span>        <span style=color:#6272a4># concat_attention shape: (batch_size, seq_len_q, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dense(concat_attention)  <span style=color:#6272a4># (batch_size, seq_len_q, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> output, attention_weights
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>temp_mha <span style=color:#ff79c6>=</span> MultiHeadAttention(d_model<span style=color:#ff79c6>=</span><span style=color:#bd93f9>512</span>, num_heads<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8</span>)
</span></span><span style=display:flex><span>y <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>1</span>, <span style=color:#bd93f9>60</span>, <span style=color:#bd93f9>512</span>))  <span style=color:#6272a4># (batch_size, encoder_sequence, d_model)</span>
</span></span><span style=display:flex><span>out, attn <span style=color:#ff79c6>=</span> temp_mha(y, k<span style=color:#ff79c6>=</span>y, q<span style=color:#ff79c6>=</span>y, mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(out<span style=color:#ff79c6>.</span>shape, attn<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 点式前馈网络(Point wise feed forward network)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 点式前馈网络由两层全联接层组成，两层之间有一个ReLU激活函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>point_wise_feed_forward_network</span>(d_model, dff):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Sequential([
</span></span><span style=display:flex><span>        tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(dff, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>),  <span style=color:#6272a4># input shape: (batch_size, seq_len, dff)</span>
</span></span><span style=display:flex><span>        tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(d_model)  <span style=color:#6272a4># output shape: (batch_size, seq_len, d_model)</span>
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_ffn <span style=color:#ff79c6>=</span> point_wise_feed_forward_network(<span style=color:#bd93f9>512</span>, <span style=color:#bd93f9>2048</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(sample_ffn(tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>50</span>, <span style=color:#bd93f9>512</span>)))<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 编码器层</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个编码器层包括以下子层：</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 1. 多头注意力(有填充遮挡)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 2. 点式前馈网络(Point wise feed forward networks)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个子层在其周围有一个残差连接，然后进行层归一化。残差连接有助于避免深度网络中的梯度消失问题。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model(最后一个)维度完成的。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Transformer中有N个编码器层。</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>EncoderLayer</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Layer):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, d_model, num_heads, dff, rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(EncoderLayer, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha <span style=color:#ff79c6>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ffn <span style=color:#ff79c6>=</span> point_wise_feed_forward_network(d_model, dff)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>LayerNormalization(epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>LayerNormalization(epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x, training, mask):
</span></span><span style=display:flex><span>        attn_output, _ <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha(x, x, x, mask)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        attn_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout1(attn_output, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>        out1 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm1(x <span style=color:#ff79c6>+</span> attn_output)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ffn_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ffn(out1)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        ffn_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout2(ffn_output, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>        out2 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm2(out1 <span style=color:#ff79c6>+</span> ffn_output)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> out2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_encoder_layer <span style=color:#ff79c6>=</span> EncoderLayer(<span style=color:#bd93f9>512</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>2048</span>)  <span style=color:#6272a4># 定义编码器层的一个实例</span>
</span></span><span style=display:flex><span>sample_encoder_layer_output <span style=color:#ff79c6>=</span> sample_encoder_layer(tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>43</span>, <span style=color:#bd93f9>512</span>)), <span style=color:#ff79c6>False</span>, <span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;编码器层的输出shape: &#39;</span>, sample_encoder_layer_output<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 解码器层(Decoder layer)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个解码器层包括以下子层：</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 1. 遮挡的多头注意力(前瞻遮挡和填充遮挡)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 2. 多头注意力(用填充遮挡)。V(数值)和K(主键)接收编码器输出作为输入。Q(请求)接收遮挡的多头注意力子层的输出。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 3. 点式前馈网络</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 每个子层在其周围有一个残差连接，然后进行层归一化。每个子层的输出是LayerNorm(x + Sublayer(x))。归一化是在d_model(最后一个)维度完成的</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Transformer中共有N个解码器层。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 当Q接收到解码器的第一个注意力块的输出，并且K接收到编码器的输出时，注意力权重表示根据编码器的输出赋予解码器输入的重要性</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 换一种说法，解码器通过查看编码器输出和对其自身输出的自注意力，预测下一个词。参看按比缩放的点积注意力部分的演示</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>DecoderLayer</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Layer):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, d_model, num_heads, dff, rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(DecoderLayer, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha1 <span style=color:#ff79c6>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha2 <span style=color:#ff79c6>=</span> MultiHeadAttention(d_model, num_heads)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ffn <span style=color:#ff79c6>=</span> point_wise_feed_forward_network(d_model, dff)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>LayerNormalization(epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>LayerNormalization(epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm3 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>LayerNormalization(epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout3 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x, enc_output, training, look_ahead_mask, padding_mask):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># enc_output.shape == (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        attn1, attn_weights_block1 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha1(x, x, x, look_ahead_mask)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        attn1 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout1(attn1, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>        out1 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm1(attn1 <span style=color:#ff79c6>+</span> x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        attn2, attn_weights_block2 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>mha2(enc_output, enc_output, out1,
</span></span><span style=display:flex><span>                                               padding_mask)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        attn2 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout2(attn2, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>        out2 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm2(attn2 <span style=color:#ff79c6>+</span> out1)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ffn_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ffn(out2)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        ffn_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout3(ffn_output, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>        out3 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layernorm3(ffn_output <span style=color:#ff79c6>+</span> out2)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> out3, attn_weights_block1, attn_weights_block2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 测试解码器层</span>
</span></span><span style=display:flex><span>sample_decoder_layer <span style=color:#ff79c6>=</span> DecoderLayer(<span style=color:#bd93f9>512</span>, <span style=color:#bd93f9>8</span>, <span style=color:#bd93f9>2048</span>)
</span></span><span style=display:flex><span>sample_decoder_layer_output, _, _ <span style=color:#ff79c6>=</span> sample_decoder_layer(tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>50</span>, <span style=color:#bd93f9>512</span>)), sample_encoder_layer_output, <span style=color:#ff79c6>False</span>, <span style=color:#ff79c6>None</span>, <span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;解码器层的输出shape: &#39;</span>, sample_decoder_layer_output<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 编码器(Encoder)包括：</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 1. 输入嵌入(Input Embedding)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 2. 位置编码(Positional Encoding)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 3. N个编码器层(encoder layers)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 输入经过嵌入(embedding)后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Encoder</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Layer):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, num_layers, d_model, num_heads, dff, input_vocab_size,
</span></span><span style=display:flex><span>                 maximum_position_encoding, rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(Encoder, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model <span style=color:#ff79c6>=</span> d_model
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_layers <span style=color:#ff79c6>=</span> num_layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>embedding <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(input_vocab_size, d_model)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_encoding <span style=color:#ff79c6>=</span> positional_encoding(maximum_position_encoding, <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>enc_layers <span style=color:#ff79c6>=</span> [EncoderLayer(d_model, num_heads, dff, rate) <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(num_layers)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x, training, mask):
</span></span><span style=display:flex><span>        seq_len <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>shape(x)[<span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 将嵌入和位置编码相加</span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>embedding(x)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>*=</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>sqrt(tf<span style=color:#ff79c6>.</span>cast(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model, tf<span style=color:#ff79c6>.</span>float32))
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>+=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_encoding[:, :seq_len, :]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout(x, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_layers):
</span></span><span style=display:flex><span>            x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>enc_layers[i](x, training, mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_encoder <span style=color:#ff79c6>=</span> Encoder(num_layers<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, d_model<span style=color:#ff79c6>=</span><span style=color:#bd93f9>512</span>, num_heads<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8</span>, dff<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2048</span>, input_vocab_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8500</span>, maximum_position_encoding<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10000</span>)
</span></span><span style=display:flex><span>sample_encoder_output <span style=color:#ff79c6>=</span> sample_encoder(tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>62</span>)), training<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>, mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;编码器的输出shape: &#39;</span>, sample_encoder_output<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (batch_size, input_seq_len, d_model)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 解码器(Decoder)包括：</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 输出嵌入(Output Embedding)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 位置编码(Positional Encoding)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># N 个解码器层(decoder layers)</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 目标(target)经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Decoder</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Layer):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, num_layers, d_model, num_heads, dff, target_vocab_size,
</span></span><span style=display:flex><span>                 maximum_position_encoding, rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(Decoder, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model <span style=color:#ff79c6>=</span> d_model
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_layers <span style=color:#ff79c6>=</span> num_layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>embedding <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Embedding(target_vocab_size, d_model)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_encoding <span style=color:#ff79c6>=</span> positional_encoding(maximum_position_encoding, d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dec_layers <span style=color:#ff79c6>=</span> [DecoderLayer(d_model, num_heads, dff, rate)
</span></span><span style=display:flex><span>                           <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(num_layers)]
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dropout(rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x, enc_output, training, look_ahead_mask, padding_mask):
</span></span><span style=display:flex><span>        seq_len <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>shape(x)[<span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>        attention_weights <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>embedding(x)  <span style=color:#6272a4># (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>*=</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>sqrt(tf<span style=color:#ff79c6>.</span>cast(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model, tf<span style=color:#ff79c6>.</span>float32))
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>+=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_encoding[:, :seq_len, :]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout(x, training<span style=color:#ff79c6>=</span>training)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>num_layers):
</span></span><span style=display:flex><span>            x, block1, block2 <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            attention_weights[<span style=color:#f1fa8c>&#39;decoder_layer</span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>_block1&#39;</span><span style=color:#ff79c6>.</span>format(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)] <span style=color:#ff79c6>=</span> block1
</span></span><span style=display:flex><span>            attention_weights[<span style=color:#f1fa8c>&#39;decoder_layer</span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>_block2&#39;</span><span style=color:#ff79c6>.</span>format(i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)] <span style=color:#ff79c6>=</span> block2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># x.shape == (batch_size, target_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x, attention_weights
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_decoder <span style=color:#ff79c6>=</span> Decoder(num_layers<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, d_model<span style=color:#ff79c6>=</span><span style=color:#bd93f9>512</span>, num_heads<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8</span>, dff<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2048</span>, target_vocab_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8000</span>, maximum_position_encoding<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5000</span>)
</span></span><span style=display:flex><span>output, attn <span style=color:#ff79c6>=</span> sample_decoder(tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>26</span>)),
</span></span><span style=display:flex><span>                              enc_output<span style=color:#ff79c6>=</span>sample_encoder_output,
</span></span><span style=display:flex><span>                              training<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>, look_ahead_mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>,
</span></span><span style=display:flex><span>                              padding_mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;解码器的输出shape: &#39;</span>, output<span style=color:#ff79c6>.</span>shape, attn[<span style=color:#f1fa8c>&#39;decoder_layer2_block2&#39;</span>]<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建Transformer</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Transformer包括编码器，解码器和最后的线性层。解码器的输出是线性层的输入，返回线性层的输出。</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Transformer</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Model):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.1</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(Transformer, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder <span style=color:#ff79c6>=</span> Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder <span style=color:#ff79c6>=</span> Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_layer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(target_vocab_size)  <span style=color:#6272a4># 用于预测的最后一层</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):
</span></span><span style=display:flex><span>        enc_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder(inp, training, enc_padding_mask)  <span style=color:#6272a4># (batch_size, inp_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
</span></span><span style=display:flex><span>        dec_output, attention_weights <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)
</span></span><span style=display:flex><span>        final_output <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_layer(dec_output)  <span style=color:#6272a4># (batch_size, tar_seq_len, target_vocab_size)</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> final_output, attention_weights
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample_transformer <span style=color:#ff79c6>=</span> Transformer(num_layers<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>, d_model<span style=color:#ff79c6>=</span><span style=color:#bd93f9>512</span>, num_heads<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8</span>, dff<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2048</span>, input_vocab_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8500</span>, target_vocab_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>8000</span>, pe_input<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10000</span>, pe_target<span style=color:#ff79c6>=</span><span style=color:#bd93f9>6000</span>)
</span></span><span style=display:flex><span>temp_input <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>62</span>))
</span></span><span style=display:flex><span>temp_target <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>random<span style=color:#ff79c6>.</span>uniform((<span style=color:#bd93f9>64</span>, <span style=color:#bd93f9>26</span>))
</span></span><span style=display:flex><span>fn_out, _ <span style=color:#ff79c6>=</span> sample_transformer(temp_input, temp_target, training<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>, enc_padding_mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, look_ahead_mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, dec_padding_mask<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Transformer输出的shape: &#39;</span>, fn_out<span style=color:#ff79c6>.</span>shape)  <span style=color:#6272a4># (batch_size, tar_seq_len, target_vocab_size)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 配置超参数（hyperparameters）</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 为了让本示例小且相对较快，已经减小了num_layers、 d_model和dff的值</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Transformer的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048</span>
</span></span><span style=display:flex><span>num_layers <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>
</span></span><span style=display:flex><span>d_model <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>
</span></span><span style=display:flex><span>dff <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>512</span>
</span></span><span style=display:flex><span>num_heads <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input_vocab_size <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(tokenizer_pt<span style=color:#ff79c6>.</span>word_index) <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span>target_vocab_size <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(tokenizer_en<span style=color:#ff79c6>.</span>word_index) <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span>dropout_rate <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 优化器(Optimizer)，根据论文中的公式，将Adam优化器与自定义的学习速率调度程序(scheduler)配合使用</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>CustomSchedule</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>schedules<span style=color:#ff79c6>.</span>LearningRateSchedule):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, d_model, warmup_steps<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4000</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(CustomSchedule, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model <span style=color:#ff79c6>=</span> d_model
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>cast(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model, tf<span style=color:#ff79c6>.</span>float32)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>warmup_steps <span style=color:#ff79c6>=</span> warmup_steps
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__call__</span>(<span style=font-style:italic>self</span>, step):
</span></span><span style=display:flex><span>        arg1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>rsqrt(step)
</span></span><span style=display:flex><span>        arg2 <span style=color:#ff79c6>=</span> step <span style=color:#ff79c6>*</span> (<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>warmup_steps <span style=color:#ff79c6>**</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1.5</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>rsqrt(<span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>d_model) <span style=color:#ff79c6>*</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>minimum(arg1, arg2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>learning_rate <span style=color:#ff79c6>=</span> CustomSchedule(d_model)
</span></span><span style=display:flex><span>optimizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam(learning_rate, beta_1<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.9</span>, beta_2<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.98</span>, epsilon<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-9</span>)  <span style=color:#6272a4># 定义优化器</span>
</span></span><span style=display:flex><span>temp_learning_rate_schedule <span style=color:#ff79c6>=</span> CustomSchedule(d_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>ylabel(<span style=color:#f1fa8c>&#34;Learning Rate&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>xlabel(<span style=color:#f1fa8c>&#34;Train Step&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>plot(temp_learning_rate_schedule(tf<span style=color:#ff79c6>.</span>range(<span style=color:#bd93f9>40000</span>, dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>float32)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 损失函数与指标</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 由于目标序列是填充过的，因此在计算损失函数时，应用填充遮挡非常重要</span>
</span></span><span style=display:flex><span>loss_object <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>SparseCategoricalCrossentropy(from_logits<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, reduction<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>loss_function</span>(real, pred):
</span></span><span style=display:flex><span>    mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>logical_not(tf<span style=color:#ff79c6>.</span>math<span style=color:#ff79c6>.</span>equal(real, <span style=color:#bd93f9>0</span>))
</span></span><span style=display:flex><span>    loss_ <span style=color:#ff79c6>=</span> loss_object(real, pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>cast(mask, dtype<span style=color:#ff79c6>=</span>loss_<span style=color:#ff79c6>.</span>dtype)
</span></span><span style=display:flex><span>    loss_ <span style=color:#ff79c6>*=</span> mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>reduce_mean(loss_)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>Mean(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;train_loss&#39;</span>)
</span></span><span style=display:flex><span>train_accuracy <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>SparseCategoricalAccuracy(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;train_accuracy&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练与检查点(Training and checkpointing)</span>
</span></span><span style=display:flex><span>transformer <span style=color:#ff79c6>=</span> Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input<span style=color:#ff79c6>=</span>input_vocab_size, pe_target<span style=color:#ff79c6>=</span>target_vocab_size, rate<span style=color:#ff79c6>=</span>dropout_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_masks</span>(inp, tar):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 编码器填充遮挡</span>
</span></span><span style=display:flex><span>    enc_padding_mask <span style=color:#ff79c6>=</span> create_padding_mask(inp)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 在解码器的第二个注意力模块使用，该填充遮挡用于遮挡编码器的输出</span>
</span></span><span style=display:flex><span>    dec_padding_mask <span style=color:#ff79c6>=</span> create_padding_mask(inp)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 在解码器的第一个注意力模块使用，用于填充(pad)和遮挡(mask)解码器获取到的输入的后续标记(future tokens)</span>
</span></span><span style=display:flex><span>    look_ahead_mask <span style=color:#ff79c6>=</span> create_look_ahead_mask(tf<span style=color:#ff79c6>.</span>shape(tar)[<span style=color:#bd93f9>1</span>])
</span></span><span style=display:flex><span>    dec_target_padding_mask <span style=color:#ff79c6>=</span> create_padding_mask(tar)
</span></span><span style=display:flex><span>    combined_mask <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>maximum(dec_target_padding_mask, look_ahead_mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> enc_padding_mask, combined_mask, dec_padding_mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建检查点的路径和检查点管理器(manager)。这将用于在每n个周期(epochs)保存检查点</span>
</span></span><span style=display:flex><span>checkpoint_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;checkpoints/pt_en&#34;</span>
</span></span><span style=display:flex><span>ckpt <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>train<span style=color:#ff79c6>.</span>Checkpoint(transformer<span style=color:#ff79c6>=</span>transformer, optimizer<span style=color:#ff79c6>=</span>optimizer)
</span></span><span style=display:flex><span>ckpt_manager <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>train<span style=color:#ff79c6>.</span>CheckpointManager(ckpt, checkpoint_path, max_to_keep<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 如果检查点存在，则恢复最新的检查点。</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> ckpt_manager<span style=color:#ff79c6>.</span>latest_checkpoint:
</span></span><span style=display:flex><span>    ckpt<span style=color:#ff79c6>.</span>restore(ckpt_manager<span style=color:#ff79c6>.</span>latest_checkpoint)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;模型从检查点中恢复&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 目标(target)被分成了 tar_inp 和 tar_real。tar_inp作为输入传递到解码器。tar_real是位移了1的同一个输入：在tar_inp中的每个位置，tar_real包含了应该被预测到的下一个标记</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 例如，sentence = &#34;SOS A lion in the jungle is sleeping EOS&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># tar_inp = &#34;SOS A lion in the jungle is sleeping&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># tar_real = &#34;A lion in the jungle is sleeping EOS&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Transformer是一个自回归模型：它一次作一个部分的预测，然后使用到目前为止的自身的输出来决定下一步要做什么。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 在训练过程中，本示例使用了teacher-forcing的方法。无论模型在当前时间步骤下预测出什么，teacher-forcing方法都会将真实的输出传递到下一个时间步骤上。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 当 transformer预测每个词时，自注意力功能使它能够查看输入序列中前面的单词，从而更好地预测下一个单词。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 为了防止模型在期望的输出上达到峰值，模型使用了前瞻遮挡(look-ahead mask)。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 该@tf.function将追踪-编译train_step到TF图中，以更快执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 批次大小(最后一批次较小)导致的再追踪，使用input_signature指定更多的通用形状。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_step_signature <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>TensorSpec(shape<span style=color:#ff79c6>=</span>(<span style=color:#ff79c6>None</span>, <span style=color:#ff79c6>None</span>), dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>int64),
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>TensorSpec(shape<span style=color:#ff79c6>=</span>(<span style=color:#ff79c6>None</span>, <span style=color:#ff79c6>None</span>), dtype<span style=color:#ff79c6>=</span>tf<span style=color:#ff79c6>.</span>int64),
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># @tf.function(input_signature=train_step_signature)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>train_step</span>(inp, tar):
</span></span><span style=display:flex><span>    tar_inp <span style=color:#ff79c6>=</span> tar[:, :<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>    tar_real <span style=color:#ff79c6>=</span> tar[:, <span style=color:#bd93f9>1</span>:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    enc_padding_mask, combined_mask, dec_padding_mask <span style=color:#ff79c6>=</span> create_masks(inp, tar_inp)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> tf<span style=color:#ff79c6>.</span>GradientTape() <span style=color:#ff79c6>as</span> tape:
</span></span><span style=display:flex><span>        predictions, _ <span style=color:#ff79c6>=</span> transformer(inp, tar_inp, <span style=color:#ff79c6>True</span>, enc_padding_mask, combined_mask, dec_padding_mask)
</span></span><span style=display:flex><span>        loss <span style=color:#ff79c6>=</span> loss_function(tar_real, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    gradients <span style=color:#ff79c6>=</span> tape<span style=color:#ff79c6>.</span>gradient(loss, transformer<span style=color:#ff79c6>.</span>trainable_variables)
</span></span><span style=display:flex><span>    optimizer<span style=color:#ff79c6>.</span>apply_gradients(<span style=color:#8be9fd;font-style:italic>zip</span>(gradients, transformer<span style=color:#ff79c6>.</span>trainable_variables))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_loss(loss)
</span></span><span style=display:flex><span>    train_accuracy(tar_real, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>train</span>(EPOCHS):
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;模型训练&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 葡萄牙语作为输入语言，英语为目标语言</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> epoch <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(EPOCHS):
</span></span><span style=display:flex><span>        start <span style=color:#ff79c6>=</span> time<span style=color:#ff79c6>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        train_loss<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>        train_accuracy<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># inp -&gt; portuguese, tar -&gt; english</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> (batch, (inp, tar)) <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(train_dataset):
</span></span><span style=display:flex><span>            train_step(inp, tar)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> batch <span style=color:#ff79c6>%</span> <span style=color:#bd93f9>10</span> <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Epoch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c> Batch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c> Loss </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c> Accuracy </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>                    epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, batch, train_loss<span style=color:#ff79c6>.</span>result(), train_accuracy<span style=color:#ff79c6>.</span>result()))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> (epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>) <span style=color:#ff79c6>%</span> <span style=color:#bd93f9>2</span> <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>            ckpt_save_path <span style=color:#ff79c6>=</span> ckpt_manager<span style=color:#ff79c6>.</span>save()
</span></span><span style=display:flex><span>            <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Saving checkpoint for epoch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c> at </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, ckpt_save_path))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Epoch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c> Loss </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c> Accuracy </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, train_loss<span style=color:#ff79c6>.</span>result(), train_accuracy<span style=color:#ff79c6>.</span>result()))
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Time taken for 1 epoch: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c> secs</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(time<span style=color:#ff79c6>.</span>time() <span style=color:#ff79c6>-</span> start))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>evaluate</span>(inp_sentence):
</span></span><span style=display:flex><span>    start_token <span style=color:#ff79c6>=</span> tokenizer_pt<span style=color:#ff79c6>.</span>word_index[<span style=color:#f1fa8c>&#39;&lt;start&gt;&#39;</span>]
</span></span><span style=display:flex><span>    end_token <span style=color:#ff79c6>=</span> tokenizer_pt<span style=color:#ff79c6>.</span>word_index[<span style=color:#f1fa8c>&#39;&lt;end&gt;&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    inp_sentence_list <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> inp_sentence<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39; &#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> i <span style=color:#ff79c6>in</span> tokenizer_pt<span style=color:#ff79c6>.</span>word_index<span style=color:#ff79c6>.</span>keys():
</span></span><span style=display:flex><span>            inp_sentence_list<span style=color:#ff79c6>.</span>append(tokenizer_pt<span style=color:#ff79c6>.</span>word_index[i])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># inp sentence is portuguese, hence adding the start and end token</span>
</span></span><span style=display:flex><span>    inp_sentence <span style=color:#ff79c6>=</span> [start_token] <span style=color:#ff79c6>+</span> inp_sentence_list <span style=color:#ff79c6>+</span> [end_token]
</span></span><span style=display:flex><span>    encoder_input <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>expand_dims(inp_sentence, <span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># as the target is english, the first word to the transformer should be the english &lt;start&gt; token.</span>
</span></span><span style=display:flex><span>    decoder_input <span style=color:#ff79c6>=</span> [tokenizer_en<span style=color:#ff79c6>.</span>word_index[<span style=color:#f1fa8c>&#39;&lt;start&gt;&#39;</span>]]
</span></span><span style=display:flex><span>    output <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>expand_dims(decoder_input, <span style=color:#bd93f9>0</span>)  <span style=color:#6272a4># 变为二维数据</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>50</span>):
</span></span><span style=display:flex><span>        enc_padding_mask, combined_mask, dec_padding_mask <span style=color:#ff79c6>=</span> create_masks(encoder_input, output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># predictions.shape == (batch_size, seq_len, vocab_size)</span>
</span></span><span style=display:flex><span>        predictions, attention_weights <span style=color:#ff79c6>=</span> transformer(encoder_input,
</span></span><span style=display:flex><span>                                                     output,
</span></span><span style=display:flex><span>                                                     <span style=color:#ff79c6>False</span>,
</span></span><span style=display:flex><span>                                                     enc_padding_mask,
</span></span><span style=display:flex><span>                                                     combined_mask,
</span></span><span style=display:flex><span>                                                     dec_padding_mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># select the last word from the seq_len dimension</span>
</span></span><span style=display:flex><span>        predictions <span style=color:#ff79c6>=</span> predictions[:, <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>:, :]  <span style=color:#6272a4># (batch_size, 1, vocab_size)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        predicted_id <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>cast(tf<span style=color:#ff79c6>.</span>argmax(predictions, axis<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>), tf<span style=color:#ff79c6>.</span>int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># concatentate the predicted_id to the output which is given to the decoder as its input.</span>
</span></span><span style=display:flex><span>        output <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>concat([output, predicted_id], axis<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> tf<span style=color:#ff79c6>.</span>squeeze(output, axis<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>), attention_weights
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>translate</span>(sentence):
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;进行一次翻译&#39;</span>)
</span></span><span style=display:flex><span>    result, attention_weights <span style=color:#ff79c6>=</span> evaluate(sentence)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(result<span style=color:#ff79c6>.</span>numpy())
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> result[<span style=color:#bd93f9>1</span>:]  <span style=color:#6272a4># 去掉&lt;start&gt;开始标记</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(result)):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> result[i] <span style=color:#ff79c6>==</span> tokenizer_en<span style=color:#ff79c6>.</span>word_index[<span style=color:#f1fa8c>&#39;&lt;end&gt;&#39;</span>]:
</span></span><span style=display:flex><span>            result <span style=color:#ff79c6>=</span> result[:i]  <span style=color:#6272a4># 去掉&lt;end&gt;结束标记</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    predicted_sentence <span style=color:#ff79c6>=</span> [tokenizer_en<span style=color:#ff79c6>.</span>index_word[i] <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> result <span style=color:#ff79c6>if</span> i <span style=color:#ff79c6>&lt;=</span> <span style=color:#8be9fd;font-style:italic>len</span>(tokenizer_en<span style=color:#ff79c6>.</span>word_index)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Input: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(sentence))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Predicted translation: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>format(<span style=color:#f1fa8c>&#34; &#34;</span><span style=color:#ff79c6>.</span>join(predicted_sentence)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>__name__</span> <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    mode <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;translate&#39;</span>  <span style=color:#6272a4># train/translate</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> mode <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;train&#39;</span>:
</span></span><span style=display:flex><span>        train(EPOCHS<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>elif</span> mode <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;translate&#39;</span>:
</span></span><span style=display:flex><span>        pt_sent <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;não gosto de maçã e pêssego .&#34;</span>
</span></span><span style=display:flex><span>        en_sent <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;i don&#39;t like apple and peach .&#34;</span>
</span></span><span style=display:flex><span>        translate(pt_sent)
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Real translation: &#34;</span>, en_sent)
</span></span></code></pre></div><h1 id=bert>BERT</h1><p><strong>BERT(bidirectional encoder representations from transformers)<strong>是一种语言模型，用于从无标签文本中学习词的</strong>深度双向表示</strong>，这个过程称为<strong>预训练(pre-train)</strong>。在BERT预训练后，可以附加下游任务，通过<strong>微调(fine-tuning)</strong> 的方式来完成NLP任务，如句子级别的自然语言推断、解释任务以及单词级别的NER、QA等任务。</p><p>目前有两种将预训练语言表示应用于下游任务的策略，即<strong>基于特征(feature-based)<strong>的方法及</strong>微调(fine-tuning)</strong>。二者的典型代表分别是<strong>ELMo</strong>和<strong>GPT</strong>。两种方法在预训练过程中使用的目标函数都是相同的，都是使用单向的语言模型取学习一般的语言表示(ELMo是由两个单向的LSTM拼接实现的，本质还是单向模型)。</p><p>BERT的作者认为，ELMo和GPT这类模型的<strong>单向性</strong>限制了下游任务的效果。BERT通过两种方式进行预训练：(1) <strong>masked language model, MLM</strong>：将输入中的标记(token)随机mask一部分，使其不可见，然后让模型去基于这些被遮盖的标记的上下文来预测这些单词的id。(2) <strong>next sentence prediction, NSP</strong>：给出两个句子A和B，让模型去判断B是否是A的下一个句子。</p><p>BERT模型的两种预训练方式以及其在下游任务中的应用示意图如下所示：</p><div align=center><img src=/Kimages/4/image-20200425201254058.png style=zoom:45%></div><p>BERT模型与ELMo和GPT的区别如下所示：</p><div align=center><img src=/Kimages/4/image-20200426101648120.png style=zoom:40%></div><h2 id=模型结构>模型结构</h2><p>BERT模型本质上是一个多层的双向Transformer的编码器。设Transformer编码器层的数量为$L$，隐含层单元的数量为$H$，注意力头的数量为$A$。论文中提出了两种尺寸的BERT，即$\bold{BERT}_{\bold{BASE}}(L=12,H=768,A=12)$，以及$\bold{BERT}_{\bold{LARGE}}(L=12,H=1024,A=16)$。</p><p>为了让BERT能处理更多类型的下游任务，输入数据可以包含两种形式，即一个单独的句子以及一个句子对(例如question和answer)。文章使用wordpiece embedding对输入单词进行表示，词汇表大小为30,000。每个句子的开头标记都是一个特殊类别标记“[CLS]”，<strong>最终对应这个标记的隐含状态便是用于分类任务的整个句子的表示</strong>。包含两个输入句子的桔子堆被合并为一个序列，并使用两种方法对这两个句子进行区分：(1) 在两个句子之间添加一个特殊标记[SEP]；(2) 为每个标记添加一个学习得到的嵌入，该嵌入表明每个标记属于句子A还是句子B。记$E$为输入标记的嵌入，$C \in \mathbb R^H$为[CLS]对应的最终隐含层向量，第$i$个输入标记对应的最终隐含层向量为$T_i \in \mathbb R^H$。</p><p>对于一个给定的标记，<strong>其输入表示为token embedding、segment embedding和position embedding之和</strong>。</p><div align=center><img src=/Kimages/4/image-20200425210720499.png style=zoom:30%></div><h2 id=预训练bert>预训练BERT</h2><p>BERT通过masked language model和next sentence prediction两种方式进行预训练。</p><p>(1) <strong>masked language model, MLM</strong>：为了训练一个深度双向的表示，论文简单地将输入标记序列中的一些单词随机遮盖后让BERT完成预测这些单词的任务。被遮盖的单词最终的隐含层向量被送入一个输出的softmax层，通过训练的方式不断提高模型预测的准确率。实验遮盖了<strong>每个序列中15%的wordpiece标记</strong>。MLM产生的一个缺点是，[MASK]标记导致了预训练与微调的不匹配，因为[MASK]标记不出现在微调过程中。为了解决这个问题，对于被随机选中第$i$个masked标记，其<strong>以80%的可能性被替换为[MASK]标记，10%的可能性被替换为一个随机的标记，10%的可能性保持不变</strong>。然后，$T_i$被用于预测原始标记。</p><p>(2) <strong>next sentence prediction, NSP</strong>：许多例如QA、NLI的下游任务基于理解句子之间的关系，这种关系无法直接被语言模型捕获。NSP任务可用于捕获句子之间的关系，其具体做法如下：从每个预训练样本中选择句子A和B时，有50%的可能性句子B确实是跟在句子A后面的句子(被标记为<strong>IsNext</strong>)，还有50%的可能性句子B是从语料库中随机选取的(被标记为<strong>NotNext</strong>)。在BERT中，$C$([CLS]标记对应位置的最终隐含层向量)用于NSP。这个预训练方法对QA和NLI任务非常有效。</p><p>预训练的数据来自BooksCorpus(300M words)和Wikipedia(2,500M words)。使用像维基百科这样文档级别的语料库非常有帮助，因为其能够使得BERT更好地捕获长程依赖。</p><h2 id=微调bert>微调BERT</h2><p>仅需要改变输入和输出，BERT便能用于完成很多种类的NLP任务。在微调的过程中，BERT中的全部参数都会以端到端的方式进行微调。下图展示了对于不同任务，BERT输入和输出的变化：</p><div align=center><img src=/Kimages/4/image-20200426100129779.png style=zoom:30%></div><div align=center><img src=/Kimages/4/image-20200426100154337.png style=zoom:30%></div><h2 id=实验结果>实验结果</h2><p>在不同NLP任务上的实验结果如下：</p><p>(1) GLUE</p><div align=center><img src=/Kimages/4/image-20200426100615444.png style=zoom:40%></div><p>(2) SQuAD 1.1</p><div align=center><img src=/Kimages/4/image-20200426100706463.png style=zoom:40%></div><p>(3) SQuAD 2.0</p><div align=center><img src=/Kimages/4/image-20200426100809998.png style=zoom:40%></div><p>(4) SWAG</p><div align=center><img src=/Kimages/4/image-20200426100834569.png style=zoom:40%></div><p>(5) 预训练方式调整对实验结果的影响</p><div align=center><img src=/Kimages/4/image-20200426101035301.png style=zoom:40%></div><p>(6) 网络结构调整对实验结果的影响</p><div align=center><img src=/Kimages/4/image-20200426101208031.png style=zoom:40%></div><p>(7) 命名实体识别</p><div align=center><img src=/Kimages/4/image-20200426101335649.png style=zoom:40%></div><h1 id=ernie>ERNIE</h1><h2 id=ernie-10>ERNIE 1.0</h2><p>Google提出的BERT模型，利用Transformer的多层self-attention双向建模能力，在各项NLP下游任务中都取得了很好的成绩。但是，BERT模型主要是<strong>聚焦在针对字或者英文word粒度的完形填空学习上面</strong>，没有充分利用<strong>训练数据当中词法结构，语法结构，以及语义信息</strong>去学习建模。比如“我要买苹果手机”，BERT模型将“我 要 买 苹 果 手 机”每个字都统一对待，<strong>在预训练时随机进行遮盖(mask)</strong>，丢失了“苹果手机”是一个很火的名词这一信息，这个是<strong>词法信息</strong>的缺失。同时“我 + 买 + 名词”是一个非常明显的购物意图的句式，BERT没有对此类<strong>语法结构</strong>进行专门的建模，如果预训练的语料中只有“我要买苹果手机”，“我要买华为手机”，哪一天出现了一个新的手机牌子比如栗子手机，而这个手机牌子在预训练的语料当中并不存在，<strong>没有基于词法结构以及句法结构的建模，对于这种新出来的词是很难给出一个很好的向量表示的</strong>，而ERNIE 1.0通过进行<strong>实体(entity)和短语(phrase)的masking</strong>，极大地增强了通用语义表示能力，在多项任务中均取得了大幅度超越BERT的效果。</p><p><strong>ERNIE和BERT不同的masking策略</strong>如下所示：</p><div align=center><img src=/Kimages/4/image-20200527092631883.png style=zoom:40%></div><p>与BERT相同，ERNIE的整体网络架构也使用Transformer的编码器。Transformer可以通过<strong>自注意力机制</strong>捕捉句子中每个标记的上下文信息，并<strong>生成一系列上下文嵌入(contextual embedding)</strong>。</p><p>ERNIE使用先验知识来增强预训练语言模型，其提出了一种<strong>多阶段的知识masking策略</strong>，将短语和实体层次的知识整合到语言表达中，而<strong>不是直接加入知识嵌入</strong>。下图描述了句子的不同masking级别：</p><div align=center><img src=/Kimages/4/image-20200527103621711.png style=zoom:40%></div><p>(1) <strong>Basic-level masking</strong>：Basifc-level masking是第一个阶段，它把一个句子看作一个基本语言单位的序列，对于英语，基本语言单位是<strong>单词</strong>，对于汉语，基本语言单位是<strong>汉字</strong>。在训练过程中，随机屏蔽15%的基本语言单元，并使用句子中的其他基本单元作为输入，训练一个Transformer的编码器来预测屏蔽单元。基于Basic-level masking，我们可以得到一个基本的单词表示。<strong>因为它是在基本语义单元的随机掩码上训练的，所以很难对高层语义知识进行完全建模</strong>。这个过程与BERT相同。</p><p>(2) <strong>Phrase-level masking</strong>：Phrase-level masking是第二个阶段。<strong>短语</strong>是作为概念单位的一小组单词或字符。<strong>对于英语，我们使用词汇分析和分块工具来获取句子中短语的边界</strong>，并<strong>使用一些依赖于语言的切分工具来获取其他语言(如汉语)中的单词/短语信息</strong>。在Phrase-level masking阶段，<strong>仍然使用基本语言单元作为训练输入</strong>，与随机基本单元mask不同，这次我们随机选择句子中的几个短语，对同一短语中的<strong>所有基本单元进行mask和预测</strong>。<strong>在此阶段，短语信息被编码到单词嵌入中</strong>。</p><p>(3) <strong>Entity-level masking</strong>：Entity-level masking是第三个阶段。<strong>名称实体</strong>包含个人、地点、组织、产品等，可以用适当的名称表示。它可以是抽象的，也可以是物理存在的。通常<strong>实体在句子中包含重要信息</strong>。和Phrase-level masking阶段一样，<strong>首先分析句子中的命名实体</strong>，然后屏蔽和预测实体中的所有语言单元。经过三个阶段的学习，得到了一个<strong>由丰富的语义信息增强的词表示</strong>。</p><p>ERNIE使用中文维基百科、百度百科、百度新闻和百度贴吧的综合语料库进行预训练。</p><p>此外，ERNIE<strong>使用多轮对话修改BERT中的NSP(next sentence prediction)任务</strong>：</p><div align=center><img src=/Kimages/4/image-20200527105452130.png style=zoom:40%></div><p>在ERNIE中，NSP任务变为了<strong>DLM(dialogue language model)</strong> 任务。使用dialogue embedding来区分不同的对话角色，可以表示多轮对话。与BERT中的<strong>MLM(masked language model)</strong> 一样，masks被应用于强制模型来预测查询和响应条件下的丢失单词。此外，通过用随机选择的句子替换查询Q或响应R来生成假样本。该模型用于判断多回合对话是真是假。DLM任务帮助ERNIE学习对话中的隐含关系，这也增强了模型学习语义表示的能力。</p><h2 id=ernie-20>ERNIE 2.0</h2><p>ERNIE 2.0在ERNIE 1.0的基础上继续进行改进：其构建了3类任务，包括word-aware tasks、structure-aware tasks和semantic-aware tasks。这些任务都是无监督或弱监督，所以可以从海量数据中获取到。对于<strong>多任务的预训练</strong>，本文框架是在一个持续的学习模式中训练所有这些任务。ERNIE 2.0先用一个简单的任务训练一个初始模型，然后不断引入新的预训练任务对模型进行升级。对于一个新任务，先用前一个任务的参数进行初始化。然后，新任务将与之前的任务一起训练，以确保模型不会忘记它所学到的知识。</p><div align=center><img src=/Kimages/4/image-20211230155514333.png style=zoom:40%></div><p>如上图所示，ERNIE 2.0的输入包括四部分：Token embedding、Sentence embedding、Position embedding和Task embedding。作者设计了七个预训练任务，并归属于三个类别，以此来提升预训练效果。</p><h1 id=gpt>GPT</h1><p>BERT模型使用Transormer encoder结构对文本进行编码并预训练，然后通过fine-tuning完成下游任务。与BERT不同，GPT是基于Transformer decoder进行构建的，其可以看作是一个<strong>语言模型(language model)</strong>，通过一个接一个生成单词，从而生成连续的句子。在每个新单词产生后，该单词就被添加在之前生成的单词序列后面，这个序列会成为模型下一步的新输入。这种机制叫做<strong>自回归(auto-regression)</strong>。</p><p>GPT使用语言模型任务进行预训练，然后可以进一步fine-tuning从而适应多种不同的下游任务，如下所示：</p><div align=center><img src=/Kimages/4/image-20211230162346254.png style=zoom:40%></div><p>GPT-1的思想是先通过在无标签的数据上学习一个通用的语言模型，然后再根据特定热任务进行微调。而GPT2的目的是训练一个泛化能力更强的词向量模型，<strong>使用无监督的预处训练做有监督的任务</strong>，即，将各类有监督任务构建为语言模型的形式进行预训练。GPT2并没有网络结构上进行更多的创新和设计，而是使用了更多的参数和更大的数据集。GPT-2的核心思想概括为：任何有监督任务都是语言模型的一个子集，当模型的容量非常大且数据量足够丰富时，仅仅靠训练语言模型的学习便可以完成其他有监督学习的任务。</p><p>GPT-3进一步提升了语言模型的能力，仅仅需要zero-shot或者few-shot，GPT-3就可以在下游任务表现的非常好。除了几个常见的NLP任务，GPT-3还在很多非常困难的任务上也有惊艳的表现，例如撰写人类难以判别的文章，甚至编写SQL查询语句，React或者JavaScript代码等。而这些强大能力的能力则依赖于GPT-3的1,750亿参数量， 45TB的训练数据以及高达1,200万美元的训练费用。GPT、GPT-2和GPT-3的对比如下。</p><div align=center><img src=/Kimages/4/image-20211230163905638.png style=zoom:45%></div><h1 id=参考资料>参考资料</h1><ul><li>Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.</li><li>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances in neural information processing systems. 2017: 5998-6008.</li><li>Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</li><li>Sun Y, Wang S, Li Y, et al. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.</li><li>Sun Y, Wang S, Li Y, et al. Ernie 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv:1907.12412, 2019.</li><li>图解Transformer：https://jalammar.github.io/illustrated-transformer/</li><li>ERNIE简介：https://blog.csdn.net/PaddlePaddle/article/details/102713947</li></ul><hr><ul class=pager><li class=previous><a href=/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/ data-toggle=tooltip data-placement=top title=自然语言处理：“序列到序列”与“注意力机制”>&larr;
Previous Post</a></li><li class=next><a href=/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/ data-toggle=tooltip data-placement=top title=自然语言处理：自然语言处理应用>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>