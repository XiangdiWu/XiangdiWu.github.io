<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="深度学习：神经网络"><meta property="og:title" content="深度学习：神经网络"><meta property="twitter:title" content="深度学习：神经网络"><meta name=description content="本文主要介绍神经网络，包括神经网络的基本结构、激活函数、前馈神经网络、反向传播算法、卷积神经网络、循环神经网络、图神经网络等。"><meta property="og:description" content="本文主要介绍神经网络，包括神经网络的基本结构、激活函数、前馈神经网络、反向传播算法、卷积神经网络、循环神经网络、图神经网络等。"><meta property="twitter:description" content="本文主要介绍神经网络，包括神经网络的基本结构、激活函数、前馈神经网络、反向传播算法、卷积神经网络、循环神经网络、图神经网络等。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>深度学习：神经网络-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/deep-learning title="Deep Learning">Deep Learning</a></div><h1>深度学习：神经网络</h1><h2 class=subheading>NN-Neural Network</h2><span class=meta>Posted by
XiangdiWu
on
Monday, October 12, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p><strong>人工神经网络(artificial neural network, ANN)</strong> 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象，构建<strong>人工神经元</strong>，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为<strong>神经网络(neural network, NN)</strong>。</p><p>神经网络最早是作为一种主要的<strong>连接主义模型</strong>。20世纪80年代中后期，最流行的一种连接主义模型是<strong>分布式并行处理(parallel distributed processing, PDP)模型</strong>，其有3个主要特性：(1) 信息表示是分布式的(非局部的)；(2) 记忆和知识是存储在单元之间的连接上；(3)通过逐渐改变单元之间的连接强度来学习新的知识。</p><p>连接主义的神经网络有着多种多样的网络结构以及学习方法，虽然早期模型强调模型的<strong>生物可解释性(biological plausibility)</strong>，但后期更关注于<strong>对某种特定认知能力的模拟</strong>，比如物体识别、语言理解等. 尤其在引入误差反向传播来改进其学习能力之后，神经网络也越来越多地应用在各种机器学习任务上。随着训练数据的增多以及(并行)计算能力的增强，神经网络在很多机器学习任务上已经取得了很大的突破，特别是在语音、图像等感知信号的处理任务上，神经网络表现出了卓越的学习能力。</p><p>目前受到关注的是采用<strong>误差反向传播</strong>来进行学习的神经网络，即作为<strong>一种机器学习模型</strong>的神经网络。从机器学习的角度来看，神经网络一般可以看作是一个<strong>非线性模型</strong>，其基本组成单元为<strong>具有非线性激活函数的神经元</strong>，通过大量神经元之间的连接，使得神经网络成为一种<strong>高度非线性的模型</strong>。神经元之间的<strong>连接权重</strong>就是<strong>需要学习的参数</strong>，可以在机器学习的<strong>框架</strong>下通过<strong>梯度下降</strong>方法来进行学习。</p><h1 id=神经网络的基本结构>神经网络的基本结构</h1><p>神经网络的基本组成单位是<strong>神经元(neuron)</strong>，如下所示：</p><div align=center><img src=/Kimages/3/image-20200428203202172.png style=zoom:35%></div><p>假设一个神经元接收$D$个输入$x_1,x_2,\cdots,x_D$，令向量$\boldsymbol{x}=\left[x_{1} ; x_{2} ; \cdots ; x_{D}\right]$表示这组输入，并用<strong>净输入(net input)</strong>$z\in \mathbb R$表示一个神经元所获得的输入信号的加权和，则神经元的<strong>前向(forward)运算</strong>过程如下：</p>$$
\begin{aligned}
z &=\sum_{d=1}^{D} w_{d} x_{d}+b \\
&=\boldsymbol{w}^{\text T} \boldsymbol{x}+b
\end{aligned}
$$<p>其中$\boldsymbol{w}=[w_{1} ; w_{2} ; \cdots ; w_{D}] \in \mathbb{R}^{D}$是$D$维的<strong>权重(weight)<strong>向量，$b\in \mathbb R$是</strong>偏置(bias)</strong>。净输入$z$在经过一个非线性函数$f$后，得到神经元的<strong>激活值(activation)</strong>$a=f(z)$，其中非线性函数$f$称为<strong>激活函数</strong>。</p><p>与<strong>感知机(perceptron)<strong>相比，神经元的激活函数通常是</strong>连续可导的函数</strong>，而感知机的激活函数仅仅是在$x=0$处不可导的阶跃函数，用于将输出值进行二分，不需要进行反向梯度计算。</p><p>到目前为止，研究者已经发明了各种各样的神经网络结构。目前常用的神经网络结构有以下三种：</p><p>(1) <strong>前馈网络</strong>：前馈网络中各个神经元按接收信息的先后分为不同的组。每一组可以看作一个<strong>神经层</strong>。每一层中的神经元接收前一层神经元的输出，并输出到下一层神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播，可以用一个有向无环路图表示。前馈网络包括<strong>全连接前馈网络</strong>和<strong>卷积神经网络</strong>等。前馈网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单，易于实现。</p><p>(2) <strong>记忆网络</strong>：也称反馈网络，网络中的神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息。和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态。记忆神经网络中的信息传播可以是单向或双向传递，因此可用一个<strong>有向循环图或无向图</strong>来表示。记忆网络包括<strong>循环神经网络</strong>、<strong>Hopfield网络</strong>、<strong>玻尔兹曼机</strong>、<strong>受限玻尔兹曼机</strong>等。记忆网络可以看作一个程序，具有更强的计算和记忆能力。为了增强记忆网络的记忆容量，可以引入外部记忆单元和读写机制，用来保存一些网络的中间状态，称为<strong>记忆增强神经网络(memory augmented neural network, MANN)</strong>，比如<strong>神经图灵机</strong>和<strong>记忆网络</strong>等。</p><p>(3) <strong>图网络</strong>：前馈网络和记忆网络的输入都可以表示为向量或向量序列。但实际应用中很多数据是图结构的数据，比如知识图谱、社交网络、分子网络等。前馈网络和记忆网络很难处理图结构的数据。图网络是定义在图结构数据上的神经网络。图中每个节点都由一个或一组神经元构成。节点之间的连接可以是有向的，也可以是无向的。每个节点可以收到来自相邻节点或自身的信息.</p><p><strong>图网络是前馈网络和记忆网络的泛化</strong>，包含很多不同的实现方式，比如<strong>图卷积网络(graph convolutional network, GCN)</strong>、<strong>图注意力网络(graph attention network, GAT)</strong>、<strong>消息传递神经网络(message passing neural network, MPNN)</strong> 等。</p><p>三种网络结构示意图如下所示：</p><div align=center><img src=/Kimages/3/image-20200428224849253.png style=zoom:30%></div><h1 id=激活函数>激活函数</h1><p><strong>激活函数(activation function)</strong> 的作用是为神经网络的传播过程增添非线性因素。为了增强网络的表示能力，激活函数需要具备以下几点性质：</p><p>(1) <strong>连续并可导(允许少数点不可导)的非线性函数</strong>，可以直接利用数值优化的方法优化参数。</p><p>(2) 激活函数及其导函数要<strong>尽可能的简单</strong>，有利于提高网络计算效率。</p><p>(3) 导函数的值域要在一个<strong>合适的区间</strong>内，不能太大(影响稳定性)或太小(影响效率)。</p><p>神经网络中每个神经元常用的激活函数如下：</p><p>(1) <strong>sigmoid函数</strong>：可以看成是一个映射函数，把一个实数域的输入映射到$(0,1)$这一实数域中。当输入值在0附近时，sigmoid函数近似为线性函数；当输入值靠近两端时，对输入进行抑制。输入越小，函数值越接近于0；输入越大，函数值越接近于1。这样的特点也<strong>和生物神经元类似</strong>，对一些输入会产生兴奋(输出为1)，对另一些输入产生抑制(输出为0)。和感知器使用的阶跃激活函数相比，sigmoid函数是连续可导的，其数学性质更好。</p><p>因为sigmoid函数的性质，使得装备了sigmoid激活函数的神经元具有以下两点性质：(1) 其输出<strong>直接可以看作是概率分布</strong>，使得神经网络可以更好地和统计学习模型进行结合；(2) 其可以看作是一个<strong>软性门(soft gate)</strong>，用来控制其他神经元输出信息的数量。sigmoid函数的数学表达式如下：</p>$$
\sigma(x)=\frac{1}{1+\exp (-x)}
$$<p>(2) <strong>tanh函数</strong>：与sigmoid函数相似，只不过是将实数域的输入映射到$(-1,1)$区间内。其定义为：</p>$$
\tanh (x)=\frac{\exp (x)-\exp (-x)}{\exp (x)+\exp (-x)}
$$<p>下图给出了sigmoid函数和tanh函数的图像。tanh函数的输出是零中心化的(zero-centered)，而sigmoid函数的输出恒大于0。非零中心化的输出会使 得其后一层的神经元的输入发生<strong>偏置偏移(bias shift)</strong>，并<strong>进一步使得梯度下降的收敛速度变慢</strong>。</p><div align=center><img src=/Kimages/3/image-20200428214520104.png style=zoom:30%></div><p>(3) <strong>ReLU函数</strong>：ReLU(Rectified Linear Unit, 修正线性单元)是目前深度神经网络中经常使用的激活函数。</p>$$
\begin{aligned}
\operatorname{ReLU}(x) &=\left\{\begin{array}{ll}
x, & x \geqslant 0 \\
0, & x<0
\end{array}\right.\\
&=\max (0, x)
\end{aligned}
$$<p>ReLU函数的<strong>优</strong>是：采用ReLU的神经元只需要进行加、乘和比较的操作，计算上更加高效。ReLU函数被认为有生物上的解释性，比如<strong>单侧抑制</strong>、<strong>宽兴奋边界</strong>(即兴奋程度也可以非常高)。在生物神经网络中，同时处于兴奋状态的神经元非常稀疏，人脑中在同一时刻大概只有1%至4%的神经元处于活跃状态。sigmoid激活函数会导致一个<strong>非稀疏的神经网络</strong>，而ReLU却具有很好的<strong>稀疏性</strong>，大约50%的神经元会处于激活状态。</p><p>在优化方面，相比于sigmoid函数的两端饱和，ReLU函数为<strong>左饱和函数</strong>，且在$x>0$时导数为1，在一定程度上缓解了神经网络的<strong>梯度消失</strong>问题，加速梯度下降的收敛速度。</p><p>ReLU函数的<strong>缺点</strong>：ReLU函数的输出是<strong>非零中心化的</strong>，会给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。此外，ReLU神经元在训练时比较容易“死亡”。在训练时，如果参数在一次不恰当的更新后，隐藏层中的某个ReLU神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中<strong>永远不能被激活</strong>。这种现象称为<strong>死亡ReLU问题(dying ReLU problem)</strong>。</p><p>(4) <strong>Leaky ReLU函数</strong>：在输入$x<0$时，保持一个很小的梯度$\lambda$. 这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活。</p>$$
\begin{aligned}
\text {LeakyReLU}(x) &=\left\{\begin{array}{ll}
x, & \text { if } x>0 \\
\gamma x, & \text { if } x \leqslant 0
\end{array}\right.\\
&=\max (0, x)+\gamma \min (0, x)
\end{aligned}
$$<p>(5) <strong>ELU函数</strong>：一个近似的零中心化的非线性函数，其定义为：</p>$$
\begin{aligned}
\operatorname{ELU}(x) &=\left\{\begin{array}{ll}
x & \text { if } x>0 \\
\gamma(\exp (x)-1) & \text { if } x \leqslant 0
\end{array}\right.\\
&=\max (0, x)+\min (0, \gamma(\exp (x)-1))
\end{aligned}
$$<p>其中$\gamma \geqslant 0$是一个<strong>超参数</strong>，决定$x \leqslant 0$时的饱和曲线，并调整输出均值在0附近。</p><p>(6) <strong>Softplus函数</strong>：可以看做ReLU函数的平滑版本，其导数刚好为sigmoid函数。Softplus函数虽然也具有单侧抑制、宽兴奋边界的特性，却没有稀疏激活性。</p>$$
\text {Softplus}(x)=\log (1+\exp (x))
$$<p>下图给出了ReLU、Leaky ReLU、ELU以及Softplus函数的示例。</p><div align=center><img src=/Kimages/3/image-20200428215823434.png style=zoom:30%></div><p>(7) <strong>Swish函数</strong>：一种**自门控(self-gated)**激活函数，定义为：</p>$$
\operatorname{swish}(x)=x \sigma(\beta x)
$$<p>其中$\sigma(\cdot)$为sigmoid函数，$\beta$为可学习的参数或一个固定超参数，$\sigma(\cdot) \in (0,1)$可以看作是一种软性的门控机制。当$\sigma(\beta x)$接近于1时，门处于“开”状态，激活函数的输出近似于$x$本身；当$\sigma(\beta x)$接近于0时，门的状态为“关”，激活函数的输出近似于0。下图给出了Swish函数的实例。</p><div align=center><img src=/Kimages/3/image-20200428223811432.png style=zoom:30%></div><p>除了以上这些激活函数外，还有各式各样其他激活函数，如<strong>Maxout单元</strong>、<strong>高斯误差线性单元(GELU)</strong> 等。</p><h1 id=前馈神经网络>前馈神经网络</h1><p><strong>前馈神经网络(feed-forward neural network)</strong> 是结构最简单的神经网络，其信息从输入层通过多个隐含层传递到输出层，然后误差通过反向传播算法再从输出层逐层传播至输入层。其网络层次表示如下：</p><div align=center><img src=/Kimages/3/image-20200428225023895.png style=zoom:25%></div><p>前馈神经网络可用如下参数表示：</p><div align=center><img src=/Kimages/3/image-20200428225743651.png style=zoom:25%></div><p>前馈神经网络通过下面公式进行<strong>前向传播</strong>：</p>$$
\begin{aligned}
&\boldsymbol{z}^{(l)}=W^{(l)} \cdot \boldsymbol{a}^{(l-1)}+\boldsymbol{b}^{(l)}\\
&\boldsymbol{a}^{(l)}=f_{l}(\boldsymbol{z}^{(l)})
\end{aligned}
$$<p>上式也可以合并写作$\boldsymbol{z}^{(l)}=W^{(l)} \cdot f_{l-1}(\boldsymbol{z}^{(l-1)})+\boldsymbol{b}^{(l)}$或者$\boldsymbol{a}^{(l)}=f_{l}(W^{(l)} \cdot \boldsymbol{a}^{(l-1)}+\boldsymbol{b}^{(l)})$。</p><p>这样，前馈神经网络可以通过逐层的信息传递，得到<strong>网络最后的输出</strong>$a^{(l)}$。整个网络可以看作一个复合函数$\phi(\boldsymbol{x} ; W, \boldsymbol{b})$，将向量$\boldsymbol x$作为第一层的输入$\boldsymbol a^{(0)}$，将最后一层$L$的输出$\boldsymbol a^{(L)}$作为整个函数的输出：</p>$$
\boldsymbol{x}=\boldsymbol{a}^{(0)} \rightarrow \boldsymbol{z}^{(1)} \rightarrow \boldsymbol{a}^{(1)} \rightarrow \boldsymbol{z}^{(2)} \rightarrow \cdots \rightarrow \boldsymbol{a}^{(L-1)} \rightarrow \boldsymbol{z}^{(L)} \rightarrow \boldsymbol{a}^{(L)}=\phi(\boldsymbol{x} ; W, \boldsymbol{b})
$$<p>神经网络的学习过程就是通过训练集调整参数，使得输入和输出“相匹配”。给定训练集$\mathcal{D}=\{(\boldsymbol{x}^{(n)}, \boldsymbol y^{(n)})\}_{n=1}^{N}$，将每个样本$\boldsymbol{x}^{(n)}$输入给前馈神经网络，得到网络输出为$\hat{y}^{(n)}$，其在数据集$\mathcal D$上的<strong>结构化风险函数</strong>为：</p>$$
\mathcal{R}(W, \boldsymbol{b})=\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}({\boldsymbol y}^{(n)}, \hat{{\boldsymbol y}}^{(n)})+\frac{1}{2} \lambda\|W\|_{F}^{2}
$$<p>其中$\mathcal L(\cdot)$为损失函数，分类问题一般采用<strong>交叉熵损失函数</strong>，而回归问题一般采用<strong>均方误差损失函数</strong>。有了学习准则和训练样本，网络参数可以通过<strong>梯度下降法</strong>来激进型学习：</p>$$
\begin{aligned}
W^{(l)} & \leftarrow W^{(l)}-\alpha \frac{\partial \mathcal{R}(W, b)}{\partial W^{(l)}} \\
&=W^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N}(\frac{\partial \mathcal{L}(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)})}{\partial W^{(l)}})+\lambda W^{(l)}\right) \\
\boldsymbol{b}^{(l)} & \leftarrow \boldsymbol{b}^{(l)}-\alpha \frac{\partial \mathcal{R}(W, \boldsymbol{b})}{\partial \boldsymbol{b}^{(l)}} \\
&=\boldsymbol{b}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \frac{\partial \mathcal{L}(\boldsymbol{y}^{(n)}, \hat{\boldsymbol{y}}^{(n)})}{\partial \boldsymbol{b}^{(l)}}\right)
\end{aligned}
$$<p><strong>输出层(output layer)</strong> 的设计要与标签$y$相符合，因此，对于分类问题，网络输出层的激活函数通常采用<strong>softmax函数</strong>，其神经元的个数等于类别个数，而<strong>网络的输出相当于每个类的后验概率</strong>。对于回归问题，输出层的激活函数一般采用<strong>恒等函数</strong>。</p><h1 id=反向传播算法>反向传播算法</h1><p>假设采用随机梯度下降进行神经网络参数学习，给定一个样本$(\boldsymbol{x}, \boldsymbol{y})$，将其输入到神经网络模型中，得到网络输出为$\hat{\boldsymbol y}$。假设损失函数为$\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$，要进行参数学习就需要计算损失函数关于每个参数的<strong>偏导数</strong>。根据链式法则，</p>$$
\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}
$$$$
\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}} \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}
$$<p>上面两个公式中的第二项都是损失函数关于第$l$层神经元$\boldsymbol z^{(l)}$的偏导数，称为<strong>误差项</strong>，可以一次计算得到。</p><p>这样，我们只需要计算三个偏导数，分别为$\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}}$，$\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}}$和$\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}$。</p><p>(1) 计算偏导数$\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}}$(<strong>向量关于标量的偏导数</strong>)</p>$$
\begin{aligned}
\frac{\partial \boldsymbol{z}^{(l)}}{\partial w_{i j}^{(l)}} &=\left[\frac{\partial z_{1}^{(l)}}{\partial w_{i j}^{(l)}}, \cdots, \frac{\partial z_{i}^{(l)}}{\partial w_{i j}^{(l)}}, \cdots, \frac{\partial z_{m^{(l)}}^{(l)}}{\partial w_{i j}^{(l)}}\right] \\
&=\left[0, \cdots, \frac{\partial\left(\boldsymbol{w}_{i:}^{(l)} \boldsymbol{a}^{(l-1)}+b_{i}^{(l)}\right)}{\partial w_{i j}^{(l)}}, \cdots, 0\right] \\
&=\left[0, \cdots, a_{j}^{(l-1)}, \cdots, 0\right] \\
& \triangleq \mathbb{I}_{i}\left(a_{j}^{(l-1)}\right) \in \mathbb{R}^{m^{(l)}}
\end{aligned}
$$<p>其中$\boldsymbol{w}_{i:}^{(l)}$为权重矩阵$W^{(l)}$的第$i$行，$\mathbb{I}_{i}(a_{j}^{(l-1)})$ 表示第$i$个元素为$a_{j}^{(l-1)}$，其余为0的行向量。</p><p>(2) 计算偏导数$\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}}$(<strong>向量关于向量的偏导数</strong>)</p>$$
\frac{\partial \boldsymbol{z}^{(l)}}{\partial \boldsymbol{b}^{(l)}}=\boldsymbol{I}_{m^{(l)}} \in \mathbb{R}^{m^{(l)} \times m^{(l)}}
$$<p>为$m^{(l)}\times m^{(l)}$大小的<strong>单位矩阵</strong>。</p><p>(3) 计算偏导数$\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}$(<strong>标量关于向量的偏导数</strong>)</p><p>偏导数$\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}}$表示第$l$层神经元对最终损失的影响，也反映了最终损失对第$l$层神经元的敏感程度，因此一般称为第$l$层神经元的<strong>误差项</strong>，用$\delta^{(l)}$来表示：</p>$$
\delta^{(l)} \triangleq \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \in \mathbb{R}^{m^{(l)}}
$$<p>误差项$\delta^{(l)}$也间接反映了<strong>不同神经元对网络能力的贡献程度</strong>，从而比较好地解决了<strong>贡献度分配问题(credit assignment problem, CAP)</strong>。</p><p>根据$\boldsymbol{z}^{(l+1)}=W^{(l+1)} \boldsymbol{a}^{(l)}+\boldsymbol{b}^{(l+1)}$，有：</p>$$
\frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}}=(W^{(l+1)})^{\mathrm{T}}
$$<p>根据$\boldsymbol{a}^{(l)}=f_{l}(\boldsymbol{z}^{(l)})$，其中$f_l(\cdot)$为按位计算的函数，因此有：</p>$$
\begin{aligned}
\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}} &=\frac{\partial f_{l}(\boldsymbol{z}^{(l)})}{\partial \boldsymbol{z}^{(l)}} \\
&=\operatorname{diag}(f_{l}^{\prime}(\boldsymbol{z}^{(l)}))
\end{aligned}
$$<p>因此，根据链式法则，第$l$层的误差项为：</p>$$
\begin{aligned}
\delta^{(l)} & \triangleq \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l)}} \\
&=\frac{\partial \boldsymbol{a}^{(l)}}{\partial \boldsymbol{z}^{(l)}} \cdot \frac{\partial \boldsymbol{z}^{(l+1)}}{\partial \boldsymbol{a}^{(l)}} \cdot \frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{z}^{(l+1)}} \\
&=\operatorname{diag}(f_{l}^{\prime}(\boldsymbol{z}^{(l)})) \cdot(W^{(l+1)})^{\mathrm{T}} \cdot \delta^{(l+1)} \\
&=f_{l}^{\prime}(\boldsymbol{z}^{(l)}) \odot((W^{(l+1)})^{\mathrm{T}} \delta^{(l+1)})
\end{aligned}
$$<p>在计算出上面三个偏导数之后，损失函数$\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$对参数$\boldsymbol w_{ij}^{(l)}$的偏导数可以写为：</p>$$
\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial w_{i j}^{(l)}}=\mathbb{I}_{i}(a_{j}^{(l-1)}) \delta^{(l)}=\delta_{i}^{(l)} a_{j}^{(l-1)}
$$<p>进一步，$\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$关于第$l$层参数矩阵$W^{(l)}$的偏导数(梯度)为：</p>$$
\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial W^{(l)}}=\delta^{(l)}(\boldsymbol{a}^{(l-1)})^{\mathrm{T}}
$$<p>同理，$\mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})$关于第$l$层偏置$\boldsymbol b^{(l)}$的偏导数(梯度)为：</p>$$
\frac{\partial \mathcal{L}(\boldsymbol{y}, \hat{\boldsymbol{y}})}{\partial \boldsymbol{b}^{(l)}}=\delta^{(l)}
$$<p>将梯度写成如上这种统一的形式，便可以实现<strong>并行自动微分计算</strong>，大大节省计算时间。</p><p>在计算出每一层的误差时候，我们就可以得到每一层参数的梯度。因此，使用反向传播算法的前馈神经网络的训练过程可以分为以下三步：(1) 前馈计算每一层的净输入$\boldsymbol z^{(l)}$和激活值$\boldsymbol a^{(l)}$直到最后一层；(2) 反向计算每一层的误差项$\delta^{(l)}$；(3) 计算每一层参数的偏导数，并更新参数。使用反向传播算法的随机梯度下降训练过程如下所示：</p><div align=center><img src=/Kimages/3/image-20200429181346985.png style=zoom:40%></div><h1 id=通用近似定理>通用近似定理</h1><p>前馈神经网络具有很强的拟合能力，常见的连续非线性函数都可以用前馈神经网络来近似。</p><p><strong>通用近似定理(universal approximation theorem)</strong> 如下：令$\phi(\cdot)$是一个非常数、有界、单调递增的连续函数，$\mathcal{I}_{d}$是一个$d$维的单位超立方体$[0,1]^d$，$C(\mathcal{I}_{d})$是定义在$\mathcal{I}_{d}$上的连续函数集合。对于任何一个函数$f \in C(\mathcal{I}_{d})$，存在一个整数$m$和一组实数$v_i,b_i \in \mathbb R$以及实数向量$\boldsymbol w_i \in \mathbb R^d,i=1,\cdots,m$，以至我们可以定义函数：</p>$$
F(\boldsymbol{x})=\sum_{i=1}^{m} v_{i} \phi(\boldsymbol{w}_{i}^{\mathrm{T}} \boldsymbol{x}+b_{i})
$$<p>作为函数$f$的近似实现，即$|F(\boldsymbol{x})-f(\boldsymbol{x})|<\epsilon, \forall \boldsymbol{x} \in \mathcal{I}_{d}$，其中$epsilon>0$是一个很小的正数。通用近似定理在实数空间$\mathbb R^d$中的有界闭集上依然成立。</p><p>根据通用近似定理，对于具有<strong>线性输出层</strong>和<strong>至少一个使用“挤压”性质的激活函数的隐藏层</strong>组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何一个定义在实数空间$\mathbb R^d$中的有界闭集函数 。所谓<strong>挤压性质的函数是指像sigmoid函数的有界函数</strong>，但神经网络的通用近似性质也被证明<strong>对于其它类型的激活函数，比如ReLU，也都是适用的</strong>。</p><p>通用近似定理只是说明了神经网络的计算能力可以去近似一个给定的连续函数，但并没有给出如何找到这样一个网络，以及是否是最优的。此外，当应用到机器学习时，真实的映射函数并不知道，一般是通过经验风险最小化和正则化来进行参数学习。因为神经网络的强大能力，反而容易在训练集上<strong>过拟合</strong>。</p><h1 id=自动梯度计算>自动梯度计算</h1><p>神经网络的参数主要通过梯度下降来进行优化。当确定了风险函数以及网络结构后，我们就可以手动用链式法则来计算风险函数对每个参数的梯度，并用代码进行实现。但是<strong>手动求导并转换为计算机程序的过程非常琐碎并容易出错，导致实现神经网络变得十分低效</strong>。实际上，参数的梯度可以让计算机来自动计算。目前，主流的<strong>深度学习框架</strong>都包含了自动梯度计算的功能，即我们可以只考虑网络结构并用代码实现，其梯度可以自动进行计算，无需人工干预，这样可以大幅提高开发效率。自动计算梯度的方法可以分为以下三类：数值微分、符号微分和自动微分。</p><h2 id=数值微分>数值微分</h2><p><strong>数值微分(numerical differentiation)</strong> 使用数值方法进行微分求解。函数$f(x)$的点$x$的导数定义为：</p>$$
f^{\prime}(x)=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}
$$<p>在使用数值方法进行计算时，通常将$\Delta x$设置为一个非常小的值，然后即可计算出数值结果。为了使计算结果更加稳定，实际应用中通常采用下式来表示梯度：</p>$$
f^{\prime}(x)=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x-\Delta x)}{2 \Delta x}
$$<p>数值微分非常容易实现，但找到一个合适的扰动$\Delta x$十分困难。若$\Delta x$过小，会引起数值运算问题，比如<strong>舍入误差</strong>；若$\Delta x$过大，会增加<strong>截断误差</strong>，使得计算不准确。因此，数值微分的实用性较差。</p><h2 id=符号微分>符号微分</h2><p><strong>符号微分(symbolic differentiation)</strong> 是一种基于符号计算的自动求导方法。符号计算也叫代数计算，是指用计算机来处理带有变量的数学表达式。这里的变量看作是符号(symbols)，一般不需要代入具体的值。<strong>符号计算的输入和输出都是数学表达式</strong>，一般包括对数学表达式的化简、因式分解、微分、积分、解代数方程、求解常微分方程等运算。符号计算一般来讲是对输入的表达式，通过迭代或递归使用一些事先定义的规则进行转换。当转换结果不能再继续使用变换规则时，便停止计算。</p><p>符号微分可以在编译时就计算梯度的数学表示，并进一步利用符号计算方法进行优化。此外，符号计算的一个优点是<strong>符号计算和平台无关，可以在CPU或GPU上运行</strong>。符号微分也有一些不足之处。一是<strong>编译时间较长</strong>，特别是对于循环，需要很长时间进行编译；二是为了进行符号微分，一般需要<strong>设计一种专门的语言来表示数学表达式</strong>，并且要对变量(符号)进行预先声明；三是<strong>很难对程序进行调试</strong>。</p><h2 id=自动微分>自动微分</h2><p><strong>自动微分(automatic differentiation, AD)</strong> 是一种可以对一个(程序)函数进行计算导数的方法。符号微分的处理对象是数学表达式，而自动微分的处理对象是一个函数或一段程序。自动微分可以直接在原始程序代码进行微分，因此自动微分成为目前大多数深度学习框架的首选。</p><p>自动微分的基本原理是<strong>所有的数值计算可以分解为一些基本操作</strong>，包含四则运算和一些初等函数等，然后<strong>利用链式法则来自动计算一个复合函数的梯度</strong>。这里以一个神经网络中常见的复合函数的例子来说明自动微分的过程。令复合函数$f(x;w,b)$为：</p>$$
f(x ; w, b)=\frac{1}{\exp (-(w x+b))+1}
$$<p>该复合函数可以用如下<strong>计算图(computational graph)</strong> 进行表示：</p><div align=center><img src=/Kimages/3/image-20200429195107920.png style=zoom:35%></div><p>从计算图上可以看出，该复合函数由六个基本函数组成，每个基本函数的导数都很简单，可以通过规则实现：</p><div align=center><img src=/Kimages/3/image-20200429195546467.png style=zoom:30%></div><p>整个复合函数$f(x;w,b)$关于参数$w$和$b$的梯度可以通过计算图上的结点$f(x;w,b)$与参数$w$和$b$之间路径上所有的导数连乘来得到，即：</p>$$
\begin{aligned}
\frac{\partial f(x ; w, b)}{\partial w}&=\frac{\partial f(x ; w, b)}{\partial h_{6}} \frac{\partial h_{6}}{\partial h_{5}} \frac{\partial h_{5}}{\partial h_{4}} \frac{\partial h_{4}}{\partial h_{3}} \frac{\partial h_{3}}{\partial h_{2}} \frac{\partial h_{2}}{\partial h_{1}} \frac{\partial h_{1}}{\partial w} \\
\frac{\partial f(x ; w, b)}{\partial b}&=\frac{\partial f(x ; w, b)}{\partial h_{6}} \frac{\partial h_{6}}{\partial h_{5}} \frac{\partial h_{5}}{\partial h_{4}} \frac{\partial h_{4}}{\partial h_{3}} \frac{\partial h_{3}}{\partial h_{2}} \frac{\partial h_{2}}{\partial b}
\end{aligned}
$$<p><strong>如果函数和参数之间有多条路径，可以将这多条路径上的导数再进行相加，得到最终的梯度</strong>。</p><p>按照计算导数的顺序，自动微分可以分为两种模式：<strong>前向模式和反向模式</strong>。前向模式是按计算图中计算方向的相同方向来递归地计算梯度，反向模式是按计算图中计算方向的相反方向来递归地计算梯度。前向模式和反向模式可以看作是应用链式法则的两种梯度累积方式。可以看出，<strong>反向模式和反向传播的计算梯度的方式相同</strong>。</p><p>对于一般的函数形式$f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$，前向模式需要对每一个输入变量都进行一遍遍历，共需要$n$遍。而反向模式需要对每一个输出都进行一个遍历，共需要$m$遍。当$n>m$时，反向模式更高效。在前馈神经网络中，<strong>风险函数</strong>为$f: \mathbb{R}^{n} \rightarrow \mathbb{R}$，输出为标量，因此<strong>采用反向模式为最有效的计算方式，只需要一遍计算</strong>。</p><p>计算图按构建方式可以分为<strong>静态计算图(static computational graph)<strong>和</strong>动态计算图(dynamic computational graph)</strong>。静态计算图是在编译时构建计算图，计算图构建好之后在程序运行时不能改变，而动态计算图是在程序运行时动态构建。两种构建方式各有优缺点。静态计算图在构建时可以进行优化，并行能力强，但灵活性比较差；动态计算图则不容易优化，当不同输入的网络结构不一致时，难以并行计算，但是灵活性比较高。</p><p>在目前常用的深度学习框架里，<strong>Tensorflow</strong>采用的是静态计算图，而<strong>PyTorch</strong>采用的是动态计算图。</p><h1 id=tensorflow实现全连接神经网络进行mnist手写数字识别>Tensorflow实现全连接神经网络进行MNIST手写数字识别</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(tf<span style=color:#ff79c6>.</span>__version__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 下载MNIST数据并进行归一化</span>
</span></span><span style=display:flex><span>mnist <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>datasets<span style=color:#ff79c6>.</span>mnist
</span></span><span style=display:flex><span>(X_train, y_train), (X_test, y_test) <span style=color:#ff79c6>=</span> mnist<span style=color:#ff79c6>.</span>load_data()
</span></span><span style=display:flex><span>X_train, X_test <span style=color:#ff79c6>=</span> X_train <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>255.0</span>, X_test <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>255.0</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(X_train<span style=color:#ff79c6>.</span>shape, X_test<span style=color:#ff79c6>.</span>shape, y_train<span style=color:#ff79c6>.</span>shape, y_test<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 第一种构造模型的方式：使用keras序列化API，该方法仅能构造线性神经网络结构</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;1. use keras sequential API to build a neural network&#39;</span>)
</span></span><span style=display:flex><span>neural_network_beginner <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>models<span style=color:#ff79c6>.</span>Sequential([
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Input(shape<span style=color:#ff79c6>=</span>(<span style=color:#bd93f9>28</span>, <span style=color:#bd93f9>28</span>)),
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Flatten(),
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>512</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>128</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>10</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;softmax&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 打印模型信息</span>
</span></span><span style=display:flex><span>neural_network_beginner<span style=color:#ff79c6>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># complie函数中的常用配置：优化器、损失函数和评价指标</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># categorical_crossentropy和sparse_categorical_crossentropy都是计算多分类crossentropy的，只是对y的格式要求不同。</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 如果是categorical_crossentropy，那y必须为one-hot形式</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 如果是sparse_categorical_crossentropy，那y就是原始的整数形式，比如[1, 0, 2, 0, 2]</span>
</span></span><span style=display:flex><span>neural_network_beginner<span style=color:#ff79c6>.</span>compile(optimizer<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;adam&#39;</span>, loss<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 训练模型</span>
</span></span><span style=display:flex><span>neural_network_beginner<span style=color:#ff79c6>.</span>fit(X_train, y_train, batch_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>64</span>, epochs<span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>, validation_data<span style=color:#ff79c6>=</span>(X_test, y_test))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 评估结果</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(neural_network_beginner<span style=color:#ff79c6>.</span>evaluate(X_test, y_test))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 第二种构造模型的方式：继承tf.keras.models.Model类，并重写__init__()和call()函数</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;2. use keras model subclassing API to build a neural network for classification&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 将数据转化为tf.data.Dataset格式</span>
</span></span><span style=display:flex><span>train_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices((X_train, y_train))<span style=color:#ff79c6>.</span>shuffle(<span style=color:#bd93f9>10000</span>)<span style=color:#ff79c6>.</span>batch(<span style=color:#bd93f9>64</span>)
</span></span><span style=display:flex><span>test_ds <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices((X_test, y_test))<span style=color:#ff79c6>.</span>batch(<span style=color:#bd93f9>64</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>NeuralNetwork</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>models<span style=color:#ff79c6>.</span>Model):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(NeuralNetwork, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>flatten <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Flatten()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>hidden_1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>512</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>hidden_2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>128</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>output_layer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>10</span>, activation<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;softmax&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>flatten(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>hidden_1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>hidden_2(x)
</span></span><span style=display:flex><span>        out <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>output_layer(x)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>neural_network_advanced <span style=color:#ff79c6>=</span> NeuralNetwork()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义损失函数和优化器</span>
</span></span><span style=display:flex><span>loss_function <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>SparseCategoricalCrossentropy()
</span></span><span style=display:flex><span>optimizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 评价指标</span>
</span></span><span style=display:flex><span>train_loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>Mean(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;train_loss&#39;</span>)
</span></span><span style=display:flex><span>train_accuracy <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>SparseCategoricalAccuracy(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;train_accuracy&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>test_loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>Mean(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;test_loss&#39;</span>)
</span></span><span style=display:flex><span>test_accuracy <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>SparseCategoricalAccuracy(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;test_accuracy&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义训练步</span>
</span></span><span style=display:flex><span>@tf.function
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>train_step</span>(images, labels):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> tf<span style=color:#ff79c6>.</span>GradientTape() <span style=color:#ff79c6>as</span> tape:
</span></span><span style=display:flex><span>        predictions <span style=color:#ff79c6>=</span> neural_network_advanced(images)
</span></span><span style=display:flex><span>        loss <span style=color:#ff79c6>=</span> loss_function(labels, predictions)
</span></span><span style=display:flex><span>    gradients <span style=color:#ff79c6>=</span> tape<span style=color:#ff79c6>.</span>gradient(loss, neural_network_advanced<span style=color:#ff79c6>.</span>trainable_variables)
</span></span><span style=display:flex><span>    optimizer<span style=color:#ff79c6>.</span>apply_gradients(<span style=color:#8be9fd;font-style:italic>zip</span>(gradients, neural_network_advanced<span style=color:#ff79c6>.</span>trainable_variables))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_loss(loss)
</span></span><span style=display:flex><span>    train_accuracy(labels, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义测试步</span>
</span></span><span style=display:flex><span>@tf.function
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>test_step</span>(images, labels):
</span></span><span style=display:flex><span>    predictions <span style=color:#ff79c6>=</span> neural_network_advanced(images)
</span></span><span style=display:flex><span>    loss <span style=color:#ff79c6>=</span> loss_function(labels, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    test_loss(loss)
</span></span><span style=display:flex><span>    test_accuracy(labels, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>EPOCHS <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>  <span style=color:#6272a4># 训练轮次</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> epoch <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(EPOCHS):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 每个epoch开始时将指标重置</span>
</span></span><span style=display:flex><span>    train_loss<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>    train_accuracy<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>    test_loss<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>    test_accuracy<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> images, labels <span style=color:#ff79c6>in</span> train_ds:
</span></span><span style=display:flex><span>        train_step(images, labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> test_images, test_labels <span style=color:#ff79c6>in</span> test_ds:
</span></span><span style=display:flex><span>        test_step(test_images, test_labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;Epoch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Loss: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Accuracy: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Test Loss: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Test Accuracy: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(template<span style=color:#ff79c6>.</span>format(epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, train_loss<span style=color:#ff79c6>.</span>result(), train_accuracy<span style=color:#ff79c6>.</span>result() <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>,
</span></span><span style=display:flex><span>                          test_loss<span style=color:#ff79c6>.</span>result(), test_accuracy<span style=color:#ff79c6>.</span>result() <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>100</span>))
</span></span></code></pre></div><h1 id=pytorch实现全连接神经网络进行mnist手写数字识别>Pytorch实现全连接神经网络进行MNIST手写数字识别</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> torchvision.datasets <span style=color:#ff79c6>import</span> mnist  <span style=color:#6272a4># 导入pytorch内置的mnist数据集</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torchvision.transforms <span style=color:#ff79c6>as</span> transforms  <span style=color:#6272a4># 导入对图像的预处理模块</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> torch.utils.data <span style=color:#ff79c6>import</span> DataLoader  <span style=color:#6272a4># 导入dataset的分批读取包</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> torch <span style=color:#ff79c6>import</span> nn  <span style=color:#6272a4># 导入神经网络包nn（可用来定义和运行神经网络）</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># functional中包含了神经网络中使用的一些常用函数，这些函数的特点是:不具有可学习的参数(如ReLU，pool，DropOut等)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.nn.functional <span style=color:#ff79c6>as</span> F
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.optim <span style=color:#ff79c6>as</span> optim  <span style=color:#6272a4># optim中实现了大多数的优化方法来更新网络权重和参数，如SGD、Adam</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义代码中用到的各个超参数</span>
</span></span><span style=display:flex><span>train_batch_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>64</span>  <span style=color:#6272a4># 指定DataLoader在训练集中每批加载的样本数量</span>
</span></span><span style=display:flex><span>test_batch_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>  <span style=color:#6272a4># 指定DataLoader在测试集中每批加载的样本数量</span>
</span></span><span style=display:flex><span>num_epoches <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>20</span>  <span style=color:#6272a4># 模型训练轮数</span>
</span></span><span style=display:flex><span>lr <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.01</span>  <span style=color:#6272a4># 设置SGD中的初始学习率</span>
</span></span><span style=display:flex><span>momentum <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.5</span>  <span style=color:#6272a4># 设置SGD中的冲量</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 对数据进行预处理</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Compose方法即是将两个操作合并一起</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用minst数据集的均值和标准差将数据标准化处理</span>
</span></span><span style=display:flex><span>transform <span style=color:#ff79c6>=</span> transforms<span style=color:#ff79c6>.</span>Compose([transforms<span style=color:#ff79c6>.</span>ToTensor(), transforms<span style=color:#ff79c6>.</span>Normalize([<span style=color:#bd93f9>0.1307</span>], [<span style=color:#bd93f9>0.3081</span>])])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 下载和分批加载数据集</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 将训练和测试数据集下载到同目录下的mnist_data文件夹下</span>
</span></span><span style=display:flex><span>train_dataset <span style=color:#ff79c6>=</span> mnist<span style=color:#ff79c6>.</span>MNIST(<span style=color:#f1fa8c>&#39;.\mnist_data&#39;</span>, train<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, transform<span style=color:#ff79c6>=</span>transform, download<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>test_dataset <span style=color:#ff79c6>=</span> mnist<span style=color:#ff79c6>.</span>MNIST(<span style=color:#f1fa8c>&#39;.\mnist_data&#39;</span>, train<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>, transform<span style=color:#ff79c6>=</span>transform, download<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># dataloader是一个可迭代对象，可以使用迭代器一样使用。其中shuffle参数为是否打乱原有数据顺序</span>
</span></span><span style=display:flex><span>train_loader <span style=color:#ff79c6>=</span> DataLoader(train_dataset, batch_size<span style=color:#ff79c6>=</span>train_batch_size, shuffle<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>test_loader <span style=color:#ff79c6>=</span> DataLoader(test_dataset, batch_size<span style=color:#ff79c6>=</span>test_batch_size, shuffle<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义一个神经网络模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Net</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, in_dim, n_hidden_1, n_hidden_2, out_dim):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(Net, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer1 <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Sequential(nn<span style=color:#ff79c6>.</span>Linear(in_dim, n_hidden_1), nn<span style=color:#ff79c6>.</span>ReLU(<span style=color:#ff79c6>True</span>))
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer2 <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Sequential(nn<span style=color:#ff79c6>.</span>Linear(n_hidden_1, n_hidden_2), nn<span style=color:#ff79c6>.</span>ReLU(<span style=color:#ff79c6>True</span>))
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer3 <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Linear(n_hidden_2, out_dim)  <span style=color:#6272a4># 最后一层接Softmax所以不需要ReLU激活</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer1(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layer3(x)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Sequential() 即相当于把多个模块按顺序封装成一个模块</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 实例化网络模型</span>
</span></span><span style=display:flex><span>device <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>device(<span style=color:#f1fa8c>&#39;cuda:0&#39;</span> <span style=color:#ff79c6>if</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_available() <span style=color:#ff79c6>else</span> <span style=color:#f1fa8c>&#39;cpu&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 网络模型参数分别为：输入层大小、隐藏层1大小、隐藏层2大小、输出层大小（10分类）</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> Net(<span style=color:#bd93f9>28</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>28</span>, <span style=color:#bd93f9>300</span>, <span style=color:#bd93f9>100</span>, <span style=color:#bd93f9>10</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 将模型移动到GPU加速计算</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义模型训练中用到的损失函数和优化器</span>
</span></span><span style=display:flex><span>criterion <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>CrossEntropyLoss()  <span style=color:#6272a4># 交叉熵损失函数</span>
</span></span><span style=display:flex><span>optimizer <span style=color:#ff79c6>=</span> optim<span style=color:#ff79c6>.</span>SGD(model<span style=color:#ff79c6>.</span>parameters(), lr<span style=color:#ff79c6>=</span>lr, momentum<span style=color:#ff79c6>=</span>momentum)
</span></span><span style=display:flex><span><span style=color:#6272a4># parameters()将model中可优化的参数传入到SGD中</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 对模型进行训练</span>
</span></span><span style=display:flex><span>losses <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 记录训练集损失</span>
</span></span><span style=display:flex><span>acces <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 记录训练集准确率</span>
</span></span><span style=display:flex><span>eval_losses <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 记录测试集损失</span>
</span></span><span style=display:flex><span>eval_acces <span style=color:#ff79c6>=</span> []  <span style=color:#6272a4># 记录测试集准确率</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> epoch <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(num_epoches):
</span></span><span style=display:flex><span>    train_loss <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    train_acc <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>train()  <span style=color:#6272a4># 指明接下来model进行的是训练过程</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 动态修改参数学习率</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> epoch <span style=color:#ff79c6>%</span> <span style=color:#bd93f9>5</span> <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>        optimizer<span style=color:#ff79c6>.</span>param_groups[<span style=color:#bd93f9>0</span>][<span style=color:#f1fa8c>&#39;lr&#39;</span>] <span style=color:#ff79c6>*=</span> <span style=color:#bd93f9>0.9</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> img, label <span style=color:#ff79c6>in</span> train_loader:
</span></span><span style=display:flex><span>        img <span style=color:#ff79c6>=</span> img<span style=color:#ff79c6>.</span>to(device)  <span style=color:#6272a4># 将img移动到GPU计算</span>
</span></span><span style=display:flex><span>        label <span style=color:#ff79c6>=</span> label<span style=color:#ff79c6>.</span>to(device)
</span></span><span style=display:flex><span>        img <span style=color:#ff79c6>=</span> img<span style=color:#ff79c6>.</span>view(img<span style=color:#ff79c6>.</span>size(<span style=color:#bd93f9>0</span>), <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>)  <span style=color:#6272a4># 把输入图像的维度由四维转化为2维，因为在torch中只能处理二维数据</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># img.size(0)为取size的第0个参数即此批样本的个数，-1为自适应参数</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 前向传播</span>
</span></span><span style=display:flex><span>        out <span style=color:#ff79c6>=</span> model(img)
</span></span><span style=display:flex><span>        loss <span style=color:#ff79c6>=</span> criterion(out, label)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 反向传播</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#ff79c6>.</span>zero_grad()  <span style=color:#6272a4># 先清空上一轮的梯度</span>
</span></span><span style=display:flex><span>        loss<span style=color:#ff79c6>.</span>backward()  <span style=color:#6272a4># 根据前向传播得到损失，再由损失反向传播求得各个梯度</span>
</span></span><span style=display:flex><span>        optimizer<span style=color:#ff79c6>.</span>step()  <span style=color:#6272a4># 根据反向传播得到的梯度优化模型中的参数</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        train_loss <span style=color:#ff79c6>+=</span> loss<span style=color:#ff79c6>.</span>item()  <span style=color:#6272a4># 所有批次损失的和</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 计算分类的准确率</span>
</span></span><span style=display:flex><span>        _, pred <span style=color:#ff79c6>=</span> out<span style=color:#ff79c6>.</span>max(<span style=color:#bd93f9>1</span>)  <span style=color:#6272a4># 返回输出二维矩阵中每一行的最大值及其下标，1含义为以第1个维度（列）为参考</span>
</span></span><span style=display:flex><span>        num_correct <span style=color:#ff79c6>=</span> (pred <span style=color:#ff79c6>==</span> label)<span style=color:#ff79c6>.</span>sum()<span style=color:#ff79c6>.</span>item()
</span></span><span style=display:flex><span>        acc <span style=color:#ff79c6>=</span> num_correct <span style=color:#ff79c6>/</span> img<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]  <span style=color:#6272a4># 每一批样本的准确率</span>
</span></span><span style=display:flex><span>        train_acc <span style=color:#ff79c6>+=</span> acc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    losses<span style=color:#ff79c6>.</span>append(train_loss <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(train_loader))  <span style=color:#6272a4># 所有样本平均损失</span>
</span></span><span style=display:flex><span>    acces<span style=color:#ff79c6>.</span>append(train_acc <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(train_loader))  <span style=color:#6272a4># 所有样本的准确率</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 运用训练好的模型在测试集上检验效果</span>
</span></span><span style=display:flex><span>    eval_loss <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    eval_acc <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将模型改为预测模式</span>
</span></span><span style=display:flex><span>    model<span style=color:#ff79c6>.</span>eval()  <span style=color:#6272a4># 指明接下来要进行模型测试（不需要反向传播）</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># with torch.no_grad():</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> img, label <span style=color:#ff79c6>in</span> test_loader:
</span></span><span style=display:flex><span>        img <span style=color:#ff79c6>=</span> img<span style=color:#ff79c6>.</span>to(device)
</span></span><span style=display:flex><span>        label <span style=color:#ff79c6>=</span> label<span style=color:#ff79c6>.</span>to(device)
</span></span><span style=display:flex><span>        img <span style=color:#ff79c6>=</span> img<span style=color:#ff79c6>.</span>view(img<span style=color:#ff79c6>.</span>size(<span style=color:#bd93f9>0</span>), <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        out <span style=color:#ff79c6>=</span> model(img)
</span></span><span style=display:flex><span>        loss <span style=color:#ff79c6>=</span> criterion(out, label)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 记录误差</span>
</span></span><span style=display:flex><span>        eval_loss <span style=color:#ff79c6>+=</span> loss<span style=color:#ff79c6>.</span>item()
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 记录准确率</span>
</span></span><span style=display:flex><span>        _, pred <span style=color:#ff79c6>=</span> out<span style=color:#ff79c6>.</span>max(<span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        num_correct <span style=color:#ff79c6>=</span> (pred <span style=color:#ff79c6>==</span> label)<span style=color:#ff79c6>.</span>sum()<span style=color:#ff79c6>.</span>item()
</span></span><span style=display:flex><span>        acc <span style=color:#ff79c6>=</span> num_correct <span style=color:#ff79c6>/</span> img<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        eval_acc <span style=color:#ff79c6>+=</span> acc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    eval_losses<span style=color:#ff79c6>.</span>append(eval_loss <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(test_loader))
</span></span><span style=display:flex><span>    eval_acces<span style=color:#ff79c6>.</span>append(eval_acc <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(test_loader))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;epoch: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Train Loss: </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>, Train Acc: </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>, Test Loss: </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>, Test Acc: </span><span style=color:#f1fa8c>{:.4f}</span><span style=color:#f1fa8c>&#39;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>.</span>format(epoch, train_loss <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(train_loader), train_acc <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(train_loader),
</span></span><span style=display:flex><span>                  eval_loss <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(test_loader), eval_acc <span style=color:#ff79c6>/</span> <span style=color:#8be9fd;font-style:italic>len</span>(test_loader)))
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li><p>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</p></li><li><p>神经网络的可解释性综述：https://blog.csdn.net/datawhale/article/details/119066418</p></li><li><p>神经网络维基百科：https://en.wikipedia.org/wiki/Neural_network</p></li><li><p>Pytorch实现手写数字识别（基于全连接神经网络）：https://blog.csdn.net/weixin_44851176/article/details/125858319</p></li></ul><hr><ul class=pager><li class=previous><a href=/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/ data-toggle=tooltip data-placement=top title=深度学习概述>&larr;
Previous Post</a></li><li class=next><a href=/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ data-toggle=tooltip data-placement=top title=深度学习：卷积神经网络>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>