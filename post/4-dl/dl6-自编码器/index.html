<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg><meta property="twitter:image" content="https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg"><meta name=title content="深度学习：自编码器"><meta property="og:title" content="深度学习：自编码器"><meta property="twitter:title" content="深度学习：自编码器"><meta name=description content="本文主要介绍自编码器的基本原理和Tensorflow实现。"><meta property="og:description" content="本文主要介绍自编码器的基本原理和Tensorflow实现。"><meta property="twitter:description" content="本文主要介绍自编码器的基本原理和Tensorflow实现。"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>深度学习：自编码器-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.pexels.com/photos/220301/pexels-photo-220301.jpeg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model
</a><a class=tag href=/tags/deep-learning title="Deep Learning">Deep Learning</a></div><h1>深度学习：自编码器</h1><h2 class=subheading>Auto-Encoder</h2><span class=meta>Posted by
XiangdiWu
on
Friday, October 16, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=自编码器>自编码器</h1><p><strong>自编码器(auto-encoder, AE)</strong> 是通过<strong>无监督</strong>的方式来学习<strong>一组数据的有效编码(或表示)</strong>。</p><p>假设有一组$d$维的样本$\boldsymbol{x}^{(n)} \in \mathbb{R}^{d}, 1 \leqslant n \leqslant N$，自编码器将这组数据映射到$p$维的<strong>特征空间</strong>得到每个样本的编码$\boldsymbol{z}^{(n)} \in \mathbb{R}^{p}, 1 \leqslant n \leqslant N$，并且希望<strong>这组编码可以重构出原来的样本</strong>。自编码器的结构可分为两部分：</p><p><strong>编码器(encoder)</strong>：$f:\mathbb R^d \rightarrow \mathbb R^p$；</p><p><strong>解码器(decoder)</strong>：$g:\mathbb R^p \rightarrow \mathbb R^d$。</p><p>自编码器的学习目标是<strong>最小化重构错误(reconstruction error)</strong>：</p>$$
\begin{aligned}
\mathcal{L} &=\sum_{n=1}^{N}\|\boldsymbol{x}^{(n)}-g(f(\boldsymbol{x}^{(n)}))\|^{2} \\
&=\sum_{n=1}^{N}\|\boldsymbol{x}^{(n)}-f \circ g(\boldsymbol{x}^{(n)})\|^{2}
\end{aligned}
$$<p>如果特征空间的维度$p$小于原始空间的维度$d$，自编码器相当于是一种降维或特征抽取方法。如果$p \geqslant d$，一定可以找到一组或多组解使得$f \circ g$为<strong>单位函数(identity function)</strong>$I(x)=x$，并使得重构错误为0。然而，这样的解并没有太多的意义。但是，如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的稀疏性、取值范围，$f$和$g$的具体形式等。如果我们让编码只能取个$k$不同的值($k < N$)，那么自编码器就成为一个$k$类的<strong>聚类问题</strong>。</p><p>最简单的自编码器是如下图所示的两层神经网络。输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。</p><div align=center><img src=/Kimages/3/image-20200806104330514.png style=zoom:25%></div><p>我们使用自编码器是为了得到有效的数据表示，因此在训练结束后，<strong>一般会去掉解码器，只保留编码器</strong>。编码器的输出可以直接作为后续机器学习模型的输入。</p><h1 id=稀疏自编码器>稀疏自编码器</h1><p>自编码器除了可以学习低维编码之外，也能够学习<strong>高维的稀疏编码</strong>。假设中间隐藏层$\boldsymbol z$的维度$p$大于输入样本$\boldsymbol x$的维度$d$，并且让$\boldsymbol z$尽量系数，这就是<strong>稀疏自编码器(sparse auto-encoder, SAE)</strong>。稀疏自编码器的优点是有很高的可解释性，并同时进行了<strong>隐式的特征选择</strong>。通过给自编码器中隐藏层单元$\boldsymbol z$加上稀疏性限制，自编码器可以学习到数据中一些有用的结构。给定$N$个训练样本$\{\boldsymbol{x}^{(n)}\}_{n=1}^{N}$，稀疏自编码器的目标函数为：</p>$$
\mathcal{L}=\sum_{n=1}^{N} \| \boldsymbol{x}^{(n)}-\boldsymbol{x}^{\prime (n)}\|^{2}+\eta \rho(Z)+\lambda\| W \|^{2}
$$<p>其中$Z=[\boldsymbol{z}^{(1)}, \cdots, \boldsymbol{z}^{(N)}]$表示所有训练样本的编码，$\rho(Z)$为稀疏性度量函数，$W$表示自编码器中的参数。$\rho(Z)$可以定义为一组训练样本中每一个神经元激活的概率。</p><p>给定$N$个训练样本，隐藏层第$j$个神经元平均活性值为：</p>$$
\hat{\rho}_{j}=\frac{1}{N} \sum_{n=1}^{N} z_{j}^{(n)}
$$$$\hat{\rho}_{j}$$<p>可以近似地看作是第$j$个神经元激活的概率。我们希望$ \hat{\rho}_{j} $接近于一个事先给定的值$\rho^* $，例如0.05，可以通过KL举例来衡量$\hat{\rho}_{j}$和$\rho^* $的差异，即</p>$$
\mathrm{KL}(\rho^{*} \| \hat{\rho}_{j})=\rho^{*} \log \frac{\rho^{*}}{\hat{\rho}_{j}}+(1-\rho^{*}) \log \frac{1-\rho^{*}}{1-\hat{\rho}_{j}}
$$<p>如果量$\hat{\rho}_{j}=\rho^*$，则$\mathrm{KL}(\rho^{*} \| \hat{\rho}_{j})=0$。</p><p>稀疏性度量函数的定义为：</p>$$
\rho(Z)=\sum_{j=1}^{p} \mathrm{KL}(\rho^{*} \| \hat{\rho}_{j})
$$<h1 id=堆叠自编码器>堆叠自编码器</h1><p>对于很多数据来说，仅使用两层神经网络的自编码器还不足以获取一种好的数据表示。为了获取更好的数据表示，我们可以使用更深层的神经网络。深层神经网络作为自编码器提取的数据表示一般会更加抽象，能够更好地捕捉到数据的语义信息。在实践中经常使用逐层堆叠的方式来训练一个深层的自编码器，称为<strong>堆叠自编码器(stacked auto-encoder, SAE)</strong>。堆叠自编码器一般可以采用<strong>逐层训练(layer-wise training)</strong> 来学习网络参数。</p><h1 id=降噪自编码器>降噪自编码器</h1><p>我们使用自编码器是为了得到有效的数据表示，而有效的数据表示除了具有最小重构错误或稀疏性等性质之外，还可以要求其具备其它性质，比如对<strong>数据部分损坏(partial destruction)</strong> 的鲁棒性。高维数据(比如图像)一般都具有一定的信息冗余，比如我们可以根据一张部分破损的图像联想出其完整内容。因此，我们希望自编码器也能够<strong>从部分损坏的数据中得到有效的数据表示，并能够恢复出完整的原始信息</strong>。</p><p><strong>降噪自编码器(denoising auto-encoder, DAE)</strong> 就是一种通过引入噪声来增加编码鲁棒性的自编码器。对于一个向量$\boldsymbol x$，我们首先根据一个比例$\mu$随机将$\boldsymbol x$的<strong>一些维度的值设置为0</strong>，得到一个<strong>被损坏的向量</strong>$\tilde{\boldsymbol{x}}$。然后将被损坏的向量$\tilde{\boldsymbol{x}}$输入给自编码器得到编码$\boldsymbol z$，并重构出原始的无损输入$\boldsymbol x$。</p><p>下图给出了自编码器和降噪自编码器的对比图：</p><div align=center><img src=/Kimages/3/image-20200806110358002.png style=zoom:30%></div><p>降噪自编码器的思想十分简单，通过引入噪声来学习更鲁棒性的数据编码，并提高模型的泛化能力。</p><h1 id=tensroflow实现自编码器完成光谱数据的降维与重构>Tensroflow实现自编码器完成光谱数据的降维与重构</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> tensorflow <span style=color:#ff79c6>as</span> tf
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> scipy.io <span style=color:#ff79c6>as</span> sio
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> numpy <span style=color:#ff79c6>as</span> np
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> matplotlib.pyplot <span style=color:#ff79c6>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据获取，需要将数据导入当前目录</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># A F K M类恒星数据各6000条</span>
</span></span><span style=display:flex><span>X_a <span style=color:#ff79c6>=</span> sio<span style=color:#ff79c6>.</span>loadmat(<span style=color:#f1fa8c>&#39;spectra_data\A.mat&#39;</span>)[<span style=color:#f1fa8c>&#39;P1&#39;</span>]
</span></span><span style=display:flex><span>X_f <span style=color:#ff79c6>=</span> sio<span style=color:#ff79c6>.</span>loadmat(<span style=color:#f1fa8c>&#39;spectra_data\F.mat&#39;</span>)[<span style=color:#f1fa8c>&#39;P1&#39;</span>]
</span></span><span style=display:flex><span>X_k <span style=color:#ff79c6>=</span> sio<span style=color:#ff79c6>.</span>loadmat(<span style=color:#f1fa8c>&#39;spectra_data\K.mat&#39;</span>)[<span style=color:#f1fa8c>&#39;P1&#39;</span>]
</span></span><span style=display:flex><span>X_m <span style=color:#ff79c6>=</span> sio<span style=color:#ff79c6>.</span>loadmat(<span style=color:#f1fa8c>&#39;spectra_data\M.mat&#39;</span>)[<span style=color:#f1fa8c>&#39;P1&#39;</span>]
</span></span><span style=display:flex><span>X_label <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#39;A&#39;</span>, <span style=color:#f1fa8c>&#39;F&#39;</span>, <span style=color:#f1fa8c>&#39;K&#39;</span>, <span style=color:#f1fa8c>&#39;M&#39;</span>]
</span></span><span style=display:flex><span>X <span style=color:#ff79c6>=</span> np<span style=color:#ff79c6>.</span>vstack((X_a, X_f, X_k, X_m))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据归一化</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(X<span style=color:#ff79c6>.</span>shape[<span style=color:#bd93f9>0</span>]):
</span></span><span style=display:flex><span>    X[i] <span style=color:#ff79c6>-=</span> np<span style=color:#ff79c6>.</span>min(X[i])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> np<span style=color:#ff79c6>.</span>max(X[i]) <span style=color:#ff79c6>!=</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>        X[i] <span style=color:#ff79c6>/=</span> np<span style=color:#ff79c6>.</span>max(X[i])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#39;Data shape: &#39;</span>, X<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 转为tf.data.Dataset格式</span>
</span></span><span style=display:flex><span>data <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>data<span style=color:#ff79c6>.</span>Dataset<span style=color:#ff79c6>.</span>from_tensor_slices(X)<span style=color:#ff79c6>.</span>shuffle(<span style=color:#bd93f9>24000</span>)<span style=color:#ff79c6>.</span>batch(<span style=color:#bd93f9>64</span>, drop_remainder<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义自编码器模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>AutoEncoder</span>(tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>Model):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(AutoEncoder, <span style=font-style:italic>self</span>)<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder_1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>64</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder_2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>10</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder_1 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>64</span>)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder_2 <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>layers<span style=color:#ff79c6>.</span>Dense(<span style=color:#bd93f9>3522</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder_1(x)
</span></span><span style=display:flex><span>        coding <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>encoder_2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder_1(coding)
</span></span><span style=display:flex><span>        rebuild <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>decoder_2(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> coding, rebuild
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ae <span style=color:#ff79c6>=</span> AutoEncoder()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义损失函数和优化器</span>
</span></span><span style=display:flex><span>loss_func <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>losses<span style=color:#ff79c6>.</span>MeanSquaredError()
</span></span><span style=display:flex><span>optimizer <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>optimizers<span style=color:#ff79c6>.</span>Adam()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loss <span style=color:#ff79c6>=</span> tf<span style=color:#ff79c6>.</span>keras<span style=color:#ff79c6>.</span>metrics<span style=color:#ff79c6>.</span>Mean(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;train_loss&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tf.function
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>train_step</span>(batch_data):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>with</span> tf<span style=color:#ff79c6>.</span>GradientTape() <span style=color:#ff79c6>as</span> tape:
</span></span><span style=display:flex><span>        coding, rebuild <span style=color:#ff79c6>=</span> ae(batch_data)
</span></span><span style=display:flex><span>        loss <span style=color:#ff79c6>=</span> loss_func(batch_data, rebuild)
</span></span><span style=display:flex><span>    gradients <span style=color:#ff79c6>=</span> tape<span style=color:#ff79c6>.</span>gradient(loss, ae<span style=color:#ff79c6>.</span>trainable_variables)
</span></span><span style=display:flex><span>    optimizer<span style=color:#ff79c6>.</span>apply_gradients(<span style=color:#8be9fd;font-style:italic>zip</span>(gradients, ae<span style=color:#ff79c6>.</span>trainable_variables))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_loss(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>EPOCHS <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> epoch <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(EPOCHS):
</span></span><span style=display:flex><span>    train_loss<span style=color:#ff79c6>.</span>reset_states()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> batch_data <span style=color:#ff79c6>in</span> data:
</span></span><span style=display:flex><span>        train_step(batch_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;Epoch </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>, Loss: </span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(template<span style=color:#ff79c6>.</span>format(epoch <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>, train_loss<span style=color:#ff79c6>.</span>result()))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample <span style=color:#ff79c6>=</span> X[:<span style=color:#bd93f9>5</span>]
</span></span><span style=display:flex><span>sample_coding, sample_rebuild <span style=color:#ff79c6>=</span> ae(sample)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(sample<span style=color:#ff79c6>.</span>shape, sample_coding<span style=color:#ff79c6>.</span>shape, sample_rebuild<span style=color:#ff79c6>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>5</span>):
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>5</span>, i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>plot(sample[i])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>subplot(<span style=color:#bd93f9>2</span>, <span style=color:#bd93f9>5</span>, i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>5</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#ff79c6>.</span>plot(sample_rebuild[i])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#ff79c6>.</span>show()
</span></span></code></pre></div><h1 id=参考资料>参考资料</h1><ul><li>邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.</li><li>AE、DAE、SAE和VAE简介：https://www.cnblogs.com/jins-note/p/12883863.html</li><li>自编码器维基百科：https://en.wikipedia.org/wiki/Autoencoder</li></ul><hr><ul class=pager><li class=previous><a href=/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/ data-toggle=tooltip data-placement=top title=深度学习：神经网络的优化>&larr;
Previous Post</a></li><li class=next><a href=/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/ data-toggle=tooltip data-placement=top title=深度学习：深度生成模型>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>