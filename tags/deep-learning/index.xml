<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Xiangdi Blog</title><link>https://xiangdiwu.github.io/tags/deep-learning/</link><description>Recent content in Deep Learning on Xiangdi Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 19 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://xiangdiwu.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>深度学习：图神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="图神经网络概述"&gt;图神经网络概述&lt;/h1&gt;
&lt;p&gt;近年来，深度学习领域关于&lt;strong&gt;图神经网络(graph neural network, GNN)&lt;/strong&gt; 的研究热情日益高涨，图神经网络已经成为各大深度学习顶会的研究热点。GNN处理非结构化数据时的出色能力使其在网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题方面都取得了新的突破。&lt;/p&gt;</description></item><item><title>深度学习：注意力机制与外部记忆</title><link>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</guid><description>&lt;p&gt;根据通用近似定理，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。&lt;/p&gt;</description></item><item><title>深度学习：深度生成模型</title><link>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率生成模型"&gt;概率生成模型&lt;/h1&gt;
&lt;p&gt;概率生成模型，简称&lt;strong&gt;生成模型(generative model)&lt;/strong&gt;，是概率统计和机器学习中的一类重要模型，指一系列用于&lt;strong&gt;随机生成&lt;/strong&gt;可观测数据的模型。假设在一个连续的或离散的高维空间$\mathcal X$中，存在一个随机向量$\boldsymbol X$服从一个未知的数据分布$p_r(\boldsymbol x),\boldsymbol x \in \mathcal X$。生成模型是根据一些可观测的样本$\boldsymbol x^{(1)},\boldsymbol x^{(2)},\cdots,\boldsymbol x^{(N)}$来学习一个参数化的模型$p_\theta(\boldsymbol x)$来近似未知分布$p_r(\boldsymbol x)$，并可以用这个模型来生成一些样本，使得&lt;strong&gt;生成样本和真实样本尽可能地相似&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：自编码器</title><link>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</guid><description>&lt;h1 id="自编码器"&gt;自编码器&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;自编码器(auto-encoder, AE)&lt;/strong&gt; 是通过&lt;strong&gt;无监督&lt;/strong&gt;的方式来学习&lt;strong&gt;一组数据的有效编码(或表示)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设有一组$d$维的样本$\boldsymbol{x}^{(n)} \in \mathbb{R}^{d}, 1 \leqslant n \leqslant N$，自编码器将这组数据映射到$p$维的&lt;strong&gt;特征空间&lt;/strong&gt;得到每个样本的编码$\boldsymbol{z}^{(n)} \in \mathbb{R}^{p}, 1 \leqslant n \leqslant N$，并且希望&lt;strong&gt;这组编码可以重构出原来的样本&lt;/strong&gt;。自编码器的结构可分为两部分：&lt;/p&gt;</description></item><item><title>深度学习：神经网络的优化</title><link>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</guid><description>&lt;h1 id="网络优化的难点"&gt;网络优化的难点&lt;/h1&gt;
&lt;h2 id="网络结构多样性"&gt;网络结构多样性&lt;/h2&gt;
&lt;p&gt;神经网络的种类非常多，比如卷积网络、循环网络等，其结构也非常不同。有些比较深，有些比较宽。不同参数在网络中的作用也有很大的差异，比如连接权重和偏置的不同，以及循环网络中循环连接上的权重和其它权重的不同。由于网络结构的多样性，我们很难找到一种通用的优化方法。&lt;strong&gt;不同的优化方法在不同网络结构上的差异也都比较大&lt;/strong&gt;。此外，网络的超参数一般也比较多，这也给优化带来很大的挑战。&lt;/p&gt;</description></item><item><title>深度学习：循环神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="序列数据和语言模型"&gt;序列数据和语言模型&lt;/h1&gt;
&lt;p&gt;全连接神经网络和卷积神经网络只能单独处理一个个的输入，&lt;strong&gt;前一个输入和后一个输入是完全没有关系的&lt;/strong&gt;。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要&lt;strong&gt;分析这些帧连接起来的整个序列&lt;/strong&gt;。这时，就需要用到深度学习领域中另一类非常重要神经网络：&lt;strong&gt;循环神经网络(recurrent neural network)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：卷积神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;卷积神经网络(convolutional neural network, CNN)&lt;/strong&gt; 是一种具有局部连接、权重共享等特性的前馈神经网络。&lt;/p&gt;
&lt;p&gt;卷积神经网络最早是主要用来处理图像信息。在用全连接前馈网络来处理图像时，会存在参数太多、局部特征不变形等缺陷。卷积神经网络利用&lt;strong&gt;局部连接、权重共享以及汇聚&lt;/strong&gt;三大结构上的特性，使得数据具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。&lt;/p&gt;</description></item><item><title>深度学习：神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;人工神经网络(artificial neural network, ANN)&lt;/strong&gt; 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象，构建&lt;strong&gt;人工神经元&lt;/strong&gt;，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为&lt;strong&gt;神经网络(neural network, NN)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习概述</title><link>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</guid><description>&lt;h1 id="表示学习"&gt;表示学习&lt;/h1&gt;
&lt;p&gt;为了提高机器学习系统的准确率，需要将输入信息转换为有效的特征，或者更一般称为&lt;strong&gt;表示(representation)&lt;/strong&gt;。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就是可以叫做&lt;strong&gt;表示学习(representation learning)&lt;/strong&gt;。&lt;/p&gt;</description></item></channel></rss>