<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Xiangdi Blog</title><link>https://xiangdiwu.github.io/tags/machine-learning/</link><description>Recent content in Machine Learning on Xiangdi Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 10 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://xiangdiwu.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>机器学习：概率图模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率图模型概述"&gt;概率图模型概述&lt;/h1&gt;
&lt;div align="center"&gt;
&lt;img src="https://xiangdiwu.github.io/Kimages/2/image-20200818114353404.png" style="zoom:40%;" /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;概率图模型(probabilistic graphical model, PGM)&lt;/strong&gt;，简称图模型(graphical model, GM)，是指一种&lt;strong&gt;用图结构来描述多元随机变量之间条件独立关系的概率模型&lt;/strong&gt;，从而给研究&lt;strong&gt;高维空间中的概率模型&lt;/strong&gt;带来了很大的便捷性。&lt;/p&gt;</description></item><item><title>机器学习：话题模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="单词向量空间与话题向量空间"&gt;单词向量空间与话题向量空间&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;潜在语义分析(latent semantic analysis, LSA)&lt;/strong&gt; 是一种无监督学习方法，主要用于文本的话题分析，其特点是通过&lt;strong&gt;矩阵分解&lt;/strong&gt;发现&lt;strong&gt;文本与单词之间基于话题的语义关系&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;文本信息处理中，传统的方法以&lt;strong&gt;单词向量&lt;/strong&gt;表示文本的语义内容，以&lt;strong&gt;单词向量空间的度量&lt;/strong&gt;表示&lt;strong&gt;文本之间的语义相似度&lt;/strong&gt;。潜在语义分析旨在解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现&lt;strong&gt;潜在的话题&lt;/strong&gt;，以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度。这也是&lt;strong&gt;话题分析(topic modeling)的基本想法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习：特征选择</title><link>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid><description>&lt;h1 id="子集搜索与评价"&gt;子集搜索与评价&lt;/h1&gt;
&lt;p&gt;给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。对当前学习任务有用的属性称为&lt;strong&gt;相关特征(relevant feature)&lt;/strong&gt;，无用的属性称为&lt;strong&gt;无关特征(irrelevant feature)&lt;/strong&gt;。&lt;strong&gt;特征选择(feature selection)&lt;/strong&gt; 是一个重要的数据预处理过程，指从给定的特征集合中选择出相关特征的子集的过程。特征选择能够减少维数灾难问题，同时降低学习任务的难度。&lt;/p&gt;</description></item><item><title>机器学习：聚类</title><link>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</guid><description>&lt;h1 id="聚类的基本概念"&gt;聚类的基本概念&lt;/h1&gt;
&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt; 中，训练样本的标记信息是未知的，目标是通过对无标记样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。聚类(clustering)任务是一种常见的无监督学习方法。&lt;/p&gt;</description></item><item><title>机器学习：降维</title><link>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</guid><description>&lt;h1 id="主成分分析"&gt;主成分分析&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;主成分分析(principal component analysis, PCA)&lt;/strong&gt; 是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如下图所示的二维数据，如果将这些数据投影到一维空间，选择数据方差最大的方向进行投影(蓝轴)，才能最大化数据的差异性，保留更多的原始数据信息。&lt;/p&gt;</description></item><item><title>机器学习：EM算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="em算法的引入"&gt;EM算法的引入&lt;/h1&gt;
&lt;p&gt;概率模型有时既含观测变量，又含隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。&lt;strong&gt;EM算法&lt;/strong&gt;就是&lt;strong&gt;含有隐变量的概率模型参数的极大似然估计法&lt;/strong&gt;，或&lt;strong&gt;极大后验概率估计法&lt;/strong&gt;。我们仅讨论极大似然估计，极大后验概率估计与其类似。&lt;/p&gt;</description></item><item><title>机器学习：集成学习</title><link>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="个体与集成"&gt;个体与集成&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;集成学习(ensemble learning)&lt;/strong&gt; 通过构建并结合多个学习期来完成学习任务，有时也被称为&lt;strong&gt;多分类器系统(multi-classifier system)&lt;/strong&gt;、&lt;strong&gt;基于委员会的学习(committee-based learning)&lt;/strong&gt; 等。&lt;/p&gt;</description></item><item><title>机器学习：决策树</title><link>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</guid><description>&lt;h1 id="决策树模型"&gt;决策树模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;决策树(decision tree)&lt;/strong&gt; 是一种基本的&lt;strong&gt;分类&lt;/strong&gt;与&lt;strong&gt;回归&lt;/strong&gt;算法。决策树呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，它可以认为是&lt;strong&gt;if-then规则&lt;/strong&gt;的集合，也可以认为是定义在特征空间与类空间上的&lt;strong&gt;条件概率分布&lt;/strong&gt;。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用训练好的决策模型进行分类。&lt;/p&gt;</description></item><item><title>机器学习：支持向量机</title><link>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid><description>&lt;h1 id="线性支持向量机与硬间隔最大化"&gt;线性支持向量机与硬间隔最大化&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;支持向量机(support vector machine, SVM)&lt;/strong&gt; 是一个经典的机器学习二分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强的优势。&lt;/p&gt;</description></item><item><title>机器学习：逻辑回归与最大熵模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="逻辑回归模型"&gt;逻辑回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;逻辑回归(logistic regression, LR)&lt;/strong&gt; 模型是一种处理&lt;strong&gt;二分类&lt;/strong&gt;问题的线性模型。逻辑回归模型由logistic分布(logistic distribution)导出。设$X$是连续随机变量，$X$服从logistic分布是指$X$具有下列&lt;strong&gt;分布函数&lt;/strong&gt;和&lt;strong&gt;密度函数&lt;/strong&gt;：
&lt;/p&gt;</description></item><item><title>机器学习：感知机</title><link>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>&lt;h1 id="感知机算法"&gt;感知机算法&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;感知机(perceptron)&lt;/strong&gt; 由Frank Rosenblatt于1957年提出，是一种广泛使用的线性分类器。感知器可谓是最简单的人工神经网络，只有一个神经元，是对生物神经元的简单数学模拟，有与生物神经元相对应的部件，如&lt;strong&gt;权重(突触)、偏置(阈值)及激活函数(细胞体)&lt;/strong&gt;，输出为+1或-1。&lt;/p&gt;</description></item><item><title>机器学习：贝叶斯分类器</title><link>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid><description>&lt;h1 id="朴素贝叶斯的学习与分类"&gt;朴素贝叶斯的学习与分类&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯(naive Bayse)&lt;/strong&gt; 算法是基于&lt;strong&gt;贝叶斯定理&lt;/strong&gt;与&lt;strong&gt;特征条件独立假设&lt;/strong&gt;的分类方法。设输入空间$\mathcal X \subseteq \mathbb R^n$为$n$维向量的集合，输出空间为类标记集合$\mathcal Y={c_1,c_2,\cdots,c_K}$。输入为特征向量$x \in \mathcal X$，输出为类标记$y \in \mathcal Y$。$P(X,Y)$是输入空间和输出空间上的随机变量$X$和$Y$的联合概率分布，训练数据集(含$N$个数据)由$P(X,Y)$独立同分布产生。朴素贝叶斯在数据集上学习&lt;strong&gt;联合概率分布&lt;/strong&gt;$P(X,Y)$。具体地，先学习以下先验概率分布及条件概率分布：&lt;/p&gt;</description></item><item><title>机器学习：K近邻算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="近邻算法原理"&gt;$\boldsymbol k$近邻算法原理&lt;/h1&gt;
&lt;p&gt;$k$近邻($k$-nearest neighbor, $k$NN)算法是一种常用的监督学习方法。其基本思想为：给定一组数据，基于某种距离度量找出训练集中与其最靠近的$k$个训练样本，然后基于这$k$个邻居的信息来进行预测。通常，在&lt;strong&gt;分类任务&lt;/strong&gt;中可使用&lt;strong&gt;投票法&lt;/strong&gt;，即选择$k$个样本中&lt;strong&gt;出现最多的类别标记&lt;/strong&gt;作为预测结果；在&lt;strong&gt;回归任务&lt;/strong&gt;中可使用平均法，即将$k$个样本的实值输出标记的平均值作为预测结果。该算法属于“&lt;strong&gt;惰性学习(lazy learning)&lt;/strong&gt;”方法之一，没有显式的学习过程。相应的，那些在训练阶段就对样本进行学习处理的方法，称为“&lt;strong&gt;急切学习(eager learning)&lt;/strong&gt;”。&lt;/p&gt;</description></item><item><title>机器学习：线性回归</title><link>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid><description>&lt;h1 id="线性回归模型"&gt;线性回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;线性回归(linear regression)&lt;/strong&gt; 是机器学习和统计学中最基础和广泛应用的模型，是一种对自变量和隐变量之间关系进行建模的回归分析。自变量数量为1时称为&lt;strong&gt;简单线性回归&lt;/strong&gt;，自变量数量大于1时称为&lt;strong&gt;多元线性回归&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习基础</title><link>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="人工智能"&gt;人工智能&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;智能(intelligence)&lt;/b&gt;是现代生活中很常见的一个词，比如智能手机、智能家居、智能驾驶等。在不同使用场合中，智能的含义也不太一样。比如“智能手机”中的“智能”一般是指由计算机控制并具有某种智能行为的意思。这里的“计算机控制”+“智能行为”隐含了对人工智能的简单定义。&lt;/p&gt;</description></item></channel></rss>