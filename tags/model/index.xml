<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model on Xiangdi Blog</title><link>https://xiangdiwu.github.io/tags/model/</link><description>Recent content in Model on Xiangdi Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 24 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://xiangdiwu.github.io/tags/model/index.xml" rel="self" type="application/rss+xml"/><item><title>论文学习《基于NGAT模型的股票长期趋势与风险建模》</title><link>https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/</guid><description>&lt;p&gt;
 &lt;img src="https://xiangdiwu.github.io/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img1.webp" alt="Image"&gt;


图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司。&lt;/p&gt;</description></item><item><title>自然语言处理：自然语言处理应用</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/</guid><description>&lt;p&gt;自然语言处理的应用（即NLP任务）分布非常广泛。本章节以任务的形式进行划分，主要介绍三类具有代表性的自然语言处理应用，包括文本分类（序列-编码-类别）、文本摘要（序列-编码-解码-序列）、以及机器阅读理解（序列-编码-同步序列）。每个部分首先介绍了应用的概念及挑战，然后介绍了一些具有代表性的论文工作。&lt;strong&gt;注意，从接触一个领域的具体任务开始，已经初步进入到了对该科研领域的探索之中。每个领域都有一些具体的任务，而确定一个任务往往就是着手开展一项研究工作的第一步。&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>自然语言处理：预训练模型</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="elmo"&gt;ELMo&lt;/h1&gt;
&lt;p&gt;预训练词向量(如word2vec和GloVe等)通常只能为一个单词产生一个特定的词向量，而忽略了该单词的&lt;strong&gt;上下文(context)&lt;/strong&gt; 关系，因而无法解决&lt;strong&gt;一词多义&lt;/strong&gt;或&lt;strong&gt;一义多词&lt;/strong&gt;的问题。&lt;strong&gt;ELMo(embeddings from language models)&lt;/strong&gt; 本质上是一个深度双向LSTM模型，用于为一个句子中的每个单词生成上下文相关的词向量。将这些上下文相关词向量编码了单词的深层次语义和句法信息，因此当ELMo应用到许多NLP任务中，这些任务的效果相对于使用静态的词向量往往能得到很大的提升。&lt;/p&gt;</description></item><item><title>自然语言处理：“序列到序列”与“注意力机制”</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/</link><pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/</guid><description>&lt;h1 id="序列到序列模型"&gt;序列到序列模型&lt;/h1&gt;
&lt;p&gt;许多单输出问题得以解决，比如命名实体识别、单词预测等。然而许多任务的输出是一个&lt;strong&gt;序列&lt;/strong&gt;，比如机器翻译、对话系统以及自动摘要等。这种问题应当使用seq2seq实现。&lt;/p&gt;</description></item><item><title>自然语言处理：语言模型</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="n元语法"&gt;N元语法&lt;/h1&gt;
&lt;p&gt;N元语言模型可以用来&lt;strong&gt;预测一个句子的下一个单词的概率&lt;/strong&gt;，或者&lt;strong&gt;计算一个句子的概率&lt;/strong&gt;。该模型常用于语音识别、拼写检查以及语法检查等领域。&lt;/p&gt;
&lt;p&gt;首先，&lt;strong&gt;一个句子“its water is so transparent that”后出现某一单词“the”的概率&lt;/strong&gt;为：&lt;/p&gt;</description></item><item><title>自然语言处理：词向量</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/</guid><description>&lt;h1 id="词向量概述"&gt;词向量概述&lt;/h1&gt;
&lt;p&gt;在自然语言处理领域，词的&lt;strong&gt;表示(representation)&lt;/strong&gt; 是一个核心问题。我们希望将单词通过某种嵌入的形式表示，以捕获词的&lt;strong&gt;含义(meaning)&lt;strong&gt;以及词和词之间的&lt;/strong&gt;关系(relationship)&lt;/strong&gt;。一个解决方法是，使用wordnet(a thesaurus containing lists of &lt;strong&gt;synonym sets&lt;/strong&gt; and &lt;strong&gt;hypernyms&lt;/strong&gt;)，如下所示：&lt;/p&gt;</description></item><item><title>自然语言处理概述</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/</guid><description>&lt;h1 id="自然语言与编程语言"&gt;自然语言与编程语言&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;自然语言处理(natural language processing, NLP)&lt;/strong&gt; 是一门融合了计算机科学、人工智能以及语言学的交叉学科(interdisciplinary field)。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现最终目标：&lt;strong&gt;理解人类语言或人工智能&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：图神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="图神经网络概述"&gt;图神经网络概述&lt;/h1&gt;
&lt;p&gt;近年来，深度学习领域关于&lt;strong&gt;图神经网络(graph neural network, GNN)&lt;/strong&gt; 的研究热情日益高涨，图神经网络已经成为各大深度学习顶会的研究热点。GNN处理非结构化数据时的出色能力使其在网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题方面都取得了新的突破。&lt;/p&gt;</description></item><item><title>深度学习：注意力机制与外部记忆</title><link>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</guid><description>&lt;p&gt;根据通用近似定理，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。&lt;/p&gt;</description></item><item><title>深度学习：深度生成模型</title><link>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率生成模型"&gt;概率生成模型&lt;/h1&gt;
&lt;p&gt;概率生成模型，简称&lt;strong&gt;生成模型(generative model)&lt;/strong&gt;，是概率统计和机器学习中的一类重要模型，指一系列用于&lt;strong&gt;随机生成&lt;/strong&gt;可观测数据的模型。假设在一个连续的或离散的高维空间$\mathcal X$中，存在一个随机向量$\boldsymbol X$服从一个未知的数据分布$p_r(\boldsymbol x),\boldsymbol x \in \mathcal X$。生成模型是根据一些可观测的样本$\boldsymbol x^{(1)},\boldsymbol x^{(2)},\cdots,\boldsymbol x^{(N)}$来学习一个参数化的模型$p_\theta(\boldsymbol x)$来近似未知分布$p_r(\boldsymbol x)$，并可以用这个模型来生成一些样本，使得&lt;strong&gt;生成样本和真实样本尽可能地相似&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：自编码器</title><link>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</guid><description>&lt;h1 id="自编码器"&gt;自编码器&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;自编码器(auto-encoder, AE)&lt;/strong&gt; 是通过&lt;strong&gt;无监督&lt;/strong&gt;的方式来学习&lt;strong&gt;一组数据的有效编码(或表示)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设有一组$d$维的样本$\boldsymbol{x}^{(n)} \in \mathbb{R}^{d}, 1 \leqslant n \leqslant N$，自编码器将这组数据映射到$p$维的&lt;strong&gt;特征空间&lt;/strong&gt;得到每个样本的编码$\boldsymbol{z}^{(n)} \in \mathbb{R}^{p}, 1 \leqslant n \leqslant N$，并且希望&lt;strong&gt;这组编码可以重构出原来的样本&lt;/strong&gt;。自编码器的结构可分为两部分：&lt;/p&gt;</description></item><item><title>深度学习：神经网络的优化</title><link>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</guid><description>&lt;h1 id="网络优化的难点"&gt;网络优化的难点&lt;/h1&gt;
&lt;h2 id="网络结构多样性"&gt;网络结构多样性&lt;/h2&gt;
&lt;p&gt;神经网络的种类非常多，比如卷积网络、循环网络等，其结构也非常不同。有些比较深，有些比较宽。不同参数在网络中的作用也有很大的差异，比如连接权重和偏置的不同，以及循环网络中循环连接上的权重和其它权重的不同。由于网络结构的多样性，我们很难找到一种通用的优化方法。&lt;strong&gt;不同的优化方法在不同网络结构上的差异也都比较大&lt;/strong&gt;。此外，网络的超参数一般也比较多，这也给优化带来很大的挑战。&lt;/p&gt;</description></item><item><title>深度学习：循环神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="序列数据和语言模型"&gt;序列数据和语言模型&lt;/h1&gt;
&lt;p&gt;全连接神经网络和卷积神经网络只能单独处理一个个的输入，&lt;strong&gt;前一个输入和后一个输入是完全没有关系的&lt;/strong&gt;。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要&lt;strong&gt;分析这些帧连接起来的整个序列&lt;/strong&gt;。这时，就需要用到深度学习领域中另一类非常重要神经网络：&lt;strong&gt;循环神经网络(recurrent neural network)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：卷积神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;卷积神经网络(convolutional neural network, CNN)&lt;/strong&gt; 是一种具有局部连接、权重共享等特性的前馈神经网络。&lt;/p&gt;
&lt;p&gt;卷积神经网络最早是主要用来处理图像信息。在用全连接前馈网络来处理图像时，会存在参数太多、局部特征不变形等缺陷。卷积神经网络利用&lt;strong&gt;局部连接、权重共享以及汇聚&lt;/strong&gt;三大结构上的特性，使得数据具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。&lt;/p&gt;</description></item><item><title>深度学习：神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;人工神经网络(artificial neural network, ANN)&lt;/strong&gt; 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象，构建&lt;strong&gt;人工神经元&lt;/strong&gt;，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为&lt;strong&gt;神经网络(neural network, NN)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习概述</title><link>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</guid><description>&lt;h1 id="表示学习"&gt;表示学习&lt;/h1&gt;
&lt;p&gt;为了提高机器学习系统的准确率，需要将输入信息转换为有效的特征，或者更一般称为&lt;strong&gt;表示(representation)&lt;/strong&gt;。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就是可以叫做&lt;strong&gt;表示学习(representation learning)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习：概率图模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率图模型概述"&gt;概率图模型概述&lt;/h1&gt;
&lt;div align="center"&gt;
&lt;img src="https://xiangdiwu.github.io/Kimages/2/image-20200818114353404.png" style="zoom:40%;" /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;概率图模型(probabilistic graphical model, PGM)&lt;/strong&gt;，简称图模型(graphical model, GM)，是指一种&lt;strong&gt;用图结构来描述多元随机变量之间条件独立关系的概率模型&lt;/strong&gt;，从而给研究&lt;strong&gt;高维空间中的概率模型&lt;/strong&gt;带来了很大的便捷性。&lt;/p&gt;</description></item><item><title>机器学习：话题模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="单词向量空间与话题向量空间"&gt;单词向量空间与话题向量空间&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;潜在语义分析(latent semantic analysis, LSA)&lt;/strong&gt; 是一种无监督学习方法，主要用于文本的话题分析，其特点是通过&lt;strong&gt;矩阵分解&lt;/strong&gt;发现&lt;strong&gt;文本与单词之间基于话题的语义关系&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;文本信息处理中，传统的方法以&lt;strong&gt;单词向量&lt;/strong&gt;表示文本的语义内容，以&lt;strong&gt;单词向量空间的度量&lt;/strong&gt;表示&lt;strong&gt;文本之间的语义相似度&lt;/strong&gt;。潜在语义分析旨在解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现&lt;strong&gt;潜在的话题&lt;/strong&gt;，以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度。这也是&lt;strong&gt;话题分析(topic modeling)的基本想法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习：特征选择</title><link>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid><description>&lt;h1 id="子集搜索与评价"&gt;子集搜索与评价&lt;/h1&gt;
&lt;p&gt;给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。对当前学习任务有用的属性称为&lt;strong&gt;相关特征(relevant feature)&lt;/strong&gt;，无用的属性称为&lt;strong&gt;无关特征(irrelevant feature)&lt;/strong&gt;。&lt;strong&gt;特征选择(feature selection)&lt;/strong&gt; 是一个重要的数据预处理过程，指从给定的特征集合中选择出相关特征的子集的过程。特征选择能够减少维数灾难问题，同时降低学习任务的难度。&lt;/p&gt;</description></item><item><title>机器学习：聚类</title><link>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</guid><description>&lt;h1 id="聚类的基本概念"&gt;聚类的基本概念&lt;/h1&gt;
&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt; 中，训练样本的标记信息是未知的，目标是通过对无标记样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。聚类(clustering)任务是一种常见的无监督学习方法。&lt;/p&gt;</description></item><item><title>机器学习：降维</title><link>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</guid><description>&lt;h1 id="主成分分析"&gt;主成分分析&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;主成分分析(principal component analysis, PCA)&lt;/strong&gt; 是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如下图所示的二维数据，如果将这些数据投影到一维空间，选择数据方差最大的方向进行投影(蓝轴)，才能最大化数据的差异性，保留更多的原始数据信息。&lt;/p&gt;</description></item><item><title>机器学习：EM算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="em算法的引入"&gt;EM算法的引入&lt;/h1&gt;
&lt;p&gt;概率模型有时既含观测变量，又含隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。&lt;strong&gt;EM算法&lt;/strong&gt;就是&lt;strong&gt;含有隐变量的概率模型参数的极大似然估计法&lt;/strong&gt;，或&lt;strong&gt;极大后验概率估计法&lt;/strong&gt;。我们仅讨论极大似然估计，极大后验概率估计与其类似。&lt;/p&gt;</description></item><item><title>机器学习：集成学习</title><link>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="个体与集成"&gt;个体与集成&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;集成学习(ensemble learning)&lt;/strong&gt; 通过构建并结合多个学习期来完成学习任务，有时也被称为&lt;strong&gt;多分类器系统(multi-classifier system)&lt;/strong&gt;、&lt;strong&gt;基于委员会的学习(committee-based learning)&lt;/strong&gt; 等。&lt;/p&gt;</description></item><item><title>机器学习：决策树</title><link>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</guid><description>&lt;h1 id="决策树模型"&gt;决策树模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;决策树(decision tree)&lt;/strong&gt; 是一种基本的&lt;strong&gt;分类&lt;/strong&gt;与&lt;strong&gt;回归&lt;/strong&gt;算法。决策树呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，它可以认为是&lt;strong&gt;if-then规则&lt;/strong&gt;的集合，也可以认为是定义在特征空间与类空间上的&lt;strong&gt;条件概率分布&lt;/strong&gt;。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用训练好的决策模型进行分类。&lt;/p&gt;</description></item><item><title>机器学习：支持向量机</title><link>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid><description>&lt;h1 id="线性支持向量机与硬间隔最大化"&gt;线性支持向量机与硬间隔最大化&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;支持向量机(support vector machine, SVM)&lt;/strong&gt; 是一个经典的机器学习二分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强的优势。&lt;/p&gt;</description></item><item><title>机器学习：逻辑回归与最大熵模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="逻辑回归模型"&gt;逻辑回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;逻辑回归(logistic regression, LR)&lt;/strong&gt; 模型是一种处理&lt;strong&gt;二分类&lt;/strong&gt;问题的线性模型。逻辑回归模型由logistic分布(logistic distribution)导出。设$X$是连续随机变量，$X$服从logistic分布是指$X$具有下列&lt;strong&gt;分布函数&lt;/strong&gt;和&lt;strong&gt;密度函数&lt;/strong&gt;：
&lt;/p&gt;</description></item><item><title>机器学习：感知机</title><link>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>&lt;h1 id="感知机算法"&gt;感知机算法&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;感知机(perceptron)&lt;/strong&gt; 由Frank Rosenblatt于1957年提出，是一种广泛使用的线性分类器。感知器可谓是最简单的人工神经网络，只有一个神经元，是对生物神经元的简单数学模拟，有与生物神经元相对应的部件，如&lt;strong&gt;权重(突触)、偏置(阈值)及激活函数(细胞体)&lt;/strong&gt;，输出为+1或-1。&lt;/p&gt;</description></item><item><title>机器学习：贝叶斯分类器</title><link>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid><description>&lt;h1 id="朴素贝叶斯的学习与分类"&gt;朴素贝叶斯的学习与分类&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯(naive Bayse)&lt;/strong&gt; 算法是基于&lt;strong&gt;贝叶斯定理&lt;/strong&gt;与&lt;strong&gt;特征条件独立假设&lt;/strong&gt;的分类方法。设输入空间$\mathcal X \subseteq \mathbb R^n$为$n$维向量的集合，输出空间为类标记集合$\mathcal Y={c_1,c_2,\cdots,c_K}$。输入为特征向量$x \in \mathcal X$，输出为类标记$y \in \mathcal Y$。$P(X,Y)$是输入空间和输出空间上的随机变量$X$和$Y$的联合概率分布，训练数据集(含$N$个数据)由$P(X,Y)$独立同分布产生。朴素贝叶斯在数据集上学习&lt;strong&gt;联合概率分布&lt;/strong&gt;$P(X,Y)$。具体地，先学习以下先验概率分布及条件概率分布：&lt;/p&gt;</description></item><item><title>机器学习：K近邻算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="近邻算法原理"&gt;$\boldsymbol k$近邻算法原理&lt;/h1&gt;
&lt;p&gt;$k$近邻($k$-nearest neighbor, $k$NN)算法是一种常用的监督学习方法。其基本思想为：给定一组数据，基于某种距离度量找出训练集中与其最靠近的$k$个训练样本，然后基于这$k$个邻居的信息来进行预测。通常，在&lt;strong&gt;分类任务&lt;/strong&gt;中可使用&lt;strong&gt;投票法&lt;/strong&gt;，即选择$k$个样本中&lt;strong&gt;出现最多的类别标记&lt;/strong&gt;作为预测结果；在&lt;strong&gt;回归任务&lt;/strong&gt;中可使用平均法，即将$k$个样本的实值输出标记的平均值作为预测结果。该算法属于“&lt;strong&gt;惰性学习(lazy learning)&lt;/strong&gt;”方法之一，没有显式的学习过程。相应的，那些在训练阶段就对样本进行学习处理的方法，称为“&lt;strong&gt;急切学习(eager learning)&lt;/strong&gt;”。&lt;/p&gt;</description></item><item><title>机器学习：线性回归</title><link>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid><description>&lt;h1 id="线性回归模型"&gt;线性回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;线性回归(linear regression)&lt;/strong&gt; 是机器学习和统计学中最基础和广泛应用的模型，是一种对自变量和隐变量之间关系进行建模的回归分析。自变量数量为1时称为&lt;strong&gt;简单线性回归&lt;/strong&gt;，自变量数量大于1时称为&lt;strong&gt;多元线性回归&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习基础</title><link>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="人工智能"&gt;人工智能&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;智能(intelligence)&lt;/b&gt;是现代生活中很常见的一个词，比如智能手机、智能家居、智能驾驶等。在不同使用场合中，智能的含义也不太一样。比如“智能手机”中的“智能”一般是指由计算机控制并具有某种智能行为的意思。这里的“计算机控制”+“智能行为”隐含了对人工智能的简单定义。&lt;/p&gt;</description></item></channel></rss>