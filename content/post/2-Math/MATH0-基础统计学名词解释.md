---
layout:      post
title:       "基础统计学名词解释"
subtitle:    "Fundamental Statistics"
description: "本文主要介绍基础统计学相关概念。"
author:      "XiangdiWu"
date:        2020-09-22
image:       "https://images.pexels.com/photos/4778611/pexels-photo-4778611.jpeg"

categories:  ["Quant"]
tags:
    - Quant
    - Math

# draft:       true
params:
    mermaid:	true
    math:		true
---


## 统计、数据和计算机


1.  统计学（Statistics）：收集、整理、分析并解释数据以支持决策的科学。
1.  描述统计（Descriptive Statistics）：用图表和概括量描述数据基本特征的方法。
1.  推断统计（Inferential Statistics）：利用样本数据对总体进行估计与假设检验的技术。
1.  变量（Variable）：可以取不同数值或类别的观测特征。
1.  无序类别变量（Nominal Categorical Variable）：仅有名称而无内在顺序的分类特征。
1.  有序类别变量（Ordinal Categorical Variable）：类别间存在自然顺序但间距未知的特征，比如：很好、好、一般、差、很差。
1.  数值变量（Numerical Variable）：又称为定量变量，以数值表示并可进行算术运算的特征。
1.  无序类别数据（Nominal Data）：属于无序类别的观察结果集合。
1.  有序类别数据（Ordinal Data）：属于有序类别的观察结果集合。
1.  数值数据（Numerical Data）：由数值构成的观察结果集合。
1.  总体（Population）：研究对象的全部个体（数据）的集合。
1.  样本（Sample）：从总体中抽取用于研究的子集。
1.  样本量（Sample Size）：样本中个体的数量。
1.  简单随机抽样（Simple Random Sampling）：每个个体等概率被抽中的抽样方法。
1.  分层抽样（Stratified Sampling）：将总体分层后在各层内随机抽样的方法。
1.  系统抽样（Systematic Sampling）：按固定间隔从有序总体中抽取个体的抽样方法。
1.  整群抽样（Cluster Sampling）：以自然群体为单位整群抽取的抽样方法。

## 用图表展示数据

18. 频数分布（Frequency Distribution）：展示各取值或区间出现次数的表格或图形。
18. 频数（Frequency）：某取值或区间在数据集中出现的次数。
18. 比例（Proportion）：部分在全体中所占的相对份额。
18. 比率（Ratio）：两个数值间的对比关系。
18. 条形图（Bar Chart）：用等宽条形高度表示类别频数的图形。
18. 帕累托图（Pareto Chart）：按频数降序排列并叠加累积比例的条形图。
18. 饼图（Pie Chart）：用扇形面积表示类别比例的圆形图。
18. 环形图（Donut Chart）：中间留空的饼图，用于展示多系列比例。
18. 直方图（Histogram）：用相邻矩形面积表示数值数据区间频数的图形。
18. 茎叶图（Stem-and-Leaf Plot）：保留原始数据形态的同时展示分布形状的图形。
18. 箱线图（Boxplot）：用五数概括展示数据分布与异常值的图形。
18. 离群值（Outlier）：在数据集中显著偏离其余观测、与整体模式不符的极端值。
18. 垂线图（Drop-Line Plot）：在分类轴上用垂线表示数值大小的图形。
18. 误差图（Error Bar Plot）：展示均值及变异或不确定性的图形。
18. 散点图（Scatter Plot）：用点的位置表示两个数值变量观测值的图形。
18. 重叠散点图（Overlaid Scatter Plot）：在同一坐标系中绘制多组散点以比较关系。
18. 雷达图（Radar Chart）：用多轴多边形展示多变量综合表现的图形。
18. 轮廓图（Contour Plot）：用等高线表示三维数据在二维平面上分布的图形。

## 用统计量描述数据

36. 简单平均值（Mean）：所有观测值之和除以观测个数的结果。
36. 中位数（Median）：排序后处于中间位置的观测值。
36. 四分位数（Quartile）：将排序数据四等分的三个分割点。
36. 众数（Mode）：数据集中出现次数最多的观测值。
36. 极差（Range）：最大值与最小值之差。
36. 四分位差（Interquartile Range）：第三四分位数与第一四分位数之差。
36. 方差（Variance）：观测值与均值偏差平方的平均，衡量数据离散度。
36. 标准差（Standard Deviation）：方差的正平方根，度量数据离散程度。
36. 标准分数（Z-score）：观测值与均值之差除以标准差后的无量纲结果。
36. 离散系数（Coefficient of Variation）：标准差与均值之比的无量纲相对变异指标。
> 当需要比较不同单位或量级数据集的相对离散程度时使用离散系数。
>
> 因标准差受量纲和均值大小的影响，无法直接比较单位或量级不同数据集的离散程度。

46. 偏态系数（Skewness）：刻画数据分布不对称程度和方向的指标。

> 偏态系数=0，数据的分布对称
>
> 偏态系数>0，数据为右偏分布
>
> 偏态系数<0，数据为左偏分布
>
> 偏态系数越接近0，偏斜程度越低

47. 峰态系数（Kurtosis）：刻画数据分布尖峭或平坦程度的指标。

> 标准正态分布的峰态系数为0
>
> 峰态系数>0，尖峰分布，数据的分布相对集中
>
> 峰态系数<0，扁平分布，数据的分布相对分散

## 概率分布

48. 概率分布（Probability Distribution）：随机变量所有可能取值及其对应概率的完整描述。
48. 概率（Probability）：事件发生可能性的度量，介于0与1之间。
48. 随机变量（Random Variable）：对随机试验结果赋予数值的变量。
48. 离散型随机变量（Discrete Random Variable）：取值可数的随机变量。
48. 连续型随机变量（Continuous Random Variable）：取值充满某区间的随机变量。
48. 期望值（Expected Value）：随机变量在大量重复试验中取值的长期平均。
48. 随机变量的方差（Variance of Random Variable）：随机变量与其期望值偏差平方的期望。
48. 参数（Parameter）：描述总体特征的未知常数。
48. 统计量（Statistic）：由样本数据计算得到的、用于估计参数的数值。
48. 抽样分布（Sampling Distribution）：统计量在所有可能样本中的概率分布。
48. 二项分布（Binomial Distribution）：n次独立伯努利试验中成功次数的离散概率分布。
48. 泊松分布（Poisson Distribution）：描述单位时空内稀有事件发生次数的离散分布。
48. 超几何分布（Hypergeometric Distribution）：无放回抽样中成功次数的离散分布。
48. 正态分布（Normal Distribution）：呈钟形、由均值和方差决定的连续对称分布。
48. 均匀分布（Uniform Distribution）：在给定区间内所有取值等可能的连续或离散分布。
48. 指数分布（Exponential Distribution）：描述泊松过程中事件间隔时间的连续分布。
48. t分布（t-Distribution）：小样本下估计正态总体均值的连续对称分布，尾部较厚。
48. 卡方分布（Chi-square Distribution）：独立标准正态变量平方和的连续偏斜分布。
48. F分布（F-Distribution）：两个独立卡方变量各除以其自由度之比的连续分布。
48. 样本均值分布（Sampling Distribution of the Mean）：样本均值的抽样分布。
48. 样本比例分布（Sampling Distribution of the Proportion）：样本比例的抽样分布。
48. 样本方差分布（Sampling Distribution of the Variance）：样本方差的抽样分布。
48. 中心极限定理（Central Limit Theorem）：无论总体分布如何，只要样本量足够大，样本均值的抽样分布趋近正态分布，从而可用正态方法进行推断。
48. 标准误差（Standard Error）：样本统计量分布的标准差，衡量估计精度。
48. 估计标准误差（Standard Error of Estimate）：总体标准差未知时，用样本标准差代替计算得到的标准误差。

## 参数估计

73. 参数估计（Parameter Estimation）：利用样本统计量推断未知总体参数的过程。

> 参数估计回答的核心统计学问题是：在总体分布形式已知、但其参数未知的前提下，如何利用样本数据构造一个统计量（点估计）或一个随机区间（区间估计），使得该统计量或区间能以既定准则（如无偏性、最小方差、置信水平）尽可能接近或覆盖总体参数的真值，并给出这种接近程度或覆盖概率的可量化刻画。

74. 估计量（Estimator）：用于估计参数的样本函数或公式。
74. 估计值（Estimate）：根据具体样本计算得到的估计量数值结果。
74. 点估计（Point Estimate）：用单一数值作为未知参数的估计。
74. 区间估计（Interval Estimate）：给出参数可能取值范围的估计方式，通常由样本统计量加减估计误差得到。
74. 置信区间（Confidence Interval）：以一定概率覆盖真实参数的区间估计。
74. 置信水平（Confidence Level）：置信区间覆盖真实参数的可靠概率。

> 例子：从某校随机抽取100名学生测得平均身高 ($\bar x = 17$) cm，并给出95%置信区间为 (168 cm, 172 cm)。
>
> 解释：95%置信水平意味着，如果用同样方法重复抽样并构造100个这样的区间，大约95个区间会把全校学生的真实平均身高包含在内。

80. 无偏性（Unbiasedness）：估计量的期望等于被估参数的性质。
80. 有效性（Efficiency）：在具有无偏性的估计量中方差最小的性质。
80. 一致性（Consistency）：随着样本量增大估计量依概率收敛于真实参数的性质。

> 例子：估计某城市成年男性平均身高 θ，用两种方法每次随机抽 100 人。
>
> 无偏性：若方法 A 的样本均值期望恰好等于 θ，则 A 具有无偏性。
>
> 有效性：在同样无偏的方法中，方法 B 的方差更小，则 B 比 A 更有效。
>
> 一致性：随着样本量 n 从 100 增至 10000，方法 A 的样本均值越来越逼近 θ，则 A 具有一致性。

83. 独立样本（Independent Samples）：两组或多组观测值互不影响的样本。
83. 配对样本（Paired Samples）：同一受试单元两次观测值成对出现的样本。

> 例子：研究新饮料对心率的影响。
>
> 独立样本：将 20 人随机分成两组，各 10 人，一组喝饮料、一组喝水，比较两组心率均值——两组人员互不相关。
>
> 配对样本：让同一批 10 人先后两天分别喝饮料和水，记录每人两次心率，然后比较同一人在两种处理下的差异——数据天然成对。

## 假设检验

85. 假设（Hypothesis）：关于总体参数或分布形式的待检验命题。
85. 假设检验（Hypothesis Testing）：利用样本数据判断假设是否成立的统计方法。

> 在总体分布未知或参数未知的背景下，先设定一对相互对立的统计假设（零假设与备择假设），再构造一个基于样本数据的检验统计量并确定其抽样分布；在控制第一类错误概率（显著性水平 α）的前提下，计算该统计量在零假设成立时的极端概率（p 值）或临界区域，从而依据事先约定的决策规则，判断样本证据是否足以拒绝零假设，并量化做出此决策可能犯的第一类与第二类错误的概率，最终对总体参数或分布形态是否与零假设存在实质性偏离给出具有可控风险的统计结论。

87. 原假设（Null Hypothesis）：也称零假设，通常表示无差异或无效果的待检验基准假设（研究者想要收集证据予以推翻的假设）。
87. 备择假设（Alternative Hypothesis）：与原假设对立、研究者希望证实的假设。
87. 双侧检验或双尾检验（Two-tailed Test）：检验统计量落在任一尾部均拒绝原假设的检验。
87. 单侧检验或单尾检验（One-tailed Test）：仅在检验统计量落在某一特定尾部才拒绝原假设的检验。
87. 左侧检验（Left-tailed Test）：拒绝域位于分布左侧的单尾检验。
87. 右侧检验（Right-tailed Test）：拒绝域位于分布右侧的单尾检验。
87. 第一类错误（Type I Error）：原假设为真却被错误拒绝。
87. 第二类错误（Type II Error）：原假设为假却被错误接受。
87. 显著性水平（Significance Level）：事先指定允许犯第一类错误的最大概率阈值。

> 例子：把显著性水平设为 0.05，相当于事先约定“只有新药真正有效的证据强到在 100 次试验中最多出现 5 次假阳性时，我们才认为它有效”。

> 显著性水平$\alpha$越小，犯第一类错误的可能性越小，但犯第二类错误的可能性随之增大。

96. 临界值（Critical Value）：划分拒绝域与接受域的边界值。
96. 拒绝域（Rejection Region）：导致拒绝原假设的检验统计量取值范围。
96. 检验统计量（Test Statistic）：用于衡量样本与原假设偏离程度的统计量。

> 常见检验统计量类型：Z检验、卡方检验、t检验、F检验

99. P值（P-value）：在原假设成立时，获得当前或更极端样本结果的概率，也称为观察到的显著性水平。

> 依据什么作出决策？
>
> -   统计量决策：实际中使用的检验统计量都是标准化检验统计量，它反映了点估计量与假设的总体参数相比相差多少个标准差的距离。根据事先给定的显著性水平$\alpha$，可以在统计量的分布上找到相应的临界值，由显著性水平和相应的临界值围成的一个区域称为拒绝域。如果统计量的值落在拒绝域内就拒绝原假设，否则就不拒绝原假设。
> -   P值决策：如果$P<\alpha$，拒绝原假设$H_0$；如果$P>\alpha$，不拒绝原假设$H_0$。P值越小，拒绝原假设的理由就越充分。一般来说，$P<0.1$代表有一些证据不利于原假设；$P<0.05$代表有适度证据不利于原假设；$P<0.01$代表有很强证据不利于原假设。统计上通常要求P值不应大于0.1。
> -   P值的意义：算出了犯第一类错误的实际概率。提供了与传统统计量决策相比更多的信息。

> 决策结果表述的注意事项：
>
> -   假设检验不能证明原假设正确。
> -   统计上显著不等于有实际意义。

100.    正态概率图（Normal Probability Plot）：用散点判断样本是否近似正态分布的图形。
100.    Q-Q图（Quantile-Quantile Plot）：比较样本分位数与理论分布分位数的图形。
100.    P-P图（Probability-Probability Plot）：比较样本累积概率与理论累积概率的图形。

> Q-Q图和P-P图：直线表示理论正态分布线，各观测点越靠近直线，且呈随机分布，表明数据越接近正态分布。使用时，样本量应尽可能大。

103.    Shapiro-Wilk正态性检验（Shapiro-Wilk Test）：小样本下用样本顺序统计量检验数据是否来自正态分布的方法。
103.    K-S正态性检验（Kolmogorov-Smirnov Test）：通过比较样本与理论正态分布的最大垂直距离检验正态性的非参数方法。

## 类别变量的推断

> 在已知样本列联表频数的前提下，利用多项分布或超几何分布构建卡方、Fisher 等检验统计量，并量化其精确 p 值，从而在控制第一类错误率 α 的条件下，检验总体中类别比例（或关联结构）是否显著偏离某个先验比例向量或独立性假设，并给出偏离方向与可信区间的可解释度量。

105.    χ²拟合优度检验（Chi-square Goodness-of-fit Test）：用χ²统计量检验样本分布与理论分布是否相符的方法。

> 例子：掷骰子 60 次，记录各面出现次数后计算 χ² = 2.4，查表得 p = 0.79 > 0.05，说明观测频数与“公平骰子”理论频数无显著差异，骰子公平的原假设不能被拒绝。

106.    观察频数（Observed Frequency）：样本中实际落在各类别的计数。
106.    期望频数（Expected Frequency）：在假设分布下理论上各类别应有的计数。
106.    一致性检验（Test of Homogeneity）：检验多个总体在某一变量上的分布是否相同的χ²检验。

> 例子：把 200 名学生随机分到 A、B 两考场，用 χ² 一致性检验得 p = 0.03 < 0.05，说明两考场成绩分布差异显著，拒绝“分布相同”的原假设，提示考场环境可能对成绩产生了影响。

109.    χ²独立性检验（Chi-square Test of Independence）：用χ²统计量检验两类别变量是否独立的检验。

> 例子：调查 300 名大学生“是否熬夜”与“是否挂科”，χ² 独立性检验得 p = 0.01 < 0.05，说明熬夜与挂科并非独立，二者存在显著关联。

110.    列联表（Contingency Table）：将两类别变量交叉分类后列出各类别频数的二维表。
110.    φ系数（Phi Coefficient）：度量2×2列联表关联强度的χ²派生系数。

> 例子：在 2×2 列联表中，φ = 0.40 表示两类别变量间存在中等强度的正向关联，即变量 A 出现时变量 B 同时出现的可能性比随机情况下高约 40% 的相对提升。

112.    Cramer's V系数（Cramer's V）：基于χ²、可适用于任意r×c列联表的关联强度系数。

> 例子：在 3×4 列联表中得到 Cramer's V = 0.25，说明两个类别变量之间存在中等偏弱的关联强度（0.25 ≈ 1/4），即知道其中一个变量的取值，可将预测另一个变量的误差减少约 6%（0.25²）。

113.    列联系数（Contingency Coefficient）：由χ²转换而来、衡量列联表关联程度的指标。

> 例子：在 3×4 列联表中算得列联系数 C = 0.35，表示两变量存在中等关联，其值介于 0 与最大可能值 0.87 之间，说明两变量并非独立，但关联强度弱于完全相关。

## 方差分析

114.    方差分析（Analysis of Variance, ANOVA）：通过比较组间与组内方差检验多总体均值差异的方法（分析各类别自变量对数值因变量影响的一种统计方法）。

> 在正态误差、方差齐性、独立观测的线性模型假设下，通过分解总平方和为组间与组内平方和并构建 F 统计量，精确检验多个总体均值是否存在显著差异，并估计差异来源、方向及效应量，同时控制整体第一类错误率 α，为多重比较与后续实验设计提供可重复、可解释的统计依据。

115.    自变量效应（Independent Variable Effect）：自变量变化引起的因变量系统性变动（因变量的误差里有多少是由于自变量造成的）。

> 通过对数据误差的分析来检验这种效应是否显著。
>
> 例子：在检验“肥料施用量对产量是否显著”时，若 F 检验显示处理平方和远大于误差平方和（p＜0.05），就表明肥料这一自变量确实对产量产生了系统性影响。

116.    因子（Factor）：方差分析中被控制的分类自变量。
116.    处理（Treatment）：因子所取的具体水平或类别。
116.    单因素方差分析（One-way ANOVA）：仅含一个因子、检验其不同处理下均值差异的方差分析。
116.    交互效应（Interaction Effect）：两因子不同水平组合对因变量产生的协同或拮抗效应。

> 例子：研究肥料与灌溉对作物产量的双因素实验发现，肥料的增产效果只在充分灌溉时才显著，而在缺水时几乎无效，这种“肥料效果随灌溉水平变化”的现象即为显著的交互效应。

120.    主效应（Main Effect）：单个因子各水平对因变量的独立平均效应。
120.    双因素方差分析（Two-way ANOVA）：同时检验两个因子及其交互效应的方差分析。
120.    无重复双因素分析（Two-way ANOVA without Replication）：每个处理组合仅一次观测的双因素方差分析。
120.    可重复双因素分析（Two-way ANOVA with Replication）：每个处理组合有多次观测、可评估交互效应的双因素方差分析。
120.    总误差（Total Error）：因变量观测值与其总均值之差的平方和。
120.    处理误差（Treatment Error）：处理效应导致的观测值偏离总均值的部分。
120.    处理效应（Treatment Effect）：处理水平差异引起的均值差异。
120.    组间误差（Between-group Error）：不同处理组均值差异带来的变异。
120.    随机误差（Random Error）：不可控因素导致的观测随机波动。
120.    组内误差（Within-group Error）：同一处理组内观测值与其组均值之差的平方和。

> 总误差：全班 50 名学生的数学成绩与年级平均分之差，反映所有差异来源的总和。
>
> 处理误差：仅由“使用新教材”这一处理引起的成绩偏离年级均值的部分。
>
> 处理效应：新教材使平均分提高 5 分，这 5 分就是处理效应。
>
> 组间误差：新教材班平均分与传统教材班平均分之差，体现两处理组间的差异。
>
> 随机误差：同一新教材班内，学生因临时状态差异导致的个人成绩围绕班级均值的波动。
>
> 组内误差：新教材班内每个学生成绩与该班平均分之差，仅反映班内随机波动。

130.    平方和（Sum of Squares, SS）：观测值与参考值离差平方的总和。
130.    总平方和（Total Sum of Squares, SST）：所有观测值与总均值离差平方的总和。
130.    处理平方和（Treatment Sum of Squares, SSTr）：各处理均值与总均值离差平方加权和。
130.    组间平方和（Between-group Sum of Squares, SSA）：同处理平方和，反映组间差异。
130.    误差平方和（Error Sum of Squares, SSE）：组内离差平方和，衡量随机误差。
130.    组内平方和（Within-group Sum of Squares）：同误差平方和。

![](/img/statistics.png)

136.    均方（Mean Square, MS）：平方和除以其自由度后的平均变异。
136.    方差（Variance）：观测值与其均值离差平方的期望，衡量数据离散程度。
136.    多重比较（Multiple Comparisons）：在ANOVA显著后对各处理均值进行两两差异检验的方法。

> 例子：ANOVA 显示 A、B、C 三种肥料对产量整体差异显著后，用 Tukey 多重比较发现仅 A 与 C 平均产量差异显著（p＜0.05），而 A 与 B、B 与 C 差异不显著，于是只需在 A、C 之间择优施肥。

139.    最小显著差异（Least Significant Difference, LSD）：用于多重比较的临界差异阈值。

> 例子：在三组肥料实验中，LSD = 2.1 公斤，表示任何两组平均产量之差若超过 2.1 公斤就被判为显著。

140.    真实显著差异（Honestly Significant Difference, HSD）：Tukey法给出的多重比较临界差异，控制整体第一类错误率。

> 例子：仍用 Tukey 法，HSD = 2.6 公斤，只要两组均值差大于 2.6 公斤即可在 5% 整体误差率下视为显著。

141.    学生化极差分布（Studentized Range Distribution）：用于多重比较检验极差与标准误之比的抽样分布。

> 例子：若 q(0.05,3,27)=3.51，则把任意两组均值差除以合并标准误得到的 q 值大于 3.51 时才认为差异显著。

142.    正态性（Normality）：数据服从或近似服从正态分布的性质。

> Shapiro-Wilk检验和 K-S检验等均可以做正态性检验，但这些检验对正态性的轻微偏离是敏感的，检验往往会导致拒绝原假设。而方差分析对正态性的要求相对宽松，当正态性略微不满足时，对分析结果的影响不是很大。

143.    方差齐性（Homogeneity of Variance）：各总体或各组方差相等的假设。

> 方差分析中，对方差齐性的要求相对比较宽松，当方差略有不齐时，对分析结果的影响不是很大。特别是当处理的样本量相同时，方差分析对不等方差是稳健的。

> 例子：用 Levene 检验比较两种肥料的产量方差，得 p = 0.18 > 0.05，说明两组方差无显著差异，满足方差齐性假设，可直接使用 t 检验比较均值。

144.    残差（Residual）：观测值与模型预测值之差。
144.    标准化残差（Standardized Residual）：残差除以其标准误后的无量纲值，用于诊断异常。
144.    独立性（Independence）：该假定要求每个样本数据是来自不同处理的独立样本。
144.    实验（Experiment）：在控制条件下主动施加处理以观察因果效应的研究过程。
144.    实验设计（Experimental Design）：为高效无偏地评估处理效应而预先规划实验单元、处理分配及测量方案的系统性方法。
144.    完全随机化设计（Completely Randomized Design）：实验单元随机分配到各处理的实验设计。
144.    处理（Treatment）：可控制的因素的各个水平。
144.    实验单元（Experimental Unit）：接受处理并被测量的最小独立实体。
144.    重复（Replication）：在每个实验条件下实验可重复进行。
144.    随机化区组设计（Randomized Block Design）：将相似单元组成区组后在区组内随机分配处理的实验设计。
144.    区组（Block）：在区组设计中，内部相对同质、用于控制混杂变量的实验单元组。
144.    析因设计（Factorial Design）：同时考察两个及以上因子及其所有可能水平组合的实验设计。

> 析因设计主要用于分析两个因素及其交互作用对实验结果的影响，采用的分析方法则是考虑交互效应的双因素方差分析。

## 一元线性回归

156.    线性回归（Linear Regression）：用线性方程描述自变量与因变量之间关系的回归方法。

> 在 Gauss-Markov 假设（线性、独立、同方差、正态误差）下，用最小二乘或极大似然估计未知回归系数，给出其无偏、有效、渐近正态的点估计与精确置信区间，进而通过 t/F 检验判断各预测变量对响应变量的条件均值是否存在显著解释力，量化效应大小与方向，并基于残差诊断与预测区间评估模型拟合优度及未来观测的不确定性。

157.    非线性回归（Nonlinear Regression）：用非线性方程刻画自变量与因变量关系的回归方法。

> 在响应变量与预测变量呈非线性关系、误差项满足独立同分布且近似正态的设定下，通过最小化残差平方和（或极大似然）迭代估计未知非线性参数，给出其渐近协方差矩阵、置信区间与参数显著性检验，进而刻画预测变量对响应变量条件均值的非线性效应大小与曲线形态，并提供带不确定度的预测区间及模型诊断工具以评估拟合优度与假设合理性。

158.    相关关系（Correlation）：两变量间存在统计关联但未必因果的联系。
158.    相关系数（Correlation Coefficient）：度量两变量线性相关强度与方向的指标。
158.    皮尔逊相关系数（Pearson r）：度量两变量线性相关强度与方向的标准化指标。

> 对于一个具体的 r 取值，根据相关经验可将相关程度分为以下几种情况：
>
> -   当$|r|\geq0.8$时，可视为高度相关；
> -   当$0.5\leq|r|<0.8$时，可视为中度相关；
> -   当$0.3\leq|r|$时，可视为低度相关；
> -   当$|r|<0.3$时，说明两个变量之间的相关程度极弱。

161.    回归分析（Regression Analysis）：建立数学模型以预测或解释因变量随自变量变化的方法。
161.    因变量（Dependent Variable）：被解释或预测的变量。
161.    自变量（Independent Variable）：用来解释或预测因变量的变量。
161.    回归模型（Regression Model）：描述因变量与自变量关系的数学表达式。
161.    估计的回归方程（Estimated Regression Equation）：基于样本数据得到的回归模型具体表达式。
161.    最小二乘法（Ordinary Least Squares, OLS）：使残差平方和最小以估计回归系数的方法。
161.    拟合优度（Goodness of Fit）：回归模型对观测数据解释程度的度量。
161.    决定系数（Coefficient of Determination, R²）：回归模型解释的因变量变异占比。

> 例子：用学习时间（小时）预测考试成绩（分），若回归模型的决定系数 R² = 0.81，则表示“学习时间”这一变量能解释考试成绩 81% 的波动，其余 19% 由其他因素或随机误差造成。

169.    估计标准误差（Standard Error of Estimate）：回归预测误差的平均标准差。

> 例子：用每日学习小时数预测考试成绩，若估计标准误差为 5 分，意味着模型预测的平均偏差约为 5 分，真实成绩通常落在预测值 ±5 分的范围内。

170.    F 检验（F-test）：在一元线性回归中一次性检验模型整体线性关系是否显著。

> 例子：研究“学习时间”能否预测“考试成绩”，若F检验的p值<0.05，就拒绝“学习时间对成绩无解释力”的原假设，说明回归模型整体显著有效。

171.    t 检验（t-test）：对单个回归系数是否显著不同于零进行的假设检验。

> 例子：在“学习时间预测考试成绩”的回归中，若对“学习时间”的回归系数做 t 检验得到 p<0.05，就说明学习时间每增加 1 小时，成绩平均提高的效应显著不为 0。

172.    平均值的置信区间（Confidence Interval for Mean Response）：给定自变量值下因变量均值的区间估计。
172.    个别值的预测区间（Prediction Interval for Individual Response）：给定自变量值下单个因变量值的区间估计。
172.    异方差性（Heteroscedasticity）：回归模型中残差的方差随自变量取值变化而不再保持常数的现象。
172.    残差（Residual）：观测值与回归模型预测值之差。
172.    标准化残差（Standardized Residual）：残差除以其标准差后的无量纲值，用于识别异常点。也称为皮尔逊残差（Pearson Residual）或半学生化残差（Semi-Studentized Residual）。

## 多元线性回归

177.    多元回归（Multiple Regression）：用一个以上自变量同时预测或解释因变量的回归方法。
177.    多元线性回归（Multiple Linear Regression）：自变量与因变量呈线性关系的多元回归模型。
177.    多元线性回归模型（Multiple Linear Regression Model）：描述因变量与多个自变量线性关系的数学表达式。
177.    多元线性回归方程（Multiple Linear Regression Equation）：总体层面的多元线性模型表达式。
177.    估计的多元线性回归方程（Estimated Multiple Linear Regression Equation）：基于样本数据得到的多元线性模型具体表达式。
177.    标准化回归系数（Standardized Regression Coefficient）：将变量标准化后得到的回归系数，用于比较各自变量相对重要性。

> 例子：研究学习时间（小时）与睡眠时长（小时）对考试成绩（分）的影响，若学习时间的标准化回归系数为 0.6、睡眠时长为 0.3，则说明学习时间每增加一个标准差，成绩平均提高 0.6 个标准差，其影响约为睡眠时长的两倍。

183.    标准化回归方程（Standardized Regression Equation）：所有变量经标准化后的回归模型。
183.    多重决定系数（Multiple Coefficient of Determination, R²）：多元模型解释的因变量变异比例。

> 例子：用学习时间和睡眠时长一起预测考试成绩，若多重决定系数 R² = 0.75，则表明这两个变量共同解释了成绩 75% 的波动，其余 25% 由其他因素或随机误差造成。

185.    调整的多重决定系数（Adjusted R²）：考虑自变量个数后修正的模型拟合优度指标。
185.    多重共线性（Multicollinearity）：多元回归中自变量间高度相关而影响估计稳定性的现象。

> 例子：用学习时间和完成作业数预测考试成绩，却发现“学习时间”与“作业数”高度相关（r≈0.9），导致回归系数符号反常、显著性降低，这就是多重共线性使结果难以解释的典型现象。

> 识别多重共线性的常用方法：
>
> -   计算方差扩大因子（VIF），VIF＞10（或＞5）视为强共线。
> -   查看容忍度（Tolerance），T＜0.1（或＜0.2）提示共线。
> -   检查自变量间的皮尔逊相关系数矩阵，|r|＞0.8（或＞0.9）表明高度相关。
> -   观察回归系数符号或大小与理论/单变量分析相反且不稳定。
> -   计算条件指数（Condition Index），CI＞30（或＞100）视为严重共线。

187.    容忍度（Tolerance）：1 减去某自变量对其余自变量回归的 R²，衡量多重共线性严重程度的指标，值越小共线性越强。
187.    方差扩大因子（Variance Inflation Factor, VIF）：容忍度的倒数，量化多重共线性导致回归系数方差膨胀的倍数，值越大共线性越严重。

> 除非确实有必要，在建立多元线性回归模型时，不要试图引入更多的自变量。选择自变量的原则是：将一个或一个以上的自变量引入回归模型时，应该使得残差平方和有显著的降低，否则就没有必要将这个自变量引入模型。

189.    向前选择（Forward Selection）：从无变量开始逐步加入显著自变量的变量筛选方法。

> 向前选择法的特点是：只要某个自变量被增加到模型中，这个变量就一定会保留在模型中。

190.    向后剔除（Backward Elimination）：从全模型开始逐步剔除不显著自变量的变量筛选方法。

> 向后剔除法的特点是：只要某个自变量被从模型中剔除，这个变量就不会再进入模型中。

191.    逐步回归（Stepwise Regression）：结合向前与向后策略动态增删自变量的变量选择方法。

> 逐步回归法的特点是：在前面步骤中增加的自变量在后面的步骤中有可能被剔除，而在前面步骤中被剔除的自变量在后面的步骤中也可能重新进入模型中。

192.    哑变量（Dummy Variable）：将类别变量数值化、取0或1以纳入回归模型的虚拟变量。

## 时间序列预测

193.    时间序列（Time Series）：按时间顺序记录的一组观测数据。
193.    趋势（Trend）：时间序列长期上升或下降的方向性变动。
193.    季节变动（Seasonal Variation）：因季节周期引起的规律性短期波动。
193.    循环波动（Cyclical Fluctuation）：超过一年的非季节性周期波动。
193.    不规则波动（Irregular Variation）：无法归因于趋势、季节或循环的随机扰动。
193.    乘法模型（Multiplicative Model）：将时间序列分解为趋势、季节、循环与不规则四成分相乘的模型。
193.    加法模型（Additive Model）：将时间序列分解为四成分相加的模型。

> 对于同一个时间序列，有几种预测方法可供选择时，以预测误差最小者为宜。

200.    平均误差（Mean Error）：预测误差的算术平均。
200.    平均绝对误差（Mean Absolute Error）：预测误差绝对值的平均。
200.    均方误差（Mean Squared Error）：预测误差平方的平均。
200.    平均百分比误差（Mean Percentage Error）：预测误差占实际值百分比的平均。
200.    平均绝对百分比误差（Mean Absolute Percentage Error）：预测误差绝对值占实际值百分比的平均。
200.    平稳序列（Stationary Series）：统计特性不随时间推移而改变的随机序列。
200.    简单平均（Simple Average）：用全部历史数据的均值作为未来预测。
200.    移动平均（Moving Average）：用最近若干期数据的均值进行平滑或预测。
200.    简单指数平滑（Simple Exponential Smoothing）：对历史观测加权衰减的平滑预测方法。

> 使用简单指数平滑法的关键是确定一个合适的平滑系数$\alpha$。
>
> -   $\alpha=0$时，预测值仅仅是重复上一期的预测结果。
> -   $\alpha=1$时，预测就是上一期实际值。
> -   $\alpha$越接近1，模型对时间序列变化的反应就越及时，因为它对当前的实际值赋予了比预测值更大的权数。一般而言，当时间序列有较大的随机波动时，宜选较大的$\alpha$；如果注重于使用近期的值进行预测，宜选较大的$\alpha$。
> -   实际应用时还应考虑预测误差，预测时可以选择几个$\alpha$进行比较，找出预测误差最小的作为最后的$\alpha$。
> -   一般$\alpha$取值不大于0.5。若大于0.5才能接近实际值，说明序列有某种趋势或波动过大，一般不适合用简单指数平滑法进行预测。

209.    线性趋势（Linear Trend）：按直线上升或下降的长期变动模式。
209.    Holt模型（Holt’s Model）：在指数平滑中加入线性趋势成分的双参数预测模型。
209.    非线性趋势（Nonlinear Trend）：非直线形态的长期变动模式。
209.    指数曲线（Exponential Curve）：按指数函数增长或下降的趋势线。
209.    多阶曲线（Polynomial Curve）：用二次及以上多项式拟合的曲线趋势。
209.    Winter模型（Winter’s Model）：在Holt基础上再加入季节成分的指数平滑预测模型。

> 预测方法的选择与评估，可以从适合的数据模式、数据要求和预测期几个关键维度进行考虑。

215.    分解预测（Decomposition Forecasting）：把时间序列拆成趋势、季节、循环和不规则成分后分别预测再叠加，以得到未来值的预测方法。
215.    季节指数（Seasonal Index）：衡量某季节相对全年平均水平的波动比例指标。

## 非参数检验

217.    非参数检验（Non-parametric Test）：不依赖总体分布假设即可检验总体特征的方法。

> 在总体分布未知或严重偏离正态、方差齐性等经典假设不可满足的情形下，利用秩、符号、置换或经验分布等无分布（distribution-free）方法构造检验统计量并推导其精确或渐近抽样分布，从而在控制第一类错误率 α 的同时，对总体中位数、位置参数、分布形态或变量间的独立性/关联性进行显著性推断，并提供可解的置信区间和效应量估计，以保障结论的稳健性与广泛适用性。

218.    符号检验（Sign Test）：用正负符号数量检验配对数据中位数差异的非参数方法。
218.    Wilcoxon符号秩检验（Wilcoxon Signed-Rank Test）：结合符号与秩次检验配对数据总体中位数差异的非参数方法。
218.    平均秩（Average Rank）：将同值观测赋平均秩次的秩处理方法。
218.    秩和（Rank Sum）：各组秩次加总用于比较分布位置的统计量。
218.    Mann-Whitney U检验（Mann-Whitney U Test）：用两独立样本秩和检验两总体分布位置是否相同的非参数方法。
218.    Wilcoxon秩和检验（Wilcoxon Rank-Sum Test）：与Mann-Whitney U检验等价的独立样本秩和检验。
218.    Kruskal-Wallis检验（Kruskal-Wallis Test）：推广至三组及以上独立样本的秩和方差分析非参数方法。
218.    斯皮尔曼秩相关系数（Spearman ρ）：基于秩次评估两变量单调相关程度的非参数指标。
218.    肯德尔秩相关系数（Kendall τ）：通过一致对比例刻画两变量秩次关联的非参数度量。

> 当总体分布能满足参数检验所需的假定时，参数检验方法的效率要比非参数检验高。但是当假定得不到满足时，非参数检验则更为有效。