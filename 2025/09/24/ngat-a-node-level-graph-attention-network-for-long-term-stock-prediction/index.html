<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="Xiangdi Blog"><meta property="og:type" content="article"><meta property="og:image" con ’tent=/img/background/stellar-trajectory.jpg><meta property="twitter:image" content="/img/background/stellar-trajectory.jpg"><meta name=title content="论文学习《基于NGAT模型的股票长期趋势与风险建模》"><meta property="og:title" content="论文学习《基于NGAT模型的股票长期趋势与风险建模》"><meta property="twitter:title" content="论文学习《基于NGAT模型的股票长期趋势与风险建模》"><meta name=description content="图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司……"><meta property="og:description" content="图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司……"><meta property="twitter:description" content="图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司……"><meta property="twitter:card" content="summary"><meta property="og:url" content="https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/"><meta name=keyword content="吴湘菂, WuXiangdi, XiangdiWu, 吴湘菂的网络日志, 吴湘菂的博客, Xiangdi Blog, 博客, 个人网站, Quant, 量化投资, 金融, 投资, 理财, 股票, 期货, 基金, 期权, 外汇, 比特币"><link rel="shortcut icon" href=/img/favicon.ico><title>论文学习《基于NGAT模型的股票长期趋势与风险建模》-吴湘菂的博客 | Xiangdi Blog</title><link rel=canonical href=/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.min.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-R757MDJ6Y6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R757MDJ6Y6")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>Xiangdi Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/categories/quant/>quant</a></li><li><a href=/categories/reading/>reading</a></li><li><a href=/categories/tech/>tech</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/vibe//>Vibe</a></li><li><a href=/travel//>TRAVEL</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/background/stellar-trajectory.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/quant title=Quant>Quant
</a><a class=tag href=/tags/model title=Model>Model</a></div><h1>论文学习《基于NGAT模型的股票长期趋势与风险建模》</h1><h2 class=subheading>NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction</h2><span class=meta>Posted by
    "Xiangdi Wu"
on
Wednesday, September 24, 2025</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p><img src=/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img1.webp alt=Image>
图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司。</p><p>然而，当前基于图模型的股票预测研究范式面临三个核心挑战 ：</p><ol><li><strong>下游任务设计的局限性</strong>：现有研究普遍聚焦于“次日趋势预测”这类短周期任务 ，但这无法充分利用图结构所能捕捉到的、通常具有滞后性的动量溢出效应。同时，这些模型大多只关注收益率，而忽略了波动率——一个衡量市场风险的关键指标。</li><li><strong>模型设计的普适性问题</strong>：通用的GNN架构（如GCN和GAT）假设图中节点的特征分布相似，这在由各具独特行为模式的公司构成的企业关系图（Corporate Relationship Graphs, CRGs）中难以成立。而一些为股票预测定制的复杂模型，则往往以牺牲泛化能力为代价。</li><li><strong>图构建方法的有效性评估缺失</strong>：企业关系图的构建大多依赖于经验，缺乏一套系统性的方法来比较不同图结构的优劣。</li></ol><p>为应对这些挑战，本文提出了一套创新的解决方案。首先，设计了一个全新的<strong>长期股票预测任务</strong>，该任务不仅预测未来一段时间的平均收益率趋势，还预测其波动率，从而更好地匹配动量溢出效应的长期特性，并为投资者提供更具操作性的洞见。其次，开发了一种名为<strong>节点级图注意力网络（Node-level Graph Attention Network, NGAT）</strong> 的新型模型架构。NGAT的核心创新在于为每个公司（节点）分配一个独特的注意力机制，使其能根据自身特性学习如何聚合邻居节点的信息，从而完美适应企业关系图的异构性。最后，通过实验揭示了仅依赖下游任务表现来评估图构建方法优劣的局限性。</p><p><img src=/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img2.webp alt=Image></p><hr><h4 id=问题构建-problem-formulation><strong>问题构建 (Problem Formulation)</strong></h4><p>为了克服传统次日预测任务的短视性，本文将研究的核心聚焦于两个全新的长期预测任务：<strong>长期收益率均值预测</strong>与<strong>长期波动率预测</strong>。这两个任务的目标是基于历史数据和企业关系图，预测目标股票在未来为期 T 个交易日内的表现。</p><p><strong>1. 长期收益率趋势预测 (Return Trend Prediction)</strong></p><p>该任务被构建为一个<strong>分类问题</strong>，旨在判断未来 T 日的平均收益率相较于过去 T 日是上涨还是下跌。</p><ul><li><p>首先，定义股票 s 在第 d 日的对数收益率：$r^s_d=log(p^s_d/p^s_{d-1})$，其中 $p^s_d$ 是调整后的收盘价。</p></li><li><p>其次，基于此定义分类标签 $y^s_{d,c}$：</p>$$y^s_{d,c}=1(\frac{1}{T}\sum\limits_{i=d+1}^{d+T}r^s_i > \frac{1}{T}\sum\limits_{i=d-T+1}^{d}r^s_i)$$<p>该公式的含义是：如果未来 T 日的平均收益率大于过去 T 日的平均收益率，则标签为1，否则为0。</p></li></ul><p>这个定义巧妙地将股价的未来走势划分成四种具有明确投资含义的情景：</p><ul><li><strong>(LP, N+)</strong> : 上一期为正收益（Last period Positive），下一期收益更高（Next period +），定义为  <strong>“飙升 (surge)”</strong> 。</li><li><strong>(LN, N+)</strong> : 上一期为负收益（Last period Negative），下一期收益更高，定义为  <strong>“反弹 (rebound)”</strong> 。</li><li><strong>(LP, N-)</strong> : 上一期为正收益，下一期收益更低，定义为  <strong>“回调 (pullback)”</strong> 。</li><li><strong>(LN, N-)</strong> : 上一期为负收益，下一期收益更低，定义为  <strong>“暴跌 (plunge)”</strong> 。</li></ul><p>这四种情景为投资决策提供了清晰的指引。“N+”情景（飙升、反弹）预示着潜在的市场机会，例如，“飙升”情景建议投资者“持有”或“增持”。“N-”情景（回调、暴跌）则警示了下行风险，例如，“暴跌”情景建议“卖出”或“做空”。</p><p><strong>2. 长期波动率预测 (Volatility Prediction)</strong></p><p>该任务被构建为一个<strong>回归问题</strong>，目标是精确预测未来 T 日的波动率数值。</p><ul><li><p>本文采用已实现的样本标准差作为波动率的估计值。</p></li><li><p>其计算公式如下：</p><p>$y^s_{d,r}=\sqrt{\frac{1}{T}\sum\limits_{i=d+1}^{d+T}(r^s_i-\mu)^2}$，其中 $\mu=\frac{1}{T}\sum\limits_{i=d+1}^{d+T}r^s_i$</p><p>这里 $\mu$ 是未来 T 日对数收益率的均值。</p></li></ul><p>通过将收益率趋势预测与波动率预测相结合，投资者不仅能判断未来市场的方向，还能评估该方向预测的可靠性及伴随的风险。这种结合为制定稳健的长期投资策略提供了更全面的市场动态理解。</p><hr><h4 id=模型架构-model-architecture><strong>模型架构 (Model Architecture)</strong></h4><p>本文提出的NGAT模型由三大模块构成：序列嵌入模块、关系嵌入模块和输出层。其整体架构和计算流程如图1所示。</p><p><strong>1. 序列嵌入 (Sequential Embedding)</strong></p><p>为捕捉股价自身的时间序列依赖性，模型首先使用循环神经网络（RNN）对历史交易数据进行编码。遵循业界常规做法，本文选用长短期记忆网络（LSTM）作为序列嵌入模块。</p><ul><li><p>对于任意股票 s，在第 d 日，模型输入一个包含过去 $\Delta{d}$ 天交易特征的时间序列矩阵 $X^s_d$。</p></li><li><p>通过LSTM网络生成该股票的序列嵌入向量 $h^s_d$，并取最后一个时间步的输出作为最终的序列表示。</p>$$h^s_d=LSTM(X^s_d)$$</li></ul><p><strong>2. 关系嵌入 (Relational Embedding)</strong></p><p>股价的变动不仅受自身历史数据影响，还受到关联股票的联动影响。因此，模型的核心在于如何有效地融合序列嵌入信息与企业间的关系信息，生成关系嵌入。</p><ul><li><p><strong>关系图构建 (Relation Building)</strong></p><ul><li>模型中的企业关系边是基于公司在财经新闻或社交媒体（如推文）中的共现关系动态构建的。</li><li>具体而言，对于每日的每一篇新闻，提取其中提及的所有公司，并为这些公司两两之间构建边。</li><li>通过对每日新闻进行聚合，得到一个无条件的日度关系属性 $R^{s_i,s_j}_d$。</li><li>为了模拟新闻影响的持续性与时效性，模型引入了一个带遗忘机制的时间加权聚合方法，用于计算最终的<strong>条件关系属性</strong> $\hat{R}^{s_i,s_j}_d$，该方法给予近期的新闻更高的权重。</li></ul>$$\hat{R}^{s_i,s_j}_d=\sum\limits_{m=0}^{\delta-1}w_m·R^{s_i,s_j}_{d-m}$$</li><li><p><strong>节点级图注意力层 (Node-level Graph Attention Layer)</strong></p><p>这是NGAT模型的<strong>核心创新</strong>。与标准GAT为所有节点使用一套共享的注意力参数不同，NGAT为每个节点（公司）都分配了一套独立的注意力机制参数。这使得模型能够针对每家公司的独特性，学习个性化的信息聚合方式。</p><ul><li><strong>注意力系数计算</strong>: 对于节点 i 和它的邻居节点 j，其注意力系数 $\beta_{ij}$ 的计算过程如下：</li></ul>$$\beta_{ij}=\frac{exp(\hat{R}^{i,j}_dLeakReLU(a^T_i[W_qh^i||W_kh^j]))}{\sum\limits_{k\in N(i)}exp(\hat{R}^{i,m}_dLeakReLU(a^T_i[W_qh^i||W_kh^m]))}$$<ul><li><strong>邻居信息聚合</strong>: 为防止Softmax归一化可能导致的“过平滑”问题（即过度强调邻居信息而丢失节点自身特性），模型在聚合邻居信息后，还拼接了节点自身的变换后嵌入。</li></ul>$$\hat{h}^i=\sigma(W[W_vh^i||\sum\limits_{j\in N(i)}\beta_{ij}W_kh^j])$$<ul><li><strong>多头注意力机制 (Multi-head Attention)</strong> : 为增强模型的稳定性和表达能力，NGAT同样采用了多头注意力机制，即将多个独立的注意力计算结果进行拼接或平均。</li></ul><ol><li>首先，对节点的序列嵌入 $h^i$ 和 $h^j$ 进行线性变换，得到查询向量（Query）和键向量（Key）。</li><li>然后，将这两个向量拼接后，通过节点i专属的注意力参数 $\alpha_i$ 进行计算，并结合前述的条件关系属性 $\hat{R}^{i,j}_d$ 进行加权。</li><li>最终，使用LeakyReLU激活函数和Softmax进行归一化，得到最终的注意力系数。</li></ol></li></ul><p><strong>3. 输出层 (Output Layer)</strong></p><p>最后，将经过多层关系嵌入模块处理后的最终表示向量 $\hat{h}^{'i}$ 输入到一个全连接输出层，得到预测结果 $\hat{y}^{i}$ 。</p><ul><li>对于回归任务（波动率预测），直接进行线性变换。</li><li>对于分类任务（收益率趋势预测），则额外经过一个Sigmoid激活函数。</li><li>模型的损失函数相应地采用均方误差（MSE）或二元交叉熵损失（Binary Cross Entropy）。</li></ul><hr><h4 id=实验设置-experiments><strong>实验设置 (Experiments)</strong></h4><p><strong>1. 数据集</strong></p><p>实验使用了两个公开的日度数据集：</p><ul><li><strong>SPNews Dataset</strong>: 包含标普500成分股公司的交易数据和相关新闻数据。</li><li><strong>ACL2018 Dataset</strong>: 包含两年的公司交易数据和相关的推文（Tweets）数据，公司间的关系通过推文中股票代码标签（如<a href>#aapl</a>）的共现来定义。</li></ul><p>对于数值型的交易数据（开、高、低、收、成交量等），进行了三步标准化处理，以消除不同股票价格量纲的差异并减少信息冗余。</p><p><strong>2. 训练设置</strong></p><p>模型的所有超参数均通过在验证集上进行网格搜索确定，例如关系图构建的记忆窗口 $\delta$（最优为5天），预测周期 T（1天、5天、10天、21天，分别对应日、周、双周、月），以及注意力头数等。优化器采用Adam。</p><p><strong>3. 基线模型</strong></p><p>为了全面评估NGAT的性能，本文选取了多种非图模型和图模型作为基线进行对比，包括：</p><ul><li>非图模型：LSTM。</li><li>通用图模型：GCN , GAT。</li><li>金融领域特定图模型：LSTM+GCN , TGC , AD-GAT。</li></ul><p><strong>4. 评估指标</strong></p><ul><li><strong>分类任务 (收益率趋势)</strong> : 采用准确率（ACC）、马修斯相关系数（MCC，适用于类别不均衡场景）和ROC曲线下面积（AUC）。</li><li><strong>回归任务 (波动率)</strong> : 采用样本外决定系数（$R^2$）和均方误差（MSE）。</li></ul><p><img src=/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img3.webp alt=Image></p><hr><h4 id=结果与讨论-results-and-discussion><strong>结果与讨论 (Results and Discussion)</strong></h4><p><strong>1. 收益率趋势预测性能</strong></p><p>实验结果明确显示，<strong>NGAT在两个数据集上的收益率趋势分类任务中均显著优于所有基线模型</strong>。</p><ul><li>相较于标准的GAT模型，NGAT取得了显著的性能提升，例如在SPNews数据集上当 T=10 时，ACC、MCC和AUC分别相对提升了2.08%、8.12%和1.41%。这充分证明了节点级注意力机制在金融建模中的普适优越性。</li><li>一个非常重要的发现是，所有模型的长期预测准确率都超过了70%，远高于传统次日预测任务中常见的50-60%的水平，这从实践上肯定了长期预测任务的价值。</li><li>在四种市场情景的识别上，NGAT尤其擅长捕捉“飙升”和“暴跌”这类关键的投资和避险机会。</li></ul><p><strong>2. 波动率预测性能</strong></p><p>在波动率预测任务中，<strong>NGAT同样稳定地超越了所有基线模型</strong>。</p><ul><li>实验清晰地表明，基于图的模型相比单纯的序列模型（如LSTM）在波动率预测上具有明显优势，验证了关系信息的重要性。</li><li>即使在由新闻文本构建的、可能包含噪声的关系图上，NGAT依然表现稳健，证明其能够从不完美的图数据中有效提取可靠信号。</li><li>一个有趣的现象是，随着预测周期 T 的增长，波动率预测的性能显著提升，其中月度波动率（T=21）的预测效果最好。这与金融实践相符，因为月度波动率比充满噪声的短期波动率在投资组合管理等应用中更具价值。</li></ul><p><img src=/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img4.webp alt=Image></p><p><strong>3. 综合分析与洞见</strong></p><ul><li><p>一个核心洞见是：<strong>除了NGAT，没有任何一个其他的图模型（无论是通用架构还是金融领域的SOTA模型）能够在所有预测周期和任务上全面超越基础的LSTM模型</strong>。</p></li><li><p>其深层原因是，不同的预测周期需要捕捉不同时间尺度下的动量溢出效应。对于一个公司而言，在周、月等不同尺度下，对其有显著影响的关联公司可能是不同的。传统的共享特征聚合机制（如GAT）无法适应这种动态变化，导致效率低下。而NGAT的节点级注意力机制恰好解决了这个问题，使其能够自适应地调整信息聚合方式，从而在各种时间尺度上都表现出色。</p></li><li><p>综合收益率和波动率的预测结果，将预测周期 T 设为21天（约一个月）是在两个任务上取得性能平衡的理想选择。</p></li></ul><p><strong>4. 图构建方法对比</strong></p><p>实验还比较了不同图构建方法（如基于新闻共现的动态图、基于收益率相关的动态图等）的性能。</p><ul><li>结果显示，无论采用何种图构建方法，NGAT的性能始终优于GAT，再次凸显了其架构优势。</li><li>同时也发现，仅凭下游任务的表现来评估图的“质量”是片面的。例如，在GAT模型下，一种图构建方法可能表现更优；但在更先进的NGAT模型下，由于模型本身强大的容错和信号提取能力，不同图方法之间的性能差异会减小。这说明，先进的模型架构可以在一定程度上弥补图构建过程中的缺陷。</li></ul><p>论文来源：<a href=https://arxiv.org/abs/2507.02018>https://arxiv.org/abs/2507.02018</a></p><p>相关代码：<a href=https://github.com/FreddieNIU/NGAT>https://github.com/FreddieNIU/NGAT</a></p><p>笔记来源：大头的算法笔记/QuantML</p><hr><ul class=pager><li class=previous><a href=/2025/07/04/quantitative_finance_core_knowledge_system/ data-toggle=tooltip data-placement=top title=量化金融学习方案>&larr;
Previous Post</a></li></ul><script src=https://giscus.app/client.js data-repo=XiangdiWu/XiangdiWu.github.io data-repo-id=R_kgDOP0pDUQ data-category=Announcements data-category-id=DIC_kwDOP0pDUc4CvwjG data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/deep-learning title="deep learning">deep learning
</a><a href=/tags/machine-learning title="machine learning">machine learning
</a><a href=/tags/math title=math>math
</a><a href=/tags/model title=model>model
</a><a href=/tags/nlp title=nlp>nlp
</a><a href=/tags/quant title=quant>quant</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://www.factorwar.com/data/factor-models/>GetAstockFactors</a></li><li><a target=_blank href=https://datawhalechina.github.io/whale-quant/#/>Whale-Quant</a></li></ul></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:bernicewu2000@outlook.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat_qrcode.jpg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/xiangdiwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="Xiangdi Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Xiangdi Blog 2025</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><style>.markmap>svg{width:100%;height:300px}</style><script>window.markmap={autoLoader:{manual:!0,onReady(){const{autoLoader:e,builtInPlugins:t}=window.markmap;e.transformPlugins=t.filter(e=>e.name!=="prism")}}}</script><script src=https://cdn.jsdelivr.net/npm/markmap-autoloader></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity="sha512-r2+FkHzf1u0+SQbZOoIz2RxWOIWfdEzRuYybGjzKq18jG9zaSfEy9s3+jMqG/zPtRor/q4qaUCYQpmSjTw8M+g==" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity="sha512-INps9zQ2GUEMCQD7xiZQbGUVnqnzEvlynVy6eqcTcHN4+aQiLo9/uaQqckDpdJ8Zm3M0QBs+Pktg4pz0kEklUg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/mhchem.min.js integrity="sha512-mxjNw/u1lIsFC09k/unscDRY3ofIYPVFbWkP8slrePcS36ht4d/OZ8rRu5yddB2uiqajhTcLD8+jupOWuYPebg==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"},{display:!0,left:"\\[",right:"\\]"}],errorcolor:"#CD5C5C",throwonerror:!1})'></script></body></html>