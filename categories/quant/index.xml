<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quant on Xiangdi Blog</title><link>https://xiangdiwu.github.io/categories/quant/</link><description>Recent content in Quant on Xiangdi Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 24 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://xiangdiwu.github.io/categories/quant/index.xml" rel="self" type="application/rss+xml"/><item><title>论文学习《基于NGAT模型的股票长期趋势与风险建模》</title><link>https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/09/24/ngat-a-node-level-graph-attention-network-for-long-term-stock-prediction/</guid><description>&lt;p&gt;
 &lt;img src="https://xiangdiwu.github.io/img/2025/09/24/%e5%9f%ba%e4%ba%8eNGAT%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%82%a1%e7%a5%a8%e9%95%bf%e6%9c%9f%e8%b6%8b%e5%8a%bf%e4%b8%8e%e9%a3%8e%e9%99%a9%e5%bb%ba%e6%a8%a1/NGAT-img1.webp" alt="Image"&gt;


图表示学习方法，特别是图神经网络（GNNs），已成为金融领域的主流技术，其核心优势在于能够利用公司间的关联关系来增强对单个公司的理解和表示。通过将公司建模为图中的节点，将它们之间的关系（如供应链、新闻共现等）作为边，GNNs能够有效捕捉所谓的“动量溢出效应”，即一家公司的股价动向会影响到相关联的其他公司。&lt;/p&gt;</description></item><item><title>量化金融学习方案</title><link>https://xiangdiwu.github.io/2025/07/04/quantitative_finance_core_knowledge_system/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/07/04/quantitative_finance_core_knowledge_system/</guid><description>&lt;h2 id="融理论基础"&gt;⾦融理论基础&lt;/h2&gt;
&lt;p&gt;量化⾦融的学习离不开⾦融学基础，以下是关键知识点：&lt;/p&gt;
&lt;h4 id="融市场与融具"&gt;⾦融市场与⾦融⼯具&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;股票、债券、期权、期货等⾦融产品的定义及特点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;⾦融市场的运作机制，包括⼀级市场与⼆级市场。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="资产定价理论"&gt;资产定价理论&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;资本资产定价模型（CAPM）：系统性⻛险与预期收益的关系。&lt;/p&gt;</description></item><item><title>45 个问题入门量化投资</title><link>https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions/</guid><description>&lt;h1 id="前言"&gt;前言&lt;/h1&gt;
&lt;p&gt;很多人对量化感兴趣，但缺乏基础知识。这一系列科普文章会是很好的入门选择。&lt;/p&gt;
&lt;p&gt;系列文章旨在提供一个全面而易于理解的入门指南。这些文章覆盖了量化投资的基本概念、策略构建、风险管理以及实际应用等多个方面，内容浅显易懂，无需任何先验知识。通过阅读这些文章，您还将了解到量化投资在现代金融市场中的重要性和应用前景。&lt;/p&gt;</description></item><item><title>Learn Quantitative Investing with 45 Questions</title><link>https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions-english/</link><pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/07/03/learn-quantitative-investing-with-45-questions-english/</guid><description>&lt;h1 id="q1-similarities-and-differences-between-quantitative-investment-and-discretionary-investment"&gt;Q1: Similarities and Differences Between Quantitative Investment and Discretionary Investment?&lt;/h1&gt;
&lt;p&gt;Broadly speaking, &lt;strong&gt;Quantitative Investment&lt;/strong&gt; is an investment methodology based on data, centered around models, and often utilizing programmatic trading as a tool.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data-driven:&lt;/strong&gt; Generally, more data points and more structured data are more conducive to modeling. If an event has never happened historically or has only occurred a few times, it is difficult to find suitable data for training. &amp;ldquo;Patterns&amp;rdquo; derived from past experiences may not be effective in such scenarios.&lt;/p&gt;</description></item><item><title>量化学习常见误区</title><link>https://xiangdiwu.github.io/2025/07/02/common-misconceptions-in-quantitative-learning/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/07/02/common-misconceptions-in-quantitative-learning/</guid><description>&lt;p&gt;量化投资的学习之路布满陷阱，许多聪明人在这里栽了跟头，不是因为他们不够努力，而是因为踩了那些看似诱人实则危险的误区。&lt;/p&gt;
&lt;h2 id="误区一认为会编程会量化投资"&gt;&lt;strong&gt;误区一：认为“会编程=会量化投资”&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;表现：&lt;/strong&gt; 过度沉迷于学习Python、Pandas、NumPy、各种库（如backtrader, zipline）甚至机器学习框架（TensorFlow, PyTorch），认为掌握了这些工具就等于掌握了赚钱的钥匙。&lt;/p&gt;</description></item><item><title>量化学习计划</title><link>https://xiangdiwu.github.io/2025/07/01/quantitative-learning-plan/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2025/07/01/quantitative-learning-plan/</guid><description>&lt;h2 id="量化入门"&gt;量化入门&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;思想和逻辑层面了解量化投资&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编程语言入门：Python和Packages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编程语言入门：金融数据处理、SQL 数据库&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经典策略入门：双均线、动量/反转、均值回归、配对交易、风险平价入门实现与评价等&lt;/p&gt;</description></item><item><title>量化投资系统的五大核心模型</title><link>https://xiangdiwu.github.io/2023/05/23/five-core-models-of-quantitative-investment-systems/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2023/05/23/five-core-models-of-quantitative-investment-systems/</guid><description>&lt;blockquote&gt;
&lt;p&gt;本文系统介绍了量化投资系统的五大核心模型，旨在通过数据驱动和理论驱动的方法优化投资决策。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;量化投资系统包括五大模型：&lt;/p&gt;
&lt;p&gt;1、超额收益模型 Alpha Models&lt;br&gt;
2、风险管理模型 Risk Model&lt;br&gt;
3、交易成本模型 Transaction Cost Models&lt;br&gt;
4、投资组合构建模型 Portfolio Construction Models&lt;br&gt;
5、执行模型 Execution Models&lt;br&gt;&lt;/p&gt;</description></item><item><title>量化编程基础： Python</title><link>https://xiangdiwu.github.io/2021/07/06/quant-python-basics/</link><pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/2021/07/06/quant-python-basics/</guid><description>&lt;h2 id="一python-基本数据类型"&gt;一、Python 基本数据类型&lt;/h2&gt;
&lt;p&gt;Python 的三大基本数据类型，是量化分析的起点：&lt;/p&gt;
&lt;h3 id="整数int"&gt;整数（int）&lt;/h3&gt;
&lt;p&gt;整数是没有小数部分的数字，在金融中常用于记录 “数量”—— 比如股票的持仓股数（1000 股）、交易日天数（252 天 / 年）、订单编号（10086）等。&lt;/p&gt;</description></item><item><title>自然语言处理：自然语言处理应用</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/</link><pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp6-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8/</guid><description>&lt;p&gt;自然语言处理的应用（即NLP任务）分布非常广泛。本章节以任务的形式进行划分，主要介绍三类具有代表性的自然语言处理应用，包括文本分类（序列-编码-类别）、文本摘要（序列-编码-解码-序列）、以及机器阅读理解（序列-编码-同步序列）。每个部分首先介绍了应用的概念及挑战，然后介绍了一些具有代表性的论文工作。&lt;strong&gt;注意，从接触一个领域的具体任务开始，已经初步进入到了对该科研领域的探索之中。每个领域都有一些具体的任务，而确定一个任务往往就是着手开展一项研究工作的第一步。&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>自然语言处理：预训练模型</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp5-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="elmo"&gt;ELMo&lt;/h1&gt;
&lt;p&gt;预训练词向量(如word2vec和GloVe等)通常只能为一个单词产生一个特定的词向量，而忽略了该单词的&lt;strong&gt;上下文(context)&lt;/strong&gt; 关系，因而无法解决&lt;strong&gt;一词多义&lt;/strong&gt;或&lt;strong&gt;一义多词&lt;/strong&gt;的问题。&lt;strong&gt;ELMo(embeddings from language models)&lt;/strong&gt; 本质上是一个深度双向LSTM模型，用于为一个句子中的每个单词生成上下文相关的词向量。将这些上下文相关词向量编码了单词的深层次语义和句法信息，因此当ELMo应用到许多NLP任务中，这些任务的效果相对于使用静态的词向量往往能得到很大的提升。&lt;/p&gt;</description></item><item><title>自然语言处理：“序列到序列”与“注意力机制”</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/</link><pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp4-seq2seq%E4%B8%8Eattention/</guid><description>&lt;h1 id="序列到序列模型"&gt;序列到序列模型&lt;/h1&gt;
&lt;p&gt;许多单输出问题得以解决，比如命名实体识别、单词预测等。然而许多任务的输出是一个&lt;strong&gt;序列&lt;/strong&gt;，比如机器翻译、对话系统以及自动摘要等。这种问题应当使用seq2seq实现。&lt;/p&gt;</description></item><item><title>自然语言处理：语言模型</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp3-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="n元语法"&gt;N元语法&lt;/h1&gt;
&lt;p&gt;N元语言模型可以用来&lt;strong&gt;预测一个句子的下一个单词的概率&lt;/strong&gt;，或者&lt;strong&gt;计算一个句子的概率&lt;/strong&gt;。该模型常用于语音识别、拼写检查以及语法检查等领域。&lt;/p&gt;
&lt;p&gt;首先，&lt;strong&gt;一个句子“its water is so transparent that”后出现某一单词“the”的概率&lt;/strong&gt;为：&lt;/p&gt;</description></item><item><title>自然语言处理：词向量</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp2-%E8%AF%8D%E5%90%91%E9%87%8F/</guid><description>&lt;h1 id="词向量概述"&gt;词向量概述&lt;/h1&gt;
&lt;p&gt;在自然语言处理领域，词的&lt;strong&gt;表示(representation)&lt;/strong&gt; 是一个核心问题。我们希望将单词通过某种嵌入的形式表示，以捕获词的&lt;strong&gt;含义(meaning)&lt;strong&gt;以及词和词之间的&lt;/strong&gt;关系(relationship)&lt;/strong&gt;。一个解决方法是，使用wordnet(a thesaurus containing lists of &lt;strong&gt;synonym sets&lt;/strong&gt; and &lt;strong&gt;hypernyms&lt;/strong&gt;)，如下所示：&lt;/p&gt;</description></item><item><title>自然语言处理概述</title><link>https://xiangdiwu.github.io/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/5-nlp/nlp1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E8%BF%B0/</guid><description>&lt;h1 id="自然语言与编程语言"&gt;自然语言与编程语言&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;自然语言处理(natural language processing, NLP)&lt;/strong&gt; 是一门融合了计算机科学、人工智能以及语言学的交叉学科(interdisciplinary field)。这门学科研究的是如何通过机器学习等技术，让计算机学会处理人类语言，乃至实现最终目标：&lt;strong&gt;理解人类语言或人工智能&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：图神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl9-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="图神经网络概述"&gt;图神经网络概述&lt;/h1&gt;
&lt;p&gt;近年来，深度学习领域关于&lt;strong&gt;图神经网络(graph neural network, GNN)&lt;/strong&gt; 的研究热情日益高涨，图神经网络已经成为各大深度学习顶会的研究热点。GNN处理非结构化数据时的出色能力使其在网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题方面都取得了新的突破。&lt;/p&gt;</description></item><item><title>深度学习：注意力机制与外部记忆</title><link>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/</guid><description>&lt;p&gt;根据通用近似定理，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。&lt;/p&gt;</description></item><item><title>深度学习：深度生成模型</title><link>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl7-%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率生成模型"&gt;概率生成模型&lt;/h1&gt;
&lt;p&gt;概率生成模型，简称&lt;strong&gt;生成模型(generative model)&lt;/strong&gt;，是概率统计和机器学习中的一类重要模型，指一系列用于&lt;strong&gt;随机生成&lt;/strong&gt;可观测数据的模型。假设在一个连续的或离散的高维空间$\mathcal X$中，存在一个随机向量$\boldsymbol X$服从一个未知的数据分布$p_r(\boldsymbol x),\boldsymbol x \in \mathcal X$。生成模型是根据一些可观测的样本$\boldsymbol x^{(1)},\boldsymbol x^{(2)},\cdots,\boldsymbol x^{(N)}$来学习一个参数化的模型$p_\theta(\boldsymbol x)$来近似未知分布$p_r(\boldsymbol x)$，并可以用这个模型来生成一些样本，使得&lt;strong&gt;生成样本和真实样本尽可能地相似&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：自编码器</title><link>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl6-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</guid><description>&lt;h1 id="自编码器"&gt;自编码器&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;自编码器(auto-encoder, AE)&lt;/strong&gt; 是通过&lt;strong&gt;无监督&lt;/strong&gt;的方式来学习&lt;strong&gt;一组数据的有效编码(或表示)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设有一组$d$维的样本$\boldsymbol{x}^{(n)} \in \mathbb{R}^{d}, 1 \leqslant n \leqslant N$，自编码器将这组数据映射到$p$维的&lt;strong&gt;特征空间&lt;/strong&gt;得到每个样本的编码$\boldsymbol{z}^{(n)} \in \mathbb{R}^{p}, 1 \leqslant n \leqslant N$，并且希望&lt;strong&gt;这组编码可以重构出原来的样本&lt;/strong&gt;。自编码器的结构可分为两部分：&lt;/p&gt;</description></item><item><title>深度学习：神经网络的优化</title><link>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</link><pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl5-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</guid><description>&lt;h1 id="网络优化的难点"&gt;网络优化的难点&lt;/h1&gt;
&lt;h2 id="网络结构多样性"&gt;网络结构多样性&lt;/h2&gt;
&lt;p&gt;神经网络的种类非常多，比如卷积网络、循环网络等，其结构也非常不同。有些比较深，有些比较宽。不同参数在网络中的作用也有很大的差异，比如连接权重和偏置的不同，以及循环网络中循环连接上的权重和其它权重的不同。由于网络结构的多样性，我们很难找到一种通用的优化方法。&lt;strong&gt;不同的优化方法在不同网络结构上的差异也都比较大&lt;/strong&gt;。此外，网络的超参数一般也比较多，这也给优化带来很大的挑战。&lt;/p&gt;</description></item><item><title>深度学习：循环神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="序列数据和语言模型"&gt;序列数据和语言模型&lt;/h1&gt;
&lt;p&gt;全连接神经网络和卷积神经网络只能单独处理一个个的输入，&lt;strong&gt;前一个输入和后一个输入是完全没有关系的&lt;/strong&gt;。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要&lt;strong&gt;分析这些帧连接起来的整个序列&lt;/strong&gt;。这时，就需要用到深度学习领域中另一类非常重要神经网络：&lt;strong&gt;循环神经网络(recurrent neural network)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习：卷积神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;卷积神经网络(convolutional neural network, CNN)&lt;/strong&gt; 是一种具有局部连接、权重共享等特性的前馈神经网络。&lt;/p&gt;
&lt;p&gt;卷积神经网络最早是主要用来处理图像信息。在用全连接前馈网络来处理图像时，会存在参数太多、局部特征不变形等缺陷。卷积神经网络利用&lt;strong&gt;局部连接、权重共享以及汇聚&lt;/strong&gt;三大结构上的特性，使得数据具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数更少。&lt;/p&gt;</description></item><item><title>深度学习：神经网络</title><link>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;p&gt;&lt;strong&gt;人工神经网络(artificial neural network, ANN)&lt;/strong&gt; 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象，构建&lt;strong&gt;人工神经元&lt;/strong&gt;，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络。在人工智能领域，人工神经网络也常常简称为&lt;strong&gt;神经网络(neural network, NN)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>深度学习概述</title><link>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</link><pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/4-dl/dl1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</guid><description>&lt;h1 id="表示学习"&gt;表示学习&lt;/h1&gt;
&lt;p&gt;为了提高机器学习系统的准确率，需要将输入信息转换为有效的特征，或者更一般称为&lt;strong&gt;表示(representation)&lt;/strong&gt;。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就是可以叫做&lt;strong&gt;表示学习(representation learning)&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习：概率图模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml15-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="概率图模型概述"&gt;概率图模型概述&lt;/h1&gt;
&lt;div align="center"&gt;
&lt;img src="https://xiangdiwu.github.io/Kimages/2/image-20200818114353404.png" style="zoom:40%;" /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;概率图模型(probabilistic graphical model, PGM)&lt;/strong&gt;，简称图模型(graphical model, GM)，是指一种&lt;strong&gt;用图结构来描述多元随机变量之间条件独立关系的概率模型&lt;/strong&gt;，从而给研究&lt;strong&gt;高维空间中的概率模型&lt;/strong&gt;带来了很大的便捷性。&lt;/p&gt;</description></item><item><title>机器学习：话题模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml14-%E8%AF%9D%E9%A2%98%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="单词向量空间与话题向量空间"&gt;单词向量空间与话题向量空间&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;潜在语义分析(latent semantic analysis, LSA)&lt;/strong&gt; 是一种无监督学习方法，主要用于文本的话题分析，其特点是通过&lt;strong&gt;矩阵分解&lt;/strong&gt;发现&lt;strong&gt;文本与单词之间基于话题的语义关系&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;文本信息处理中，传统的方法以&lt;strong&gt;单词向量&lt;/strong&gt;表示文本的语义内容，以&lt;strong&gt;单词向量空间的度量&lt;/strong&gt;表示&lt;strong&gt;文本之间的语义相似度&lt;/strong&gt;。潜在语义分析旨在解决这种方法不能准确表示语义的问题，试图从大量的文本数据中发现&lt;strong&gt;潜在的话题&lt;/strong&gt;，以话题向量表示文本的语义内容，以话题向量空间的度量更准确地表示文本之间的语义相似度。这也是&lt;strong&gt;话题分析(topic modeling)的基本想法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习：特征选择</title><link>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</link><pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml13-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</guid><description>&lt;h1 id="子集搜索与评价"&gt;子集搜索与评价&lt;/h1&gt;
&lt;p&gt;给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。对当前学习任务有用的属性称为&lt;strong&gt;相关特征(relevant feature)&lt;/strong&gt;，无用的属性称为&lt;strong&gt;无关特征(irrelevant feature)&lt;/strong&gt;。&lt;strong&gt;特征选择(feature selection)&lt;/strong&gt; 是一个重要的数据预处理过程，指从给定的特征集合中选择出相关特征的子集的过程。特征选择能够减少维数灾难问题，同时降低学习任务的难度。&lt;/p&gt;</description></item><item><title>机器学习：聚类</title><link>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml11-%E8%81%9A%E7%B1%BB/</guid><description>&lt;h1 id="聚类的基本概念"&gt;聚类的基本概念&lt;/h1&gt;
&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt; 中，训练样本的标记信息是未知的，目标是通过对无标记样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。聚类(clustering)任务是一种常见的无监督学习方法。&lt;/p&gt;</description></item><item><title>机器学习：降维</title><link>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml12-%E9%99%8D%E7%BB%B4/</guid><description>&lt;h1 id="主成分分析"&gt;主成分分析&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;主成分分析(principal component analysis, PCA)&lt;/strong&gt; 是一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如下图所示的二维数据，如果将这些数据投影到一维空间，选择数据方差最大的方向进行投影(蓝轴)，才能最大化数据的差异性，保留更多的原始数据信息。&lt;/p&gt;</description></item><item><title>机器学习：EM算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml10-em%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="em算法的引入"&gt;EM算法的引入&lt;/h1&gt;
&lt;p&gt;概率模型有时既含观测变量，又含隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。&lt;strong&gt;EM算法&lt;/strong&gt;就是&lt;strong&gt;含有隐变量的概率模型参数的极大似然估计法&lt;/strong&gt;，或&lt;strong&gt;极大后验概率估计法&lt;/strong&gt;。我们仅讨论极大似然估计，极大后验概率估计与其类似。&lt;/p&gt;</description></item><item><title>机器学习：集成学习</title><link>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml9-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="个体与集成"&gt;个体与集成&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;集成学习(ensemble learning)&lt;/strong&gt; 通过构建并结合多个学习期来完成学习任务，有时也被称为&lt;strong&gt;多分类器系统(multi-classifier system)&lt;/strong&gt;、&lt;strong&gt;基于委员会的学习(committee-based learning)&lt;/strong&gt; 等。&lt;/p&gt;</description></item><item><title>机器学习：决策树</title><link>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml8-%E5%86%B3%E7%AD%96%E6%A0%91/</guid><description>&lt;h1 id="决策树模型"&gt;决策树模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;决策树(decision tree)&lt;/strong&gt; 是一种基本的&lt;strong&gt;分类&lt;/strong&gt;与&lt;strong&gt;回归&lt;/strong&gt;算法。决策树呈树形结构，在分类问题中表示基于特征对实例进行分类的过程，它可以认为是&lt;strong&gt;if-then规则&lt;/strong&gt;的集合，也可以认为是定义在特征空间与类空间上的&lt;strong&gt;条件概率分布&lt;/strong&gt;。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用训练好的决策模型进行分类。&lt;/p&gt;</description></item><item><title>机器学习：支持向量机</title><link>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid><description>&lt;h1 id="线性支持向量机与硬间隔最大化"&gt;线性支持向量机与硬间隔最大化&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;支持向量机(support vector machine, SVM)&lt;/strong&gt; 是一个经典的机器学习二分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强的优势。&lt;/p&gt;</description></item><item><title>机器学习：逻辑回归与最大熵模型</title><link>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</link><pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml6-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="逻辑回归模型"&gt;逻辑回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;逻辑回归(logistic regression, LR)&lt;/strong&gt; 模型是一种处理&lt;strong&gt;二分类&lt;/strong&gt;问题的线性模型。逻辑回归模型由logistic分布(logistic distribution)导出。设$X$是连续随机变量，$X$服从logistic分布是指$X$具有下列&lt;strong&gt;分布函数&lt;/strong&gt;和&lt;strong&gt;密度函数&lt;/strong&gt;：
&lt;/p&gt;</description></item><item><title>机器学习：感知机</title><link>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml4-%E6%84%9F%E7%9F%A5%E6%9C%BA/</guid><description>&lt;h1 id="感知机算法"&gt;感知机算法&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;感知机(perceptron)&lt;/strong&gt; 由Frank Rosenblatt于1957年提出，是一种广泛使用的线性分类器。感知器可谓是最简单的人工神经网络，只有一个神经元，是对生物神经元的简单数学模拟，有与生物神经元相对应的部件，如&lt;strong&gt;权重(突触)、偏置(阈值)及激活函数(细胞体)&lt;/strong&gt;，输出为+1或-1。&lt;/p&gt;</description></item><item><title>机器学习：贝叶斯分类器</title><link>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</guid><description>&lt;h1 id="朴素贝叶斯的学习与分类"&gt;朴素贝叶斯的学习与分类&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;朴素贝叶斯(naive Bayse)&lt;/strong&gt; 算法是基于&lt;strong&gt;贝叶斯定理&lt;/strong&gt;与&lt;strong&gt;特征条件独立假设&lt;/strong&gt;的分类方法。设输入空间$\mathcal X \subseteq \mathbb R^n$为$n$维向量的集合，输出空间为类标记集合$\mathcal Y={c_1,c_2,\cdots,c_K}$。输入为特征向量$x \in \mathcal X$，输出为类标记$y \in \mathcal Y$。$P(X,Y)$是输入空间和输出空间上的随机变量$X$和$Y$的联合概率分布，训练数据集(含$N$个数据)由$P(X,Y)$独立同分布产生。朴素贝叶斯在数据集上学习&lt;strong&gt;联合概率分布&lt;/strong&gt;$P(X,Y)$。具体地，先学习以下先验概率分布及条件概率分布：&lt;/p&gt;</description></item><item><title>机器学习：K近邻算法</title><link>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml3-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid><description>&lt;h1 id="近邻算法原理"&gt;$\boldsymbol k$近邻算法原理&lt;/h1&gt;
&lt;p&gt;$k$近邻($k$-nearest neighbor, $k$NN)算法是一种常用的监督学习方法。其基本思想为：给定一组数据，基于某种距离度量找出训练集中与其最靠近的$k$个训练样本，然后基于这$k$个邻居的信息来进行预测。通常，在&lt;strong&gt;分类任务&lt;/strong&gt;中可使用&lt;strong&gt;投票法&lt;/strong&gt;，即选择$k$个样本中&lt;strong&gt;出现最多的类别标记&lt;/strong&gt;作为预测结果；在&lt;strong&gt;回归任务&lt;/strong&gt;中可使用平均法，即将$k$个样本的实值输出标记的平均值作为预测结果。该算法属于“&lt;strong&gt;惰性学习(lazy learning)&lt;/strong&gt;”方法之一，没有显式的学习过程。相应的，那些在训练阶段就对样本进行学习处理的方法，称为“&lt;strong&gt;急切学习(eager learning)&lt;/strong&gt;”。&lt;/p&gt;</description></item><item><title>机器学习：线性回归</title><link>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid><description>&lt;h1 id="线性回归模型"&gt;线性回归模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;线性回归(linear regression)&lt;/strong&gt; 是机器学习和统计学中最基础和广泛应用的模型，是一种对自变量和隐变量之间关系进行建模的回归分析。自变量数量为1时称为&lt;strong&gt;简单线性回归&lt;/strong&gt;，自变量数量大于1时称为&lt;strong&gt;多元线性回归&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>机器学习基础</title><link>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/3-ml/ml1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="人工智能"&gt;人工智能&lt;/h1&gt;
&lt;p&gt;&lt;b&gt;智能(intelligence)&lt;/b&gt;是现代生活中很常见的一个词，比如智能手机、智能家居、智能驾驶等。在不同使用场合中，智能的含义也不太一样。比如“智能手机”中的“智能”一般是指由计算机控制并具有某种智能行为的意思。这里的“计算机控制”+“智能行为”隐含了对人工智能的简单定义。&lt;/p&gt;</description></item><item><title>量化数学基础：信息论</title><link>https://xiangdiwu.github.io/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math5-%E4%BF%A1%E6%81%AF%E8%AE%BA/</guid><description>&lt;p&gt;&lt;strong&gt;信息论(information theory)&lt;/strong&gt; 是数学、物理、计算机科学等多个学科的交叉领域。信息论是由Claude Shannon最早提出的，主要研究信息的量化、存储和通信等方法。这里，&lt;strong&gt;“信息”是指一组消息的集合&lt;/strong&gt;。假设在一个噪声通道上发送消息，我们需要考虑如何对每一个信息进行编码、传输以及解码，使得接收者可以尽可能准确地重构出消息。在机器学习相关领域，信息论也有着大量的应用。比如&lt;strong&gt;特征抽取、统计推断、自然语言处理&lt;/strong&gt;等。&lt;/p&gt;</description></item><item><title>量化数学基础：概率统计</title><link>https://xiangdiwu.github.io/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math4-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/</guid><description>&lt;h1 id="概率统计的基本概念"&gt;概率统计的基本概念&lt;/h1&gt;
&lt;h2 id="样本空间与随机事件"&gt;样本空间与随机事件&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;样本空间(sample space)&lt;strong&gt;是一个随机试验&lt;/strong&gt;所有可能结果的集合&lt;/strong&gt;。例如，如果抛一枚硬币，那么样本空间的集合就是{正面, 反面}；如果抛一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为&lt;strong&gt;样本点&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>量化数学基础：数学优化</title><link>https://xiangdiwu.github.io/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math3-%E6%95%B0%E5%AD%A6%E4%BC%98%E5%8C%96/</guid><description>&lt;p&gt;&lt;b&gt;数学优化(mathematical optimization)&lt;/b&gt;问题也叫最优化问题，指在一定约束条件下，求解一个目标函数的最大值或最小值问题。数学优化问题的定义为：给定一个目标函数(也叫代价函数)$f:A\rightarrow\mathbb R$，寻找一个变量$\boldsymbol x^* \in \mathcal D$，使得对于所有$\mathcal D$中的$\boldsymbol x$，$f(\boldsymbol{x}^{*}) \leqslant f(\boldsymbol{x})$(最小化)；或者$f(\boldsymbol{x}^{*}) \geqslant f(\boldsymbol{x})$(最大化)，其中$\mathcal D$为变量$\boldsymbol x$的&lt;strong&gt;约束集&lt;/strong&gt;，也叫&lt;strong&gt;可行域&lt;/strong&gt;；$\mathcal D$中的变量被称为&lt;strong&gt;可行解&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>量化数学基础：微积分</title><link>https://xiangdiwu.github.io/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/</link><pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math2-%E5%BE%AE%E7%A7%AF%E5%88%86/</guid><description>&lt;p&gt;&lt;strong&gt;微积分(calculus)&lt;/strong&gt;是研究函数的&lt;strong&gt;微分(differentiation)&lt;/strong&gt;、&lt;strong&gt;积分(integration)&lt;/strong&gt;及其相关应用的数学分支。&lt;/p&gt;
&lt;h1 id="微分"&gt;微分&lt;/h1&gt;
&lt;h2 id="导数"&gt;导数&lt;/h2&gt;
&lt;p&gt;对于定义域和值域都是实数域的函数$f: \mathbb R \rightarrow \mathbb R$，若$f(x)$在点$x_0$的某个邻域$\Delta x$内，极限
&lt;/p&gt;</description></item><item><title>量化数学基础：线性代数</title><link>https://xiangdiwu.github.io/post/2-math/math1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</link><pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</guid><description>&lt;h1 id="向量和向量空间"&gt;向量和向量空间&lt;/h1&gt;
&lt;h2 id="向量"&gt;向量&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;标量(scalar)&lt;/b&gt;是一个实数，一般用斜体小写字母$a,b,c$来表示。&lt;b&gt;向量(vector)&lt;/b&gt;是由一组实数组成的有序数组，一个&lt;em&gt;n&lt;/em&gt;维向量$\boldsymbol a$由&lt;em&gt;n&lt;/em&gt;个有序实数组成，表示为$\boldsymbol{a}=\left[a_{1}, a_{2}, \cdots, a_{n}\right]$，其中$a_{i}$称为向量$\boldsymbol{a}$的第$i$个分量(第$i$维)。&lt;/p&gt;</description></item><item><title>基础统计学名词解释</title><link>https://xiangdiwu.github.io/post/2-math/math0-%E5%9F%BA%E7%A1%80%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate><guid>https://xiangdiwu.github.io/post/2-math/math0-%E5%9F%BA%E7%A1%80%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</guid><description>&lt;h2 id="统计数据和计算机"&gt;统计、数据和计算机&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;统计学（Statistics）：收集、整理、分析并解释数据以支持决策的科学。&lt;/li&gt;
&lt;li&gt;描述统计（Descriptive Statistics）：用图表和概括量描述数据基本特征的方法。&lt;/li&gt;
&lt;li&gt;推断统计（Inferential Statistics）：利用样本数据对总体进行估计与假设检验的技术。&lt;/li&gt;
&lt;li&gt;变量（Variable）：可以取不同数值或类别的观测特征。&lt;/li&gt;
&lt;li&gt;无序类别变量（Nominal Categorical Variable）：仅有名称而无内在顺序的分类特征。&lt;/li&gt;
&lt;li&gt;有序类别变量（Ordinal Categorical Variable）：类别间存在自然顺序但间距未知的特征，比如：很好、好、一般、差、很差。&lt;/li&gt;
&lt;li&gt;数值变量（Numerical Variable）：又称为定量变量，以数值表示并可进行算术运算的特征。&lt;/li&gt;
&lt;li&gt;无序类别数据（Nominal Data）：属于无序类别的观察结果集合。&lt;/li&gt;
&lt;li&gt;有序类别数据（Ordinal Data）：属于有序类别的观察结果集合。&lt;/li&gt;
&lt;li&gt;数值数据（Numerical Data）：由数值构成的观察结果集合。&lt;/li&gt;
&lt;li&gt;总体（Population）：研究对象的全部个体（数据）的集合。&lt;/li&gt;
&lt;li&gt;样本（Sample）：从总体中抽取用于研究的子集。&lt;/li&gt;
&lt;li&gt;样本量（Sample Size）：样本中个体的数量。&lt;/li&gt;
&lt;li&gt;简单随机抽样（Simple Random Sampling）：每个个体等概率被抽中的抽样方法。&lt;/li&gt;
&lt;li&gt;分层抽样（Stratified Sampling）：将总体分层后在各层内随机抽样的方法。&lt;/li&gt;
&lt;li&gt;系统抽样（Systematic Sampling）：按固定间隔从有序总体中抽取个体的抽样方法。&lt;/li&gt;
&lt;li&gt;整群抽样（Cluster Sampling）：以自然群体为单位整群抽取的抽样方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="用图表展示数据"&gt;用图表展示数据&lt;/h2&gt;
&lt;ol start="18"&gt;
&lt;li&gt;频数分布（Frequency Distribution）：展示各取值或区间出现次数的表格或图形。&lt;/li&gt;
&lt;li&gt;频数（Frequency）：某取值或区间在数据集中出现的次数。&lt;/li&gt;
&lt;li&gt;比例（Proportion）：部分在全体中所占的相对份额。&lt;/li&gt;
&lt;li&gt;比率（Ratio）：两个数值间的对比关系。&lt;/li&gt;
&lt;li&gt;条形图（Bar Chart）：用等宽条形高度表示类别频数的图形。&lt;/li&gt;
&lt;li&gt;帕累托图（Pareto Chart）：按频数降序排列并叠加累积比例的条形图。&lt;/li&gt;
&lt;li&gt;饼图（Pie Chart）：用扇形面积表示类别比例的圆形图。&lt;/li&gt;
&lt;li&gt;环形图（Donut Chart）：中间留空的饼图，用于展示多系列比例。&lt;/li&gt;
&lt;li&gt;直方图（Histogram）：用相邻矩形面积表示数值数据区间频数的图形。&lt;/li&gt;
&lt;li&gt;茎叶图（Stem-and-Leaf Plot）：保留原始数据形态的同时展示分布形状的图形。&lt;/li&gt;
&lt;li&gt;箱线图（Boxplot）：用五数概括展示数据分布与异常值的图形。&lt;/li&gt;
&lt;li&gt;离群值（Outlier）：在数据集中显著偏离其余观测、与整体模式不符的极端值。&lt;/li&gt;
&lt;li&gt;垂线图（Drop-Line Plot）：在分类轴上用垂线表示数值大小的图形。&lt;/li&gt;
&lt;li&gt;误差图（Error Bar Plot）：展示均值及变异或不确定性的图形。&lt;/li&gt;
&lt;li&gt;散点图（Scatter Plot）：用点的位置表示两个数值变量观测值的图形。&lt;/li&gt;
&lt;li&gt;重叠散点图（Overlaid Scatter Plot）：在同一坐标系中绘制多组散点以比较关系。&lt;/li&gt;
&lt;li&gt;雷达图（Radar Chart）：用多轴多边形展示多变量综合表现的图形。&lt;/li&gt;
&lt;li&gt;轮廓图（Contour Plot）：用等高线表示三维数据在二维平面上分布的图形。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="用统计量描述数据"&gt;用统计量描述数据&lt;/h2&gt;
&lt;ol start="36"&gt;
&lt;li&gt;简单平均值（Mean）：所有观测值之和除以观测个数的结果。&lt;/li&gt;
&lt;li&gt;中位数（Median）：排序后处于中间位置的观测值。&lt;/li&gt;
&lt;li&gt;四分位数（Quartile）：将排序数据四等分的三个分割点。&lt;/li&gt;
&lt;li&gt;众数（Mode）：数据集中出现次数最多的观测值。&lt;/li&gt;
&lt;li&gt;极差（Range）：最大值与最小值之差。&lt;/li&gt;
&lt;li&gt;四分位差（Interquartile Range）：第三四分位数与第一四分位数之差。&lt;/li&gt;
&lt;li&gt;方差（Variance）：观测值与均值偏差平方的平均，衡量数据离散度。&lt;/li&gt;
&lt;li&gt;标准差（Standard Deviation）：方差的正平方根，度量数据离散程度。&lt;/li&gt;
&lt;li&gt;标准分数（Z-score）：观测值与均值之差除以标准差后的无量纲结果。&lt;/li&gt;
&lt;li&gt;离散系数（Coefficient of Variation）：标准差与均值之比的无量纲相对变异指标。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;当需要比较不同单位或量级数据集的相对离散程度时使用离散系数。&lt;/p&gt;</description></item></channel></rss>